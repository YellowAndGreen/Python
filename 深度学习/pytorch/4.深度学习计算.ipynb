{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pOgkH1MyStW",
    "origin_pos": 0
   },
   "source": [
    "# 层和块\n",
    ":label:`sec_model_construction`\n",
    "\n",
    "当我们第一次介绍神经网络时，我们关注的是具有单一输出的线性模型。在这里，整个模型只由一个神经元组成。注意，单个神经元（1）接受一些输入；（2）生成相应的标量输出；（3）具有一组相关 *参数*（parameters），这些参数可以更新以优化某些感兴趣的目标函数。然后，当我们开始考虑具有多个输出的网络，我们就利用矢量化算法来描述整层神经元。像单个神经元一样，层（1）接受一组输入，（2）生成相应的输出，（3）由一组可调整参数描述。当我们使用softmax回归时，一个单层本身就是模型。然而，即使我们随后引入了多层感知机，我们仍然可以认为该模型保留了上面所说的基本结构。\n",
    "\n",
    "有趣的是，对于多层感知机而言，整个模型及其组成层都是这种结构。整个模型接受原始输入（特征），生成输出（预测），并包含一些参数（所有组成层的参数集合）。同样，每个单独的层接收输入（由前一层提供）生成输出（到下一层的输入），并且具有一组可调参数，这些参数根据从下一层反向传播的信号进行更新。\n",
    "\n",
    "虽然你可能认为神经元、层和模型为我们的业务提供了足够的抽象，但事实证明，我们经常发现谈论比单个层大但比整个模型小的组件更方便。例如，在计算机视觉中广泛流行的ResNet-152结构就有数百层。这些层是 由*层组*的重复模式组成。一次只实现一层这样的网络会变得很乏味。这种问题不是我们幻想出来的，这种设计模式在实践中很常见。上面提到的ResNet结构赢得了2015年ImageNet和COCO计算机视觉比赛的识别和检测任务 :cite:`He.Zhang.Ren.ea.2016`，目前ResNet结构仍然是许多视觉任务的首选结构。在其他的领域，如自然语言处理和语音，层以各种重复模式排列的类似结构现在也是普遍存在。\n",
    "\n",
    "为了实现这些复杂的网络，我们引入了神经网络*块*的概念。块可以描述单个层、由多个层组成的组件或整个模型本身。使用块进行抽象的一个好处是可以将一些块组合成更大的组件，这一过程通常是递归的。这一点在 :numref:`fig_blocks` 中进行了说明。通过定义代码来按需生成任意复杂度的块，我们可以通过简洁的代码实现复杂的神经网络。\n",
    "\n",
    "![多个层被组合成块，形成更大的模型。](http://d2l.ai/_images/blocks.svg)\n",
    ":label:`fig_blocks`\n",
    "\n",
    "从编程的角度来看，块由*类*（class）表示。它的任何子类都必须定义一个将其输入转换为输出的正向传播函数，并且必须存储任何必需的参数。注意，有些块不需要任何参数。最后，为了计算梯度，块必须具有反向传播函数。幸运的是，在定义我们自己的块时，由于自动微分（在 :numref:`sec_autograd` 中引入）提供了一些后端实现，我们只需要考虑正向传播函数和必需的参数。\n",
    "\n",
    "(**首先，我们回顾一下多层感知机**)（ :numref:`sec_mlp_concise` ）的代码。下面的代码生成一个网络，其中包含一个具有256个单元和ReLU激活函数的全连接的隐藏层，然后是一个具有10个隐藏单元且不带激活函数的全连接的输出层。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ll1GhnrIyStY",
    "origin_pos": 2,
    "outputId": "ea099ba8-d80a-4c77-9782-07348295f73f",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1839, -0.0838,  0.0086,  0.1385, -0.0363, -0.2057, -0.0634,  0.0235,\n",
       "          0.1118, -0.2660],\n",
       "        [-0.1168, -0.0667,  0.0179,  0.0684,  0.0273, -0.1577, -0.0631,  0.0392,\n",
       "          0.1353, -0.1612],\n",
       "        [-0.1690, -0.1925, -0.0656,  0.1314,  0.0359, -0.1964,  0.0469,  0.0238,\n",
       "          0.1945, -0.1040]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(3, 20)\n",
    "net(X)   # 直接获得模型的输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vt9i47dWyStZ",
    "origin_pos": 5,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "在这个例子中，我们通过实例化`nn.Sequential`来构建我们的模型，层的执行顺序是作为参数传递的。简而言之，(**`nn.Sequential`定义了一种特殊的`Module`**)，即在PyTorch中表示一个块的类。它维护了一个由`Module`组成的有序列表，注意，两个全连接层都是`Linear`类的实例，`Linear`类本身就是`Module`的子类。正向传播（`forward`）函数也非常简单：它将列表中的每个块连接在一起，将每个块的输出作为下一个块的输入。注意，到目前为止，我们一直在通过`net(X)`调用我们的模型来获得模型的输出。这实际上是`net.__call__(X)`的简写。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VU_2NttAyStZ",
    "origin_pos": 7
   },
   "source": [
    "## [**自定义块**]\n",
    "\n",
    "要想直观地了解块是如何工作的，最简单的方法可能就是自己实现一个。在实现我们自定义块之前，我们简要总结一下每个块必须提供的基本功能：\n",
    "\n",
    "1. 将输入数据作为其正向传播函数的参数。\n",
    "1. 通过正向传播函数来生成输出。请注意，输出的形状可能与输入的形状不同。例如，我们上面模型中的第一个全连接的层接收任意维的输入，但是返回一个维度256的输出。\n",
    "1. 计算其输出关于输入的梯度，可通过其反向传播函数进行访问。通常这是自动发生的。\n",
    "1. 存储和访问正向传播计算所需的参数。\n",
    "1. 根据需要初始化模型参数。\n",
    "\n",
    "在下面的代码片段中，我们从零开始编写一个块。它包含一个多层感知机，其具有256个隐藏单元的隐藏层和一个10维输出层。注意，下面的`MLP`类继承了表示块的类。我们的实现将严重依赖父类，只需要提供我们自己的构造函数（Python中的`__init__`函数）和正向传播函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sTVEvf1lyStZ",
    "origin_pos": 9,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    # 用模型参数声明层。这里，我们声明两个全连接的层\n",
    "    def __init__(self):\n",
    "        # 调用`MLP`的父类`Block`的构造函数来执行必要的初始化。\n",
    "        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数`params`（稍后将介绍）\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)  # 隐藏层\n",
    "        self.out = nn.Linear(256, 10)  # 输出层\n",
    "\n",
    "    # 定义模型的正向传播，即如何根据输入`X`返回所需的模型输出\n",
    "    def forward(self, X):\n",
    "        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ceRgFEibySta",
    "origin_pos": 11
   },
   "source": [
    "让我们首先关注正向传播函数。注意，它以`X`作为输入，计算带有激活函数的隐藏表示，并输出其未归一化的输出值。在这个`MLP`实现中，两个层都是实例变量。要了解这为什么是合理的，可以想象实例化两个多层感知机（`net1`和`net2`），并根据不同的数据对它们进行训练。当然，我们希望它们学到两种不同的模型。\n",
    "\n",
    "我们在构造函数中[**实例化多层感知机的层，然后在每次调用正向传播函数时调用这些层**]。注意一些关键细节。首先，我们定制的`__init__`函数通过`super().__init__()`调用父类的`__init__`函数，省去了重复编写适用于大多数块的模版代码的痛苦。然后我们实例化两个全连接层，分别为`self.hidden`和`self.out`。注意，除非我们实现一个新的运算符，否则我们不必担心反向传播函数或参数初始化，系统将自动生成这些。让我们试一下。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvR3wFohySta",
    "origin_pos": 13,
    "outputId": "d410481f-82c6-4a12-c029-61443e045364",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2587,  0.0077, -0.0152, -0.0801,  0.0909,  0.0766, -0.3063,  0.1413,\n",
       "          0.2121, -0.0381],\n",
       "        [ 0.0899,  0.0496, -0.0506,  0.0304,  0.1530,  0.0693, -0.1982,  0.1892,\n",
       "          0.0809, -0.0129]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4oSrE_pySta",
    "origin_pos": 15
   },
   "source": [
    "块抽象的一个主要优点是它的多功能性。我们可以子类化块以创建层（如全连接层的类）、整个模型（如上面的`MLP`类）或具有中等复杂度的各种组件。我们在接下来的章节中充分利用了这种多功能性，比如在处理卷积神经网络时。\n",
    "\n",
    "## [**顺序块**]\n",
    "\n",
    "现在我们可以更仔细地看看`Sequential`类是如何工作的。回想一下`Sequential`的设计是为了把其他模块串起来。为了构建我们自己的简化的`MySequential`，我们只需要定义两个关键函数：\n",
    "1. 一种将块逐个追加到列表中的函数。\n",
    "2. 一种正向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”。\n",
    "\n",
    "下面的`MySequential`类提供了与默认`Sequential`类相同的功能。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TVi0YK62yStb",
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for block in args:\n",
    "            # 这里，`block`是`Module`子类的一个实例。我们把它保存在'Module'类的成员变量\n",
    "            # `_children` 中。`block`的类型是OrderedDict。\n",
    "            self._modules[block] = block\n",
    "\n",
    "    def forward(self, X):\n",
    "        # OrderedDict保证了按照成员添加的顺序遍历它们\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x45VcZ3FyStb",
    "origin_pos": 20,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "在`__init__`方法中，我们将每个块逐个添加到有序字典`_modules`中。你可能会想知道为什么每个`Module`都有一个`_modules`属性，以及为什么我们使用它而不是自己定义一个Python列表。简而言之，`_modules`的主要优点是，在块的参数初始化过程中，系统知道在`_modules`字典中查找需要初始化参数的子块。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mt3LVaMgyStb",
    "origin_pos": 21
   },
   "source": [
    "当`MySequential`的正向传播函数被调用时，每个添加的块都按照它们被添加的顺序执行。现在可以使用我们的`MySequential`类重新实现多层感知机。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WswCcxGDyStb",
    "origin_pos": 23,
    "outputId": "583e735f-8ef6-4ca0-8a31-2cab1337a447",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0820, -0.0776, -0.0659, -0.0075, -0.1185, -0.0005,  0.1571, -0.1571,\n",
       "         -0.0343, -0.3000],\n",
       "        [ 0.1122, -0.0706, -0.0532,  0.0521, -0.2315,  0.0262, -0.0751,  0.0285,\n",
       "         -0.1170, -0.1475]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6pQsYVvyStc",
    "origin_pos": 25
   },
   "source": [
    "注意，`MySequential`的用法与之前为`Sequential`类编写的代码相同（如 :numref:`sec_mlp_concise` 中所述）。\n",
    "\n",
    "## [**在正向传播函数中执行代码**]\n",
    "\n",
    "`Sequential`类使模型构造变得简单，允许我们组合新的结构，而不必定义自己的类。然而，并不是所有的架构都是简单的顺序结构。当需要更大的灵活性时，我们需要定义自己的块。例如，我们可能希望在正向传播函数中执行Python的控制流。此外，我们可能希望执行任意的数学运算，而不是简单地依赖预定义的神经网络层。\n",
    "\n",
    "你可能已经注意到，到目前为止，我们网络中的所有操作都对网络的激活值及网络的参数起作用。然而，有时我们可能希望合并既不是上一层的结果也不是可更新参数的项。我们称之为常数参数（constant parameters）。例如，我们需要一个计算函数$f(\\mathbf{x},\\mathbf{w}) = c \\cdot \\mathbf{w}^\\top \\mathbf{x}$的层，其中$\\mathbf{x}$是输入，$\\mathbf{w}$是我们的参数，$c$是某个在优化过程中没有更新的指定常量。因此我们实现了一个`FixedHiddenMLP`类，如下所示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-aiZ83xyStc",
    "origin_pos": 27,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 不计算梯度的随机权重参数。因此其在训练期间保持不变。\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # 使用创建的常量参数以及`relu`和`dot`函数。\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        # 复用全连接层。这相当于两个全连接层共享参数。\n",
    "        X = self.linear(X)\n",
    "        # 控制流\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiRG19gyyStc",
    "origin_pos": 29
   },
   "source": [
    "在这个`FixedHiddenMLP`模型中，我们实现了一个隐藏层，其权重（`self.rand_weight`）在实例化时被随机初始化，之后为常量。这个权重不是一个模型参数，因此它永远不会被反向传播更新。然后，网络将这个固定层的输出通过一个全连接层。\n",
    "\n",
    "注意，在返回输出之前，我们的模型做了一些不寻常的事情。我们运行了一个while循环，在$L_1$范数大于$1$的条件下，将输出向量除以$2$，直到它满足条件为止。最后，我们返回了`X`中所有项的和。据我们所知，没有标准的神经网络执行这种操作。注意，此特定操作在任何实际任务中可能都没有用处。我们的重点只是向你展示如何将任意代码集成到神经网络计算的流程中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5yXPdzpQyStd",
    "origin_pos": 31,
    "outputId": "57e05408-c317-45ad-eacf-85785520981f",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0919, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRjx5naYyStd",
    "origin_pos": 32
   },
   "source": [
    "我们可以[**混合搭配各种组合块的方法**]。在下面的例子中，我们以一些想到的方法嵌套块。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PbMQQG-yStd",
    "origin_pos": 34,
    "outputId": "e84a275c-d4c0-44eb-ef7c-7fd2aa4d5fc2",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.4179, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOw_ZfETyStd",
    "origin_pos": 36
   },
   "source": [
    "## 效率\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oB4VP8uyStd",
    "origin_pos": 38,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "热心的读者可能会开始担心其中一些操作的效率。毕竟，我们在一个应该是高性能的深度学习库中进行了大量的字典查找、代码执行和许多其他的Python代码。Python的问题[全局解释器锁](https://wiki.python.org/moin/GlobalInterpreterLock)是众所周知的。\n",
    "在深度学习环境中，我们担心速度极快的GPU可能要等到CPU运行Python代码后才能运行另一个作业。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyWR4h3DyStd",
    "origin_pos": 40
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 层也是块。\n",
    "* 一个块可以由许多层组成。\n",
    "* 一个块可以由许多块组成。\n",
    "* 块可以包含代码。\n",
    "* 块负责大量的内部处理，包括参数初始化和反向传播。\n",
    "* 层和块的顺序连接由`Sequential`块处理。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 如果将`MySequential`中存储块的方式更改为Python列表，会出现什么样的问题？\n",
    "1. 实现一个块，它以两个块为参数，例如`net1`和`net2`，并返回正向传播中两个网络的串联输出。这也被称为平行块。\n",
    "1. 假设你想要连接同一网络的多个实例。实现一个工厂函数，该函数生成同一个块的多个实例，并在此基础上构建更大的网络。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VCg6m0P3Zh7",
    "origin_pos": 0
   },
   "source": [
    "# 参数管理\n",
    "\n",
    "一旦我们选择了架构并设置了超参数，我们就进入了训练阶段。此时，我们的目标是找到使损失函数最小化的参数值。经过训练后，我们将需要使用这些参数来做出未来的预测。此外，有时我们希望提取参数，以便在其他环境中复用它们，将模型保存到磁盘，以便它可以在其他软件中执行，或者为了获得科学的理解而进行检查。\n",
    "\n",
    "大多数情况下，我们可以忽略声明和操作参数的具体细节，而只依靠深度学习框架来完成繁重的工作。然而，当我们离开具有标准层的层叠架构时，我们有时会陷入声明和操作参数的麻烦中。在本节中，我们将介绍以下内容：\n",
    "\n",
    "* 访问参数，用于调试、诊断和可视化。\n",
    "* 参数初始化。\n",
    "* 在不同模型组件间共享参数。\n",
    "\n",
    "(**我们首先关注具有单隐藏层的多层感知机。**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6Ze85-4n3Zh9",
    "origin_pos": 2,
    "outputId": "531bbf6c-0c3d-4e69-e070-b62471ae1b0b",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0756],\n",
       "        [0.2275]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNEmQdxp3Zh-",
    "origin_pos": 4
   },
   "source": [
    "## [**参数访问**]\n",
    "\n",
    "我们从已有模型中访问参数。当通过`Sequential`类定义模型时，我们可以通过索引来访问模型的任意层。这就像模型是一个列表一样。每层的参数都在其属性中。如下所示，我们可以检查第二个全连接层的参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].state_dict()['weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zvGou8-23Zh-",
    "origin_pos": 6,
    "outputId": "d3e1f86a-8b58-472f-8a3d-58ae3822c8dd",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-0.0066, -0.2203,  0.0657,  0.1408, -0.1654, -0.3192,  0.0481,  0.2302]])), ('bias', tensor([0.1673]))])\n"
     ]
    }
   ],
   "source": [
    "print(net[2].state_dict())   # 返回模型的所有参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWtn3h773Zh-",
    "origin_pos": 8
   },
   "source": [
    "输出的结果告诉我们一些重要的事情。首先，这个全连接层包含两个参数，分别是该层的权重和偏置。两者都存储为单精度浮点数（float32）。注意，参数名称允许我们唯一地标识每个参数，即使在包含数百个层的网络中也是如此。\n",
    "\n",
    "### [**目标参数**]\n",
    "\n",
    "注意，每个参数都表示为参数（parameter）类的一个实例。要对参数执行任何操作，首先我们需要访问底层的数值。有几种方法可以做到这一点。有些比较简单，而另一些则比较通用。下面的代码从第二个神经网络层提取偏置，提取后返回的是一个参数类实例，并进一步访问该参数的值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WvpHie2C3Zh-",
    "origin_pos": 10,
    "outputId": "acd2af19-fc4b-4c91-bdab-e219aca50934",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.1673], requires_grad=True)\n",
      "tensor([0.1673])\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lybJz2iX3Zh_",
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "参数是复合的对象，包含值、梯度和额外信息。这就是为什么我们需要显式请求值的原因。\n",
    "\n",
    "除了值之外，我们还可以访问每个参数的梯度。由于我们还没有调用这个网络的反向传播，所以参数的梯度处于初始状态。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGxwWa-23Zh_",
    "origin_pos": 14,
    "outputId": "067a849a-8a87-4b87-cb08-d2456b88c178",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.grad == None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzLosjqI3Zh_",
    "origin_pos": 15
   },
   "source": [
    "### [**一次性访问所有参数**]\n",
    "\n",
    "当我们需要对所有参数执行操作时，逐个访问它们可能会很麻烦。当我们处理更复杂的块（例如，嵌套块）时，情况可能会变得特别复杂，因为我们需要递归整个树来提取每个子块的参数。下面，我们将通过演示来比较访问第一个全连接层的参数和访问所有层。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight', tensor([[ 0.0737, -0.0404, -0.0090, -0.2180],\n",
       "                      [-0.0353,  0.3339, -0.4957, -0.0064],\n",
       "                      [ 0.0721, -0.1505, -0.3756, -0.4752],\n",
       "                      [-0.2889, -0.0597, -0.1240,  0.0495],\n",
       "                      [-0.1823,  0.0118, -0.2763,  0.4452],\n",
       "                      [-0.2104,  0.1607,  0.1616, -0.4345],\n",
       "                      [ 0.2921,  0.4860,  0.1366, -0.0047],\n",
       "                      [ 0.2925,  0.3534, -0.0288,  0.3316]])),\n",
       "             ('0.bias',\n",
       "              tensor([-0.4112,  0.1462, -0.0170, -0.1234,  0.0369,  0.4674,  0.2377,  0.0980])),\n",
       "             ('2.weight',\n",
       "              tensor([[-0.0066, -0.2203,  0.0657,  0.1408, -0.1654, -0.3192,  0.0481,  0.2302]])),\n",
       "             ('2.bias', tensor([0.1673]))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0.weight', Parameter containing:\n",
       "  tensor([[ 0.0737, -0.0404, -0.0090, -0.2180],\n",
       "          [-0.0353,  0.3339, -0.4957, -0.0064],\n",
       "          [ 0.0721, -0.1505, -0.3756, -0.4752],\n",
       "          [-0.2889, -0.0597, -0.1240,  0.0495],\n",
       "          [-0.1823,  0.0118, -0.2763,  0.4452],\n",
       "          [-0.2104,  0.1607,  0.1616, -0.4345],\n",
       "          [ 0.2921,  0.4860,  0.1366, -0.0047],\n",
       "          [ 0.2925,  0.3534, -0.0288,  0.3316]], requires_grad=True)),\n",
       " ('0.bias', Parameter containing:\n",
       "  tensor([-0.4112,  0.1462, -0.0170, -0.1234,  0.0369,  0.4674,  0.2377,  0.0980],\n",
       "         requires_grad=True)),\n",
       " ('2.weight', Parameter containing:\n",
       "  tensor([[-0.0066, -0.2203,  0.0657,  0.1408, -0.1654, -0.3192,  0.0481,  0.2302]],\n",
       "         requires_grad=True)),\n",
       " ('2.bias', Parameter containing:\n",
       "  tensor([0.1673], requires_grad=True))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(net.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZB-l-Vg53ZiA",
    "origin_pos": 17,
    "outputId": "8a9994c8-670f-4a96-d9a3-89d516023042",
    "scrolled": true,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object <genexpr> at 0x000001E8C6387EB8>\n",
      "<generator object <genexpr> at 0x000001E8C6387C50>\n"
     ]
    }
   ],
   "source": [
    "print((name, param.shape) for name, param in net[0].named_parameters())\n",
    "print((name, param.shape) for name, param in net.named_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_piKo2Ob3ZiA",
    "origin_pos": 19
   },
   "source": [
    "这为我们提供了另一种访问网络参数的方式，如下所示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4RDtJprT3ZiA",
    "origin_pos": 21,
    "outputId": "c94c0e76-fc78-4541-c0bc-c8e9fe9ffce6",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0959])"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['2.bias'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0sA7fDg3ZiA",
    "origin_pos": 23
   },
   "source": [
    "### [**从嵌套块收集参数**]\n",
    "\n",
    "让我们看看，如果我们将多个块相互嵌套，参数命名约定是如何工作的。为此，我们首先定义一个生成块的函数（可以说是块工厂），然后将这些块组合到更大的块中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKSpjBmd3ZiA",
    "origin_pos": 25,
    "outputId": "ce3dbc11-dff7-4304-c4d0-e6ee284732f4",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0351],\n",
       "        [0.0351]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 4),\n",
    "                         nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        # 在这里嵌套\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a18DacpH3ZiB",
    "origin_pos": 27
   },
   "source": [
    "现在[**我们已经设计了网络，让我们看看它是如何组织的。**]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UM7dXhaq3ZiB",
    "origin_pos": 29,
    "outputId": "ab670a36-f808-4f83-f58d-1e3d4765e8c7",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sg7NQ8pi3ZiB",
    "origin_pos": 31
   },
   "source": [
    "因为层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们。例如，我们下面访问第一个主要的块，其中第二个子块的第一层的偏置项。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aO6H7lzS3ZiB",
    "origin_pos": 33,
    "outputId": "3f9df672-de72-47c7-ef00-9c73b71c39ab",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4900, -0.4889,  0.3830, -0.0363,  0.2097, -0.0641, -0.1053, -0.3248])"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet[0][1][0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psf_zzn83ZiB",
    "origin_pos": 35
   },
   "source": [
    "## 参数初始化\n",
    "\n",
    "我们知道了如何访问参数，现在让我们看看如何正确地初始化参数。我们在 :numref:`sec_numerical_stability` 中讨论了良好初始化的必要性。深度学习框架提供默认随机初始化。然而，我们经常希望根据其他规则初始化权重。深度学习框架提供了最常用的规则，也允许创建自定义初始化方法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n25tTE1m3ZiC",
    "origin_pos": 37,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "默认情况下，PyTorch会根据一个范围均匀地初始化权重和偏置矩阵，这个范围是根据输入和输出维度计算出的。PyTorch的`nn.init`模块提供了多种预置初始化方法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShKuRTo53ZiC",
    "origin_pos": 39
   },
   "source": [
    "### [**内置初始化**]\n",
    "\n",
    "让我们首先调用内置的初始化器。下面的代码将所有权重参数初始化为标准差为0.01的高斯随机变量，且将偏置参数设置为0。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PqdFmiag3ZiC",
    "origin_pos": 41,
    "outputId": "9feb4fad-7b24-4e19-bc45-b3c47a4a97ab",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0104,  0.0022, -0.0064, -0.0095]), tensor(0.))"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQmpxpyP3ZiC",
    "origin_pos": 43
   },
   "source": [
    "我们还可以将所有参数初始化为给定的常数（比如1）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KANAg3ae3ZiC",
    "origin_pos": 45,
    "outputId": "15cd2842-3050-4ebb-8959-0bddd9be0246",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TReXtbe03ZiD",
    "origin_pos": 47
   },
   "source": [
    "我们还可以[**对某些块应用不同的初始化方法**]。例如，下面我们使用Xavier初始化方法初始化第一层，然后第二层初始化为常量值42。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kldlgFJq3ZiD",
    "origin_pos": 49,
    "outputId": "8bbab14d-aff3-40e2-ee92-c0c75b2995e3",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1484,  0.2210,  0.3737, -0.3520])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "def xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "\n",
    "net[0].apply(xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAF_lAoW3ZiD",
    "origin_pos": 51
   },
   "source": [
    "### [**自定义初始化**]\n",
    "\n",
    "有时，深度学习框架没有提供我们需要的初始化方法。在下面的例子中，我们使用以下的分布为任意权重参数$w$定义初始化方法：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    w \\sim \\begin{cases}\n",
    "        U(5, 10) & \\text{ with probability } \\frac{1}{4} \\\\\n",
    "            0    & \\text{ with probability } \\frac{1}{2} \\\\\n",
    "        U(-10, -5) & \\text{ with probability } \\frac{1}{4}\n",
    "    \\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMFOsAQ83ZiD",
    "origin_pos": 53,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "同样，我们实现了一个`my_init`函数来应用到`net`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "MAtUJWBW3ZiD",
    "origin_pos": 56,
    "outputId": "1fafe4d7-1da3-447b-9a78-aca35aa5c2e8",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([8, 4])\n",
      "Init weight torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.7353, -0.0000, -9.6500,  7.8387],\n",
       "        [-8.5784, -6.1520,  0.0000,  0.0000]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\n",
    "            \"Init\",\n",
    "            *[(name, param.shape) for name, param in m.named_parameters()][0])\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQnszRdM3ZiD",
    "origin_pos": 58
   },
   "source": [
    "注意，我们始终可以直接设置参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4AFXtL093ZiE",
    "origin_pos": 60,
    "outputId": "3bda974b-3e10-4a05-9c19-da1f930f23f1",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0, 0] = 42\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CS61UYGY3ZiE",
    "origin_pos": 63
   },
   "source": [
    "## [**参数绑定**]\n",
    "\n",
    "有时我们希望在多个层间共享参数。让我们看看如何优雅地做这件事。在下面，我们定义一个稠密层，然后使用它的参数来设置另一个层的参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ODCBu9Vw3ZiE",
    "origin_pos": 65,
    "outputId": "0d8bed4b-c28e-4139-848d-60a879a3405c",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# 我们需要给共享层一个名称，以便可以引用它的参数。\n",
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), shared, nn.ReLU(), shared,\n",
    "                    nn.ReLU(), nn.Linear(8, 1))\n",
    "net(X)\n",
    "# 检查参数是否相同\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "# 确保它们实际上是同一个对象，而不只是有相同的值。\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GUrhNbc3ZiE",
    "origin_pos": 67,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "这个例子表明第二层和第三层的参数是绑定的。它们不仅值相等，而且由相同的张量表示。因此，如果我们改变其中一个参数，另一个参数也会改变。你可能会想，当参数绑定时，梯度会发生什么情况？答案是由于模型参数包含梯度，因此在反向传播期间第二个隐藏层和第三个隐藏层的梯度会加在一起。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkUd3Ewg3ZiE",
    "origin_pos": 68
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 我们有几种方法可以访问、初始化和绑定模型参数。\n",
    "* 我们可以使用自定义初始化方法。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 使用 :numref:`sec_model_construction` 中定义的`FancyMLP`模型，访问各个层的参数。\n",
    "1. 查看初始化模块文档以了解不同的初始化方法。\n",
    "1. 构建包含共享参数层的多层感知机并对其进行训练。在训练过程中，观察模型各层的参数和梯度。\n",
    "1. 为什么共享参数是个好主意？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwHAmgNs-OsK",
    "origin_pos": 0
   },
   "source": [
    "# 自定义层\n",
    "\n",
    "深度学习成功背后的一个因素是，可以用创造性的方式组合广泛的层，从而设计出适用于各种任务的结构。例如，研究人员发明了专门用于处理图像、文本、序列数据和执行动态编程的层。早晚有一天，你会遇到或要自己发明一个在深度学习框架中还不存在的层。在这些情况下，你必须构建自定义层。在本节中，我们将向你展示如何操作。\n",
    "\n",
    "## 不带参数的层\n",
    "\n",
    "首先，我们(**构造一个没有任何参数的自定义层**)。如果你还记得我们在 :numref:`sec_model_construction` 对块的介绍，这应该看起来很眼熟。下面的`CenteredLayer`类要从其输入中减去均值。要构建它，我们只需继承基础层类并实现正向传播功能。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_z7mIdl-OsM",
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X - X.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppMdEso0-OsN",
    "origin_pos": 4
   },
   "source": [
    "让我们通过向其提供一些数据来验证该层是否按预期工作。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dyltx-Ez-OsN",
    "origin_pos": 6,
    "outputId": "0c1b49a5-ed82-4b95-9868-a7b5a54a8e9f",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = CenteredLayer()\n",
    "layer(torch.FloatTensor([1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0j5C6kcm-OsO",
    "origin_pos": 8
   },
   "source": [
    "现在，我们可以[**将层作为组件合并到构建更复杂的模型中**]。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hTX8nJX-OsO",
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCDg5U5V-OsO",
    "origin_pos": 12
   },
   "source": [
    "作为额外的健全性检查，我们可以向网络发送随机数据后，检查均值是否为0。由于我们处理的是浮点数，因为存储精度的原因，我们仍然可能会看到一个非常小的非零数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hupOl7Gw-OsP",
    "origin_pos": 14,
    "outputId": "0f1078a4-eae2-4e2e-9b02-edb9a23786b9",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.8626e-09, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = net(torch.rand(4, 8))\n",
    "Y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syZ2zQwP-OsP",
    "origin_pos": 16
   },
   "source": [
    "## [**带参数的层**]\n",
    "\n",
    "既然我们知道了如何定义简单的层，那么让我们继续定义具有参数的层，这些参数可以通过训练进行调整。我们可以使用内置函数来创建参数，这些函数提供一些基本的管理功能。比如管理访问、初始化、共享、保存和加载模型参数。这样做的好处之一是，我们不需要为每个自定义层编写自定义序列化程序。\n",
    "\n",
    "现在，让我们实现自定义版本的全连接层。回想一下，该层需要两个参数，一个用于表示权重，另一个用于表示偏置项。在此实现中，我们使用ReLU作为激活函数。该层需要输入参数：`in_units`和`units`，分别表示输入和输出的数量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etMGGSkC-OsP",
    "origin_pos": 18,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_units, units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units, units))\n",
    "        self.bias = nn.Parameter(torch.randn(units,))\n",
    "\n",
    "    def forward(self, X):\n",
    "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
    "        return F.relu(linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFfL5SzS-OsP",
    "origin_pos": 21,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "接下来，我们实例化`MyLinear`类并访问其模型参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0yMUPo6-OsQ",
    "origin_pos": 23,
    "outputId": "ef1e7145-a195-47c3-f553-66856372e324",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1476, -1.2550,  0.0803],\n",
       "        [-0.8999, -1.3699,  0.4572],\n",
       "        [-0.1212,  0.2888, -0.7945],\n",
       "        [ 0.7072,  0.4077,  0.6760],\n",
       "        [-0.3221,  1.4658,  0.8462]], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = MyLinear(5, 3)\n",
    "linear.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1HkgoWF-OsQ",
    "origin_pos": 25
   },
   "source": [
    "我们可以[**使用自定义层直接执行正向传播计算**]。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CxI94iz1-OsQ",
    "origin_pos": 27,
    "outputId": "8a0a71bc-1813-4fd6-c291-b5bf53b5bf1e",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 3.1955, 0.0000],\n",
       "        [0.0000, 1.7145, 0.0637]])"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(torch.rand(2, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0AMN3ce-OsQ",
    "origin_pos": 29
   },
   "source": [
    "我们还可以(**使用自定义层构建模型**)。我们可以像使用内置的全连接层一样使用自定义层。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5w2OOzOW-OsQ",
    "origin_pos": 31,
    "outputId": "23b3df2d-a9b0-46c8-a640-214d35241c6d",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))\n",
    "net(torch.rand(2, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmlZFv0W-OsR",
    "origin_pos": 33
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 我们可以通过基本层类设计自定义层。这允许我们定义灵活的新层，其行为与库中的任何现有层不同。\n",
    "* 在自定义层定义完成后，就可以在任意环境和网络结构中调用该自定义层。\n",
    "* 层可以有局部参数，这些参数可以通过内置函数创建。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 设计一个接受输入并计算张量降维的层，它返回$y_k = \\sum_{i, j} W_{ijk} x_i x_j$。\n",
    "1. 设计一个返回输入数据的傅立叶系数前半部分的层。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwDcD8l1-O5h",
    "origin_pos": 0
   },
   "source": [
    "# 读写文件\n",
    "\n",
    "到目前为止，我们讨论了如何处理数据，以及如何构建、训练和测试深度学习模型。然而，有时我们对所学的模型足够满意，我们希望保存训练的模型以备将来在各种环境中使用（甚至可能在部署中进行预测）。此外，当运行一个耗时较长的训练过程时，最佳的做法是定期保存中间结果（检查点），以确保在服务器电源被不小心断掉时不会损失几天的计算结果。因此，现在是时候学习如何加载和存储权重向量和整个模型。本节将讨论这些问题。\n",
    "\n",
    "## (**加载和保存张量**)\n",
    "\n",
    "对于单个张量，我们可以直接调用`load`和`save`函数分别读写它们。这两个函数都要求我们提供一个名称，`save`要求将要保存的变量作为输入。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQpqYi9d-O5i",
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "x = torch.arange(4)\n",
    "torch.save(x, 'x-file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09onRnm1-O5j",
    "origin_pos": 4
   },
   "source": [
    "我们现在可以将存储在文件中的数据读回内存。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YbqCEAAi-O5j",
    "origin_pos": 6,
    "outputId": "cf0fd2b1-fdc9-46bf-fee2-11c8ae0e9ffe",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = torch.load('x-file')\n",
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAuU_tDZ-O5k",
    "origin_pos": 8
   },
   "source": [
    "我们可以[**存储一个张量列表，然后把它们读回内存。**]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lOl9WbLw-O5k",
    "origin_pos": 10,
    "outputId": "9a3fa6bb-2661-4cc0-f0e7-4a17d95b9127",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.zeros(4)\n",
    "torch.save([x, y], 'x-files')\n",
    "x2, y2 = torch.load('x-files')\n",
    "(x2, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhUh68Fq-O5k",
    "origin_pos": 12
   },
   "source": [
    "我们甚至可以(**写入或读取从字符串映射到张量的字典**)。当我们要读取或写入模型中的所有权重时，这很方便。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LLskEklD-O5l",
    "origin_pos": 14,
    "outputId": "ca2b986f-9240-4b3e-922d-0d450b28e2ee",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict = {'x': x, 'y': y}\n",
    "torch.save(mydict, 'mydict')\n",
    "mydict2 = torch.load('mydict')\n",
    "mydict2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUmDTW3g-O5l",
    "origin_pos": 16
   },
   "source": [
    "## [**加载和保存模型参数**]\n",
    "\n",
    "保存单个权重向量（或其他张量）确实是有用的，但是如果我们想保存整个模型，并在以后加载它们，单独保存每个向量则会变得很麻烦。毕竟，我们可能有数百个参数散布在各处。因此，深度学习框架提供了内置函数来保存和加载整个网络。需要注意的一个重要细节是，这将保存模型的参数而不是保存整个模型。例如，如果我们有一个3层多层感知机，我们需要单独指定结构。因为模型本身可以包含任意代码，所以模型本身难以序列化。因此，为了恢复模型，我们需要用代码生成结构，然后从磁盘加载参数。让我们从熟悉的多层感知机开始尝试一下。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHxP8jiY-O5l",
    "origin_pos": 18,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.output = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "\n",
    "net = MLP()\n",
    "X = torch.randn(size=(2, 20))\n",
    "Y = net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bmZ7vl5-O5m",
    "origin_pos": 20
   },
   "source": [
    "接下来，我们[**将模型的参数存储为一个叫做“mlp.params”的文件。**]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKVKJ8Gx-O5m",
    "origin_pos": 22,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'mlp.params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6M2CEL--O5m",
    "origin_pos": 24
   },
   "source": [
    "为了恢复模型，我们[**实例化了原始多层感知机模型的一个备份。**]我们没有随机初始化模型参数，而是(**直接读取文件中存储的参数。**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSxBBG4p-O5m",
    "origin_pos": 26,
    "outputId": "459c114b-36bb-4689-ac69-be873b2e3aff",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load('mlp.params'))\n",
    "clone.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6AetvrR-O5m",
    "origin_pos": 28
   },
   "source": [
    "由于两个实例具有相同的模型参数，在输入相同的`X`时，两个实例的计算结果应该相同。让我们来验证一下。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8JJ7Okc-O5m",
    "origin_pos": 30,
    "outputId": "0feeeadf-575b-4ca8-92df-49891bc4c042",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_clone = clone(X)\n",
    "Y_clone == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJEuOjex-O5n",
    "origin_pos": 32
   },
   "source": [
    "## 小结\n",
    "\n",
    "* `save`和`load`函数可用于张量对象的文件读写。\n",
    "* 我们可以通过参数字典保存和加载网络的全部参数。\n",
    "* 保存结构必须在代码中完成，而不是在参数中完成。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 即使不需要将经过训练的模型部署到不同的设备上，存储模型参数还有什么实际的好处？\n",
    "1. 假设我们只想复用网络的一部分，以将其合并到不同的网络结构中。比如说，如果你想在一个新的网络中使用之前网络的前两层，你该怎么做？\n",
    "1. 如何同时保存网络结构和参数？你会对结构加上什么限制？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hu8ZARj--Pnw",
    "origin_pos": 0
   },
   "source": [
    "# GPU\n",
    ":label:`sec_use_gpu`\n",
    "\n",
    "在 :numref:`tab_intro_decade` 中，我们讨论了过去20年中计算能力的快速增长。简而言之，自2000年以来，GPU性能每十年增长1000倍。这提供了巨大的机会，但也表明需要提供这样的性能。\n",
    "\n",
    "在本节中，我们开始讨论如何利用这种计算性能进行研究。首先是使用单个GPU，然后是如何使用多个GPU和多个服务器（具有多个GPU）。\n",
    "\n",
    "具体来说，我们将讨论如何使用单个NVIDIA GPU进行计算。首先，确保至少安装了一个NVIDIA GPU。然后，下载[NVIDIA驱动和CUDA](https://developer.nvidia.com/cuda-downloads)并按照提示设置适当的路径。当这些准备工作完成，就可以使用`nvidia-smi`命令来(**查看显卡信息。**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "y237who0-Pny",
    "origin_pos": 1,
    "outputId": "f4fa6a3e-650c-4cd7-c711-fd768e5b5dd5",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 14 17:03:17 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 511.79       Driver Version: 511.79       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   35C    P8    19W / 220W |    913MiB /  8192MiB |      6%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1284    C+G   ...8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "|    0   N/A  N/A      1484    C+G                                   N/A      |\n",
      "|    0   N/A  N/A      4204    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A      8240    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      8372    C+G   ...er_engine\\wallpaper64.exe    N/A      |\n",
      "|    0   N/A  N/A      8660    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      8700    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      9552    C+G   ...ge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     10812    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A     11316    C+G   ...CloudMusic\\cloudmusic.exe    N/A      |\n",
      "|    0   N/A  N/A     11448    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     13276    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXs5tm2B-Pnz",
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "在PyTorch中，每个数组都有一个设备（device），我们通常将其称为上下文（context）。到目前为止，默认情况下，所有变量和相关的计算都分配给CPU。有时上下文可能是GPU。当我们跨多个服务器部署作业时，事情会变得更加棘手。通过智能地将数组分配给上下文，我们可以最大限度地减少在设备之间传输数据的时间。例如，当在带有GPU的服务器上训练神经网络时，我们通常希望模型的参数在GPU上。\n",
    "\n",
    "接下来，我们需要确认安装了GPU版本的PyTorch。如果已经安装了PyTorch的CPU版本，我们需要先卸载它。例如，使用`pip uninstall torch`命令，然后根据你的CUDA版本安装相应的PyTorch版本。假设你安装了CUDA10.0，你可以通过`pip install torch-cu100`安装支持CUDA10.0的PyTorch版本。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMiunYah-Pnz",
    "origin_pos": 4
   },
   "source": [
    "要运行此部分中的程序，至少需要两个GPU。注意，对于大多数桌面计算机来说，这可能是奢侈的，但在云中很容易获得，例如，通过使用AWS EC2的多GPU实例。本书的其他章节大都不需要多个GPU，所以本节只是为了展示数据如何在不同的设备之间传递。\n",
    "\n",
    "## [**计算设备**]\n",
    "\n",
    "我们可以指定用于存储和计算的设备，如CPU和GPU。默认情况下，张量是在内存中创建的，然后使用CPU计算它。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmWQc6SH-Pnz",
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "在PyTorch中，CPU和GPU可以用`torch.device('cpu')`和`torch.cuda.device('cuda')`表示。应该注意的是，`cpu`设备意味着所有物理CPU和内存。这意味着PyTorch的计算将尝试使用所有CPU核心。然而，`gpu`设备只代表一个卡和相应的显存。如果有多个GPU，我们使用`torch.cuda.device(f'cuda:{i}')`来表示第$i$块GPU（$i$从0开始）。另外，`cuda:0`和`cuda`是等价的。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "l0uLmaEZ-Pn0",
    "origin_pos": 8,
    "outputId": "fea5429b-a6f3-4945-e152-34ccff093be0",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'),\n",
       " <torch.cuda.device at 0x1aa4ae8a860>,\n",
       " <torch.cuda.device at 0x1aa4ae8a828>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.device('cpu'), torch.cuda.device('cuda'), torch.cuda.device('cuda:1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvz-bFDM-Pn0",
    "origin_pos": 10
   },
   "source": [
    "我们可以(**查询可用gpu的数量。**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dgsZPSz--Pn0",
    "origin_pos": 12,
    "outputId": "f0081027-6f7a-4bc4-f1e9-95dbaf35cebb",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3aj9wOK-Pn1",
    "origin_pos": 14
   },
   "source": [
    "现在我们定义了两个方便的函数，[**这两个函数允许我们在请求的GPU不存在的情况下运行代码。**]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5Plosqa0-Pn1",
    "origin_pos": 16,
    "outputId": "589ccc2a-57d2-4bcb-ca74-7fa74f451fbc",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0),\n",
       " device(type='cpu'),\n",
       " [device(type='cuda', index=0)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def try_gpu(i=0):  #@save\n",
    "    \"\"\"如果存在，则返回gpu(i)，否则返回cpu()。\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "def try_all_gpus():  #@save\n",
    "    \"\"\"返回所有可用的GPU，如果没有GPU，则返回[cpu(),]。\"\"\"\n",
    "    devices = [\n",
    "        torch.device(f'cuda:{i}') for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else [torch.device('cpu')]\n",
    "\n",
    "try_gpu(), try_gpu(10), try_all_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bw5LDaU0-Pn1",
    "origin_pos": 18
   },
   "source": [
    "## 张量与gpu\n",
    "\n",
    "默认情况下，张量是在CPU上创建的。我们可以[**查询张量所在的设备。**]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90N_bq3z-Pn1",
    "origin_pos": 20,
    "outputId": "948d8801-a9ee-46cb-94cb-7ed6338fac6c",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "x.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jToFJwJo-Pn2",
    "origin_pos": 22
   },
   "source": [
    "需要注意的是，无论何时我们要对多个项进行操作，它们都必须在同一个设备上。例如，如果我们对两个张量求和，我们需要确保两个张量都位于同一个设备上，否则框架将不知道在哪里存储结果，甚至不知道在哪里执行计算。\n",
    "\n",
    "### [**存储在GPU上**]\n",
    "\n",
    "有几种方法可以在GPU上存储张量。例如，我们可以在创建张量时指定存储设备。接下来，我们在第一个`gpu`上创建张量变量`X`。在GPU上创建的张量只消耗这个GPU的显存。我们可以使用`nvidia-smi`命令查看显存使用情况。一般来说，我们需要确保不创建超过GPU显存限制的数据。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZMHCXsS-Pn2",
    "origin_pos": 24,
    "outputId": "65e290b6-a709-480b-89ba-d4726ca734fe",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones(2, 3, device=try_gpu())\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tG6DDui-Pn2",
    "origin_pos": 26
   },
   "source": [
    "假设你至少有两个GPU，下面的代码将在(**第二个GPU上创建一个随机张量。**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KVKDGtso-Pn2",
    "origin_pos": 28,
    "outputId": "b8581653-f656-49de-8feb-36959bb56376",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1611, 0.1692, 0.4031],\n",
       "        [0.0282, 0.7614, 0.4806]], device='cuda:1')"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.rand(2, 3, device=try_gpu(1))\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YH42U0ej-Pn2",
    "origin_pos": 30
   },
   "source": [
    "### 复制\n",
    "\n",
    "如果我们[**要计算`X + Y`，我们需要决定在哪里执行这个操作**]。例如，如 :numref:`fig_copyto` 所示，我们可以将`X`传输到第二个GPU并在那里执行操作。\n",
    "*不要*简单地`X`加上`Y`，\n",
    "因为这会导致异常。运行时引擎不知道该怎么做：它在同一设备上找不到数据会导致失败。由于`Y`位于第二个GPU上，所以我们需要将`X`移到那里，然后才能执行相加运算。\n",
    "\n",
    "![复制数据以在同一设备上执行操作。](http://d2l.ai/_images/copyto.svg)\n",
    ":label:`fig_copyto`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_QlvTFNm-Pn3",
    "origin_pos": 32,
    "outputId": "1f957e39-efc6-44d9-996a-00ad2be6eaab",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], device='cuda:0')\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "Z = X.cuda(1)\n",
    "print(X)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vr0ajQBJ-Pn3",
    "origin_pos": 34
   },
   "source": [
    "[**现在数据在同一个GPU上（`Z`和`Y`都在），我们可以将它们相加。**]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ztoFRH-m-Pn3",
    "origin_pos": 35,
    "outputId": "67a5f694-b4ee-47a0-861c-ad34ead93ec2",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1611, 1.1692, 1.4031],\n",
       "        [1.0282, 1.7614, 1.4806]], device='cuda:1')"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y + Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o08z9GKl-Pn3",
    "origin_pos": 37,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "假设变量`Z`已经存在于第二个GPU上。如果我们还是调用`Z.cuda(1)`怎么办？它将返回`Z`，而不会复制并分配新内存。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fP4VQp5-Pn3",
    "origin_pos": 40,
    "outputId": "cc5de6f0-cbdd-411f-ea2b-8d81ef2d0e13",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z.cuda(1) is Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHhMqzkq-Pn4",
    "origin_pos": 42
   },
   "source": [
    "### 旁注\n",
    "\n",
    "人们使用GPU来进行机器学习，因为他们希望运行速度快。但是在设备之间传输变量是缓慢的。所以我们希望你百分之百确定你想做一些缓慢的事情。如果深度学习框架只是自动复制而没有崩溃，那么你可能不会意识到你已经编写了一些缓慢的代码。\n",
    "\n",
    "此外，在设备（CPU、GPU和其他机器）之间传输数据比计算慢得多。这也使得并行化变得更加困难，因为我们必须等待数据被发送（或者接收），然后才能继续进行更多的操作。这就是为什么拷贝操作要格外小心。根据经验，多个小操作比一个大操作糟糕得多。此外，除非你知道自己在做什么，否则，一次执行几个操作比代码中散布的许多单个操作要好得多。如果一个设备必须等待另一个设备才能执行其他操作，那么这样的操作可能会阻塞。这有点像排队订购咖啡，而不像通过电话预先订购时，当你在的时候发现咖啡已经准备好了。\n",
    "\n",
    "最后，当我们打印张量或将张量转换为NumPy格式时，如果数据不在内存中，框架会首先将其复制到内存中，这会导致额外的传输开销。更糟糕的是，它现在受制于可怕的全局解释器锁，这使得一切都得等待Python完成。\n",
    "\n",
    "## [**神经网络与GPU**]\n",
    "\n",
    "类似地，神经网络模型可以指定设备。下面的代码将模型参数放在GPU上。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07_QIdfe-Pn4",
    "origin_pos": 44,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(3, 1))\n",
    "net = net.to(device=try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5jWTEJ7-Pn4",
    "origin_pos": 46
   },
   "source": [
    "在接下来的几章中，我们将看到更多关于如何在GPU上运行模型的例子，因为它们将变得更加计算密集。\n",
    "\n",
    "当输入为GPU上的张量时，模型将在同一GPU上计算结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QiLLD2-4-Pn4",
    "origin_pos": 47,
    "outputId": "d110a90d-fbc7-4080-ed01-ed3435c17be5",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2346],\n",
       "        [0.2346]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzSa2xe3-Pn4",
    "origin_pos": 48
   },
   "source": [
    "让我们(**确认模型参数存储在同一个GPU上。**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PrY_xJ76-Pn4",
    "origin_pos": 50,
    "outputId": "59f98f5c-a7ab-45cb-d128-3a0adaf78ec0",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68a9aNBO-Pn5",
    "origin_pos": 52
   },
   "source": [
    "总之，只要所有的数据和参数都在同一个设备上，我们就可以有效地学习模型。在下面的章节中，我们将看到几个这样的例子。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 我们可以指定用于存储和计算的设备，例如CPU或GPU。默认情况下，数据在主内存中创建，然后使用CPU进行计算。\n",
    "* 深度学习框架要求计算的所有输入数据都在同一设备上，无论是CPU还是GPU。\n",
    "* 不经意地移动数据可能会显著降低性能。一个典型的错误如下：计算GPU上每个小批量的损失，并在命令行中将其报告给用户（或将其记录在NumPy `ndarray`中）时，将触发全局解释器锁，从而使所有GPU阻塞。最好是为GPU内部的日志分配内存，并且只移动较大的日志。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 尝试一个更大的计算任务，比如大矩阵的乘法，看看CPU和GPU之间的速度差异。一个计算量很小的任务呢？\n",
    "1. 我们应该如何在GPU上读写模型参数？\n",
    "1. 测量计算1000个$100 \\times 100$矩阵的矩阵乘法所需的时间，并记录输出矩阵的弗罗贝尼乌斯范数，一次记录一个结果，而不是在GPU上保存日志并仅传输最终结果。\n",
    "1. 测量同时在两个GPU上执行两个矩阵乘法与在一个GPU上按顺序执行两个矩阵乘法所需的时间。提示：你应该看到近乎线性的缩放。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xq6RWQTnySte",
    "origin_pos": 42,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/1827)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model-construction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "289.348px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
