{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d3c552b",
   "metadata": {},
   "source": [
    "# 数据操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8717db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937eb25d",
   "metadata": {},
   "source": [
    "## 创建向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ec5191",
   "metadata": {},
   "source": [
    "**torch.arange()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c11cbca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f14011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(12).reshape(3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3f4060",
   "metadata": {},
   "source": [
    "**torch.zeros()**，**torch.ones()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99e73c72",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(2,3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b3a8d7",
   "metadata": {},
   "source": [
    "**torch.randn()**：从均值为0、标准差为1的标准高斯（正态）分布中随机采样 <br>\n",
    "**torch.rand()**：从均匀分布中随机采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "595035ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6535,  0.4905,  0.8729, -0.1932],\n",
       "        [ 1.1841,  0.7583, -0.7591,  0.2500],\n",
       "        [ 1.8051, -0.1812,  0.1279,  0.5847]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e735812",
   "metadata": {},
   "source": [
    "## 运算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4608bed",
   "metadata": {},
   "source": [
    "**加减乘除，幂次**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00c2e9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "x + y, x - y, x * y, x / y, x**y  # **运算符是求幂运算\n",
    "torch.exp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313d5d32",
   "metadata": {},
   "source": [
    "**torch.cat**：连接张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ed56646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [ 2.,  1.,  4.,  3.],\n",
       "         [ 1.,  2.,  3.,  4.],\n",
       "         [ 4.,  3.,  2.,  1.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12, dtype=torch.float32).reshape((3, 4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d38271d",
   "metadata": {},
   "source": [
    "## 向量信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14f92f8",
   "metadata": {},
   "source": [
    "**numel()**：元素总数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6270b043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(12).numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1d4811",
   "metadata": {},
   "source": [
    "## 转换为Python 对象"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1c5930",
   "metadata": {},
   "source": [
    "**numpy()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6134f98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, torch.Tensor)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = X.numpy()\n",
    "B = torch.tensor(A)\n",
    "type(A), type(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028ca425",
   "metadata": {},
   "source": [
    "# 线性代数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8335681",
   "metadata": {
    "id": "wJIRf0bb58Md",
    "origin_pos": 0
   },
   "source": [
    "用 $\\mathbb{R}$ 表示所有（连续）*实数* 标量的空间。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2976c7",
   "metadata": {
    "id": "VkY0VMsg58Mf",
    "origin_pos": 4
   },
   "source": [
    "## 向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60265200",
   "metadata": {
    "id": "y2pNqOCL58Mg",
    "origin_pos": 8
   },
   "source": [
    "我们可以使用下标来引用向量的任一元素。例如，我们可以通过 $x_i$ 来引用第 $i$ 个元素。注意，元素 $x_i$ 是一个标量，所以我们在引用它时不会加粗。大量文献认为**列向量是向量的默认方向**，在本书中也是如此。在数学中，向量 $\\mathbf{x}$ 可以写为：\n",
    "\n",
    "$$\\mathbf{x} =\\begin{bmatrix}x_{1}  \\\\x_{2}  \\\\ \\vdots  \\\\x_{n}\\end{bmatrix},$$\n",
    ":eqlabel:`eq_vec_def`\n",
    "\n",
    "其中 $x_1, \\ldots, x_n$ 是向量的元素。在代码中，我们(**通过张量的索引来访问任一元素**)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac644dc4",
   "metadata": {
    "id": "pjzVNopc58Mg",
    "origin_pos": 10,
    "outputId": "8ffb40a9-350c-4dee-9408-c5a765d8e81a",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4fd371",
   "metadata": {
    "id": "FOpAB4Wr58Mh",
    "origin_pos": 12
   },
   "source": [
    "### 长度、维度和形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "024934b1",
   "metadata": {
    "id": "nLHdLuhj58Mh",
    "origin_pos": 14,
    "outputId": "44c4b92e-56a4-4102-9148-b7cb9ed95734",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c211c7cc",
   "metadata": {
    "id": "FKvW8_b958Mh",
    "origin_pos": 16
   },
   "source": [
    "当用张量表示一个向量（只有一个轴）时，我们也可以通过 `.shape` 属性访问向量的长度。形状（shape）是一个元组，列出了张量沿每个轴的长度（维数）。对于(**只有一个轴的张量，形状只有一个元素。**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d28e4c3",
   "metadata": {
    "id": "LrcPTHiO58Mi",
    "origin_pos": 18,
    "outputId": "f61b739c-8709-44b8-d1f1-d9707fa522cd",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7fb7a8",
   "metadata": {
    "id": "LaRfFni-58Mi",
    "origin_pos": 20
   },
   "source": [
    "## 矩阵\n",
    "\n",
    "正如向量将标量从零阶推广到一阶，矩阵将向量从一阶推广到二阶。矩阵，我们通常用粗体、大写字母来表示（例如，$\\mathbf{X}$、$\\mathbf{Y}$ 和 $\\mathbf{Z}$），在代码中表示为具有两个轴的张量。\n",
    "\n",
    "在数学表示法中，我们使用 $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ 来表示矩阵 $\\mathbf{A}$ ，其由$m$ 行和 $n$ 列的实值标量组成。直观地，我们可以将任意矩阵 $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ 视为一个表格，其中每个元素 $a_{ij}$ 属于第 $i$ 行第$j$ 列：\n",
    "\n",
    "$$\\mathbf{A}=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cd80c0",
   "metadata": {},
   "source": [
    "### **矩阵转置T**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34d8b428",
   "metadata": {
    "id": "ICIbhCRO58Mj",
    "origin_pos": 26,
    "outputId": "8b489ae8-5354-4b08-8d83-45d36081b398",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  4.,  8.],\n",
       "       [ 1.,  5.,  9.],\n",
       "       [ 2.,  6., 10.],\n",
       "       [ 3.,  7., 11.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba0eb18",
   "metadata": {},
   "source": [
    "### 哈达玛积"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e829c40",
   "metadata": {
    "id": "TOnaQ74v58Mm",
    "origin_pos": 44
   },
   "source": [
    "具体而言，[**两个矩阵的按元素乘法称为 *哈达玛积*（Hadamard product）（数学符号 $\\odot$）**]。对于矩阵 $\\mathbf{B} \\in \\mathbb{R}^{m \\times n}$，其中第 $i$ 行和第 $j$ 列的元素是 $b_{ij}$。矩阵$\\mathbf{A}$（在 :eqref:`eq_matrix_def` 中定义）和 $\\mathbf{B}$的哈达玛积为：\n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\odot \\mathbf{B} =\n",
    "\\begin{bmatrix}\n",
    "    a_{11}  b_{11} & a_{12}  b_{12} & \\dots  & a_{1n}  b_{1n} \\\\\n",
    "    a_{21}  b_{21} & a_{22}  b_{22} & \\dots  & a_{2n}  b_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_{m1}  b_{m1} & a_{m2}  b_{m2} & \\dots  & a_{mn}  b_{mn}\n",
    "\\end{bmatrix}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "568f1224",
   "metadata": {
    "id": "clLvrEdS58Mm",
    "origin_pos": 46,
    "outputId": "6069ca85-5751-461d-f648-273a6174873a",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   1,   4],\n",
       "        [  9,  16,  25],\n",
       "        [ 36,  49,  64],\n",
       "        [ 81, 100, 121]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(12).reshape((4,3))\n",
    "A * A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0ab9a9",
   "metadata": {
    "id": "KwtmXSj-58Mm",
    "origin_pos": 48
   },
   "source": [
    "将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e359890",
   "metadata": {
    "id": "YQXtFrXC58Mn",
    "origin_pos": 50,
    "outputId": "b782962d-5a89-40cf-b31e-5a43adc7a038",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2,  3,  4,  5],\n",
       "          [ 6,  7,  8,  9],\n",
       "          [10, 11, 12, 13]],\n",
       " \n",
       "         [[14, 15, 16, 17],\n",
       "          [18, 19, 20, 21],\n",
       "          [22, 23, 24, 25]]]),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "a + X, (a * X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d425c49",
   "metadata": {},
   "source": [
    "### 点积torch.dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d4cded",
   "metadata": {},
   "source": [
    "给定两个向量 $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d$，它们的 *点积*（dot product） $\\mathbf{x}^\\top \\mathbf{y}$（或 $\\langle \\mathbf{x}, \\mathbf{y}  \\rangle$）是相同位置的按元素乘积的和：$\\mathbf{x}^\\top \\mathbf{y} = \\sum_{i=1}^{d} x_i y_i$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df25408",
   "metadata": {
    "id": "b93KzXWd58Mn",
    "origin_pos": 52
   },
   "source": [
    "### 矩阵求和"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078ee37d",
   "metadata": {
    "id": "mcK1R1AU58Mn",
    "origin_pos": 60
   },
   "source": [
    "默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。\n",
    "我们还可以[**指定张量沿哪一个轴来通过求和降低维度**]。以矩阵为例，为了通过求和所有行的元素来降维（轴0），我们可以在调用函数时指定`axis=0`。\n",
    "由于输入矩阵沿0轴降维以生成输出向量，因此输入的轴0的维数在输出形状中丢失。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac0c6bc",
   "metadata": {
    "id": "ypReh4Ds58Mo",
    "origin_pos": 62,
    "outputId": "d54b24d0-95ea-41a4-9f8d-8fd1fc41d6e2",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([40., 45., 50., 55.]), torch.Size([4]))"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis0 = A.sum(axis=0)\n",
    "A_sum_axis0, A_sum_axis0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac77fdae",
   "metadata": {
    "heading_collapsed": true,
    "id": "8TYdagYy58Mp",
    "origin_pos": 80
   },
   "source": [
    "### 非降维求和\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4490f1",
   "metadata": {
    "hidden": true,
    "id": "ew4dXFu958Mp",
    "origin_pos": 82,
    "outputId": "ca92c7fc-f1b9-451f-e834-aba17666e8aa",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.],\n",
       "        [22.],\n",
       "        [38.],\n",
       "        [54.],\n",
       "        [70.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078eb320",
   "metadata": {
    "hidden": true,
    "id": "P_wTVlWb58Mq",
    "origin_pos": 84
   },
   "source": [
    "例如，由于 `sum_A` 在对每行进行求和后仍保持两个轴，我们可以(**通过广播将 `A` 除以 `sum_A`**) 。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d73b2cf",
   "metadata": {
    "hidden": true,
    "id": "3CIr0zK_58Mq",
    "origin_pos": 86,
    "outputId": "edabf208-b0a0-4397-8363-57e31afdd842",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n",
       "        [0.1818, 0.2273, 0.2727, 0.3182],\n",
       "        [0.2105, 0.2368, 0.2632, 0.2895],\n",
       "        [0.2222, 0.2407, 0.2593, 0.2778],\n",
       "        [0.2286, 0.2429, 0.2571, 0.2714]])"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A / sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805b4f9a",
   "metadata": {
    "hidden": true,
    "id": "8yIPDBZi58Mq",
    "origin_pos": 88
   },
   "source": [
    "如果我们想沿[**某个轴计算 `A` 元素的累积总和**]，比如 `axis=0`（按行计算），我们可以调用 `cumsum` 函数。此函数不会沿任何轴降低输入张量的维度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0deef7",
   "metadata": {
    "hidden": true,
    "id": "ikx0r5C558Mq",
    "origin_pos": 90,
    "outputId": "126a4872-ca2a-41e9-a86b-dc4cf6be24e0",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  6.,  8., 10.],\n",
       "        [12., 15., 18., 21.],\n",
       "        [24., 28., 32., 36.],\n",
       "        [40., 45., 50., 55.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.cumsum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014e5a43",
   "metadata": {
    "id": "_2HRV1ZX58Mr",
    "origin_pos": 100
   },
   "source": [
    "### torch.mv：矩阵-向量积\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c087ef4",
   "metadata": {},
   "source": [
    "现在我们知道如何计算点积，我们可以开始理解 *矩阵-向量积*（matrix-vector products）。回顾分别在 :eqref:`eq_matrix_def` 和 :eqref:`eq_vec_def` 中定义并画出的矩阵 $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ 和向量 $\\mathbf{x} \\in \\mathbb{R}^n$。让我们将矩阵$\\mathbf{A}$用它的行向量表示\n",
    "\n",
    "$$\\mathbf{A}=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_m \\\\\n",
    "\\end{bmatrix},$$\n",
    "\n",
    "其中每个$\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^n$ 都是行向量，表示矩阵的第 $i$ 行。[**矩阵向量积 $\\mathbf{A}\\mathbf{x}$ 是一个长度为 $m$ 的列向量，其第 $i$ 个元素是点积 $\\mathbf{a}^\\top_i \\mathbf{x}$**]：\n",
    "\n",
    "$$\n",
    "\\mathbf{A}\\mathbf{x}\n",
    "= \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_m \\\\\n",
    "\\end{bmatrix}\\mathbf{x}\n",
    "= \\begin{bmatrix}\n",
    " \\mathbf{a}^\\top_{1} \\mathbf{x}  \\\\\n",
    " \\mathbf{a}^\\top_{2} \\mathbf{x} \\\\\n",
    "\\vdots\\\\\n",
    " \\mathbf{a}^\\top_{m} \\mathbf{x}\\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "我们可以把一个矩阵 $\\mathbf{A}\\in \\mathbb{R}^{m \\times n}$ 乘法看作是一个从 $\\mathbb{R}^{n}$ 到 $\\mathbb{R}^{m}$ 向量的转换。这些转换证明是非常有用的。例如，我们可以用方阵的乘法来表示旋转。\n",
    "我们将在后续章节中讲到，我们也可以使用矩阵-向量积来描述在给定前一层的值时，求解神经网络每一层所需的复杂计算。\n",
    "\n",
    "在代码中使用张量表示矩阵-向量积，我们使用与点积相同的 `dot` 函数。当我们为矩阵 `A` 和向量 `x` 调用 `np.dot(A, x)`时，会执行矩阵-向量积。注意，`A` 的列维数（沿轴1的长度）必须与 `x` 的维数（其长度）相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010c773d",
   "metadata": {
    "id": "CCtdOKcp58Mr",
    "origin_pos": 102,
    "outputId": "d61c6521-5be6-40ca-d1d8-c5bedcfab3af",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, x.shape, torch.mv(A, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3cb9a0",
   "metadata": {
    "id": "xTD-tW7h58Mr",
    "origin_pos": 104
   },
   "source": [
    "### torch.mm：矩阵-矩阵乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e99e7e",
   "metadata": {},
   "source": [
    "如果你已经掌握了点积和矩阵-向量积的知识，那么 **矩阵-矩阵乘法**（matrix-matrix multiplication） 应该很简单。\n",
    "\n",
    "假设我们有两个矩阵 $\\mathbf{A} \\in \\mathbb{R}^{n \\times k}$ 和 $\\mathbf{B} \\in \\mathbb{R}^{k \\times m}$：\n",
    "\n",
    "$$\\mathbf{A}=\\begin{bmatrix}\n",
    " a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n",
    " a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " a_{n1} & a_{n2} & \\cdots & a_{nk} \\\\\n",
    "\\end{bmatrix},\\quad\n",
    "\\mathbf{B}=\\begin{bmatrix}\n",
    " b_{11} & b_{12} & \\cdots & b_{1m} \\\\\n",
    " b_{21} & b_{22} & \\cdots & b_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " b_{k1} & b_{k2} & \\cdots & b_{km} \\\\\n",
    "\\end{bmatrix}.$$\n",
    "\n",
    "用行向量$\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^k$ 表示矩阵$\\mathbf{A}$的第 $i$ 行，并让列向量$\\mathbf{b}_{j} \\in \\mathbb{R}^k$ 作为矩阵$\\mathbf{B}$的第 $j$ 列。要生成矩阵积 $\\mathbf{C} = \\mathbf{A}\\mathbf{B}$，最简单的方法是考虑$\\mathbf{A}$的行向量和$\\mathbf{B}$的列向量:\n",
    "\n",
    "$$\\mathbf{A}=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_n \\\\\n",
    "\\end{bmatrix},\n",
    "\\quad \\mathbf{B}=\\begin{bmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "当我们简单地将每个元素$c_{ij}$计算为点积$\\mathbf{a}^\\top_i \\mathbf{b}_j$:\n",
    "\n",
    "$$\\mathbf{C} = \\mathbf{AB} = \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_n \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\mathbf{b}_1 & \\mathbf{a}^\\top_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^\\top_{1} \\mathbf{b}_m \\\\\n",
    " \\mathbf{a}^\\top_{2}\\mathbf{b}_1 & \\mathbf{a}^\\top_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^\\top_{2} \\mathbf{b}_m \\\\\n",
    " \\vdots & \\vdots & \\ddots &\\vdots\\\\\n",
    "\\mathbf{a}^\\top_{n} \\mathbf{b}_1 & \\mathbf{a}^\\top_{n}\\mathbf{b}_2& \\cdots& \\mathbf{a}^\\top_{n} \\mathbf{b}_m\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "[**我们可以将矩阵-矩阵乘法 $\\mathbf{AB}$ 看作是简单地执行 $m$次矩阵-向量积，并将结果拼接在一起，形成一个 $n \\times m$ 矩阵**]。在下面的代码中，我们在 `A` 和 `B` 上执行矩阵乘法。这里的`A` 是一个5行4列的矩阵，`B`是一个4行3列的矩阵。相乘后，我们得到了一个5行3列的矩阵。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb063993",
   "metadata": {
    "id": "edNfYFNT58Mr",
    "origin_pos": 106,
    "outputId": "7f0b6fc7-d756-45a1-fe63-58f3e0b47abd",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  6.,  6.],\n",
       "        [22., 22., 22.],\n",
       "        [38., 38., 38.],\n",
       "        [54., 54., 54.],\n",
       "        [70., 70., 70.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.ones(4, 3)\n",
    "torch.mm(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3174e9",
   "metadata": {},
   "source": [
    "矩阵-矩阵乘法可以简单地称为 **矩阵乘法**，不应与 哈达玛积 混淆。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b979a022",
   "metadata": {},
   "source": [
    "## 范数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9a9b93",
   "metadata": {
    "id": "HOI7QY_z58Ms",
    "origin_pos": 108
   },
   "source": [
    "非正式地说，一个向量的*范数*告诉我们一个向量有多大。\n",
    "这里考虑的 *大小*（size） 概念不涉及维度，而是分量的大小。\n",
    "\n",
    "在线性代数中，向量范数是将向量映射到标量的函数 $f$。向量范数要满足一些属性。\n",
    "给定任意向量 $\\mathbf{x}$，第一个性质说，如果我们按常数因子 $\\alpha$ 缩放向量的所有元素，其范数也会按相同常数因子的 *绝对值* 缩放：\n",
    "\n",
    "$$f(\\alpha \\mathbf{x}) = |\\alpha| f(\\mathbf{x}).$$\n",
    "\n",
    "第二个性质是我们熟悉的三角不等式:\n",
    "\n",
    "$$f(\\mathbf{x} + \\mathbf{y}) \\leq f(\\mathbf{x}) + f(\\mathbf{y}).$$\n",
    "\n",
    "第三个性质简单地说范数必须是非负的:\n",
    "\n",
    "$$f(\\mathbf{x}) \\geq 0.$$\n",
    "\n",
    "这是有道理的，因为在大多数情况下，任何东西的最小的*大小*是0。最后一个性质要求范数最小为0，当且仅当向量全由0组成。\n",
    "\n",
    "$$\\forall i, [\\mathbf{x}]_i = 0 \\Leftrightarrow f(\\mathbf{x})=0.$$\n",
    "\n",
    "欧几里得距离是 $L_2$ 范数。假设$n$维向量 $\\mathbf{x}$ 中的元素是$x_1, \\ldots, x_n$，其 [**$L_2$ *范数* 是向量元素平方和的平方根：**]\n",
    "\n",
    "(**$$\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2},$$**)\n",
    "\n",
    "其中，在 $L_2$ 范数中常常省略下标 $2$，也就是说，$\\|\\mathbf{x}\\|$ 等同于 $\\|\\mathbf{x}\\|_2$。在代码中，我们可以按如下方式计算向量的 $L_2$ 范数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46e8b85",
   "metadata": {
    "id": "wgds1XW458Ms",
    "origin_pos": 110,
    "outputId": "c0610fd8-c85c-4496-c689-f64cb67768a3",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.norm(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49143d8d",
   "metadata": {
    "id": "6tUPq7VC58Ms",
    "origin_pos": 112
   },
   "source": [
    " [**$L_1$ 范数为向量元素的绝对值之和：**]\n",
    "\n",
    "(**$$\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|.$$**)\n",
    "\n",
    "与 $L_2$ 范数相比，$L_1$ 范数受异常值的影响较小。为了计算 $L_1$ 范数，我们将绝对值函数和按元素求和组合起来。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0cbfb9",
   "metadata": {
    "id": "DmXE9nPb58Ms",
    "origin_pos": 114,
    "outputId": "6b0c7d21-1f9c-4975-9440-f2b1b2500fda",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(u).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcab619",
   "metadata": {
    "id": "BLa65uI358Ms",
    "origin_pos": 116
   },
   "source": [
    "$L_2$ 范数和 $L_1$ 范数都是更一般的$L_p$范数的特例：\n",
    "\n",
    "$$\\|\\mathbf{x}\\|_p = \\left(\\sum_{i=1}^n \\left|x_i \\right|^p \\right)^{1/p}.$$\n",
    "\n",
    "类似于向量的$L_2$ 范数，[**矩阵**] $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$ (**的 *弗罗贝尼乌斯范数*（Frobenius norm） 是矩阵元素平方和的平方根：**)\n",
    "\n",
    "(**$$\\|\\mathbf{X}\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}.$$**)\n",
    "\n",
    "弗罗贝尼乌斯范数满足向量范数的所有性质。它就像是矩阵形向量的 $L_2$ 范数。调用以下函数将计算矩阵的弗罗贝尼乌斯范数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1b8af8",
   "metadata": {
    "id": "S8k58jN_58Mt",
    "origin_pos": 118,
    "outputId": "ff0941d7-a4a7-4470-f3c3-97528917884a",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torch.ones((4, 9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cccd56",
   "metadata": {},
   "source": [
    "### torch.norm：张量范数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73d9430e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor(2.4495)\n",
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.tensor as tensor\n",
    " \n",
    "a = torch.ones((2,3))  #建立tensor\n",
    "a2 = torch.norm(a)      #默认求2范数\n",
    "a1 = torch.norm(a,p=1)  #指定求1范数\n",
    " \n",
    "print(a)\n",
    "print(a2)\n",
    "print(a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-genetics",
   "metadata": {},
   "source": [
    "# 自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "designed-kuwait",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14., grad_fn=<DotBackward>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(4.0,requires_grad=True)   # 只有浮点数才能有梯度\n",
    "y = torch.dot(x,x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27387411",
   "metadata": {},
   "source": [
    "调用反向传播函数backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fadeaca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead76eb4",
   "metadata": {},
   "source": [
    "默认累积梯度，使用zero_()清楚梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "507891c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对非标量调用`backward`需要传入一个`gradient`参数，该参数指定微分函数关于`self`的梯度。\n",
    "# 在我们的例子中，我们只想求偏导数的和，所以传递一个1的梯度是合适的\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "# 等价于y.backward(torch.ones(len(x)))\n",
    "y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deeee90",
   "metadata": {},
   "source": [
    "## 分离计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06be0728",
   "metadata": {},
   "source": [
    "有时，我们希望[**将某些计算移动到记录的计算图之外**]。\n",
    "例如，假设`y`是作为`x`的函数计算的，而`z`则是作为`y`和`x`的函数计算的。\n",
    "现在，想象一下，我们想计算 `z` 关于 `x` 的梯度，但由于某种原因，我们希望将 `y` 视为一个常数，并且只考虑到 `x` 在`y`被计算后发挥的作用。\n",
    "\n",
    "在这里，我们可以分离 `y` 来返回一个新变量 `u`，该变量与 `y` 具有相同的值，但丢弃计算图中如何计算 `y` 的任何信息。换句话说，梯度不会向后流经 `u` 到 `x`。因此，下面的反向传播函数计算 `z = u * x` 关于 `x` 的偏导数，同时将 `u` 作为常数处理，而不是`z = x * x * x`关于 `x` 的偏导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e399f9c",
   "metadata": {
    "id": "8NhHjiwAtLXe",
    "origin_pos": 30,
    "outputId": "82722538-7392-4f1e-f069-228804bb06c4",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()   # 将y分离在计算图之外\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a2728f",
   "metadata": {},
   "source": [
    "调用backward函数的默认是标量，若是变量，就要使用ones_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20890e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()   # 将y分离在计算图之外\n",
    "z = u * x\n",
    "\n",
    "z.backward(torch.ones_like(z))\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413aacac",
   "metadata": {
    "id": "6B3-8drGtLXf",
    "origin_pos": 32
   },
   "source": [
    "由于记录了 `y` 的计算结果，我们可以随后在 `y` 上调用反向传播，得到 `y = x * x` 关于的`x`的导数，这里是 `2 * x`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c96761",
   "metadata": {
    "id": "GM5Jfr_ytLXf",
    "origin_pos": 34,
    "outputId": "980cd21b-c9ab-46d1-a554-ded1ee25d108",
    "pycharm": {
     "name": "#%%\n"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91e3b2b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "325.198px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
