{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1. 预备知识.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"origin_pos":0,"id":"YT3a7x17-CDc"},"source":["# 数据操作\n","\n","为了能够完成各种操作，我们需要某种方法来存储和操作数据。一般来说，我们需要做两件重要的事情：（1）获取数据；（2）在数据读入计算机后对其进行处理。如果没有某种方法来存储数据，那么获取数据是没有意义的。我们先尝试一个合成数据。首先，我们介绍$n$维数组，也称为 *张量*（tensor）。\n"]},{"cell_type":"markdown","metadata":{"id":"9YRaHGzN8bHD"},"source":["## 生成tensor"]},{"cell_type":"markdown","metadata":{"origin_pos":3,"tab":["tensorflow"],"id":"n8KuIjbh-CDl"},"source":["首先，我们导入 `tensorflow`。由于名称有点长，我们经常使用短别名 `tf` 导入它。\n"]},{"cell_type":"code","metadata":{"origin_pos":6,"tab":["tensorflow"],"id":"tzGouM14-CDl","executionInfo":{"status":"ok","timestamp":1617535889455,"user_tz":-480,"elapsed":2600,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}}},"source":["import tensorflow as tf"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":7,"id":"zjuoeZkh-CDm"},"source":["[**张量表示一个数值组成的数组，这个数组可能有多个维度**]。具有一个轴的张量对应于数学上的 *向量*（vector）。具有两个轴的张量对应于数学上的 *矩阵*（matrix）。具有两个轴以上的张量没有特殊的数学名称。\n","\n","首先，我们可以使用 `arange` 创建一个行向量 `x`。这个行向量包含以0开始的前12个整数，它们默认创建为浮点数。张量中的每个值都称为张量的 *元素*（element）。例如，张量 `x` 中有 12 个元素。除非额外指定，新的张量将存储在内存中，并采用基于CPU的计算。\n"]},{"cell_type":"code","metadata":{"origin_pos":10,"tab":["tensorflow"],"colab":{"base_uri":"https://localhost:8080/"},"id":"3vwGfw04-CDm","executionInfo":{"status":"ok","timestamp":1617456001873,"user_tz":-480,"elapsed":2653,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"db34d9bf-1818-42e5-8d0e-23287b3c78fa"},"source":["x = tf.range(12)\n","x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11], dtype=int32)>"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"origin_pos":11,"id":"HEVIKQSf-CDp"},"source":["**通过张量的 `shape` 属性来访问张量的 *形状***\n"]},{"cell_type":"code","metadata":{"origin_pos":12,"tab":["tensorflow"],"colab":{"base_uri":"https://localhost:8080/"},"id":"oj51ieKR-CDp","executionInfo":{"status":"ok","timestamp":1616587390672,"user_tz":-480,"elapsed":3491,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"453bf51e-9709-4dd6-f8c9-e1739f2af05d"},"source":["x.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([12])"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"origin_pos":13,"id":"L5Bn4OlN-CDp"},"source":["如果我们只想知道张量中元素的总数，即形状的所有元素乘积，我们可以检查它的大小（size）。\n","因为这里在处理的是一个向量，所以它的 `shape` 与它的 `size` 相同。\n"]},{"cell_type":"code","metadata":{"origin_pos":16,"tab":["tensorflow"],"colab":{"base_uri":"https://localhost:8080/"},"id":"0wwY99Tg-CDp","executionInfo":{"status":"ok","timestamp":1616587390672,"user_tz":-480,"elapsed":3484,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"a0940b74-d5a0-44d2-b81e-b9eb4ae4f001"},"source":["tf.size(x)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=int32, numpy=12>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"origin_pos":17,"id":"XaFdXnot-CDq"},"source":["[**要改变一个张量的形状而不改变元素数量和元素值，我们可以调用 `reshape` 函数。**]\n","例如，我们可以把张量 `x` 从形状为 (12, ) 的行向量转换为形状 (3, 4) 的矩阵。这个新的张量包含与转换前相同的值，但是把它们看成一个三行四列的矩阵。要重点说明一下，虽然形状发生了改变，但元素值没有变。注意，通过改变张量的形状，张量的大小不会改变。\n"]},{"cell_type":"code","metadata":{"origin_pos":19,"tab":["tensorflow"],"colab":{"base_uri":"https://localhost:8080/"},"id":"LjbKUz_N-CDq","executionInfo":{"status":"ok","timestamp":1616587390672,"user_tz":-480,"elapsed":3478,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"6c56ab7f-4f8b-47bc-95af-99be92d4d4b6"},"source":["X = tf.reshape(x, (3, 4))\n","X"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 4), dtype=int32, numpy=\n","array([[ 0,  1,  2,  3],\n","       [ 4,  5,  6,  7],\n","       [ 8,  9, 10, 11]], dtype=int32)>"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"origin_pos":20,"id":"gFSjJCKs-CDq"},"source":["不需要通过手动指定每个维度来改变形状。\n","如果我们的目标形状是 (高度, 宽度) ，那么在知道宽度后，高度应当会隐式得出，我们不必自己做除法。在上面的例子中，要获得一个有3行的矩阵，我们手动指定了它有3行和4列。幸**运的是，张量在给出其他部分后可以自动计算出一个维度。我们可以通过将希望张量自动推断的维度放置 `-1` 来调用此功能。**在上面的例子中，我们可以用 `x.reshape(-1, 4)` 或 `x.reshape(3, -1)`来取代`x.reshape(3, 4)`。\n","\n","\n","有时，我们希望[**使用全0、全1、其他常量或者从特定分布中随机采样的数字**]，来初始化矩阵。我们可以创建一个形状为 (2, 3, 4) 的张量，其中所有元素都设置为0。代码如下：\n"]},{"cell_type":"code","metadata":{"origin_pos":23,"tab":["tensorflow"],"colab":{"base_uri":"https://localhost:8080/"},"id":"1Rh64j_1-CDq","executionInfo":{"status":"ok","timestamp":1616587390672,"user_tz":-480,"elapsed":3472,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"58f2ede3-6201-4ebc-b978-5e8787497278"},"source":["tf.zeros((2, 3, 4))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=\n","array([[[0., 0., 0., 0.],\n","        [0., 0., 0., 0.],\n","        [0., 0., 0., 0.]],\n","\n","       [[0., 0., 0., 0.],\n","        [0., 0., 0., 0.],\n","        [0., 0., 0., 0.]]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"origin_pos":24,"id":"WS5vVI9D-CDr"},"source":["同样的，我们可以创建一个张量，其中所有元素都设置为1。代码如下：\n"]},{"cell_type":"code","metadata":{"origin_pos":27,"tab":["tensorflow"],"colab":{"base_uri":"https://localhost:8080/"},"id":"MTxKtK__-CDr","executionInfo":{"status":"ok","timestamp":1616587390673,"user_tz":-480,"elapsed":3468,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"3042ee0e-b89d-4128-ccd2-3f67342ed0f4"},"source":["tf.ones((2, 3, 4))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=\n","array([[[1., 1., 1., 1.],\n","        [1., 1., 1., 1.],\n","        [1., 1., 1., 1.]],\n","\n","       [[1., 1., 1., 1.],\n","        [1., 1., 1., 1.],\n","        [1., 1., 1., 1.]]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"origin_pos":28,"id":"JUZUJ7IY-CDr"},"source":["创建一个形状为 (3, 4) 的张量。其中的每个元素都从均值为0、标准差为1的标准高斯（正态）分布中随机采样。\n"]},{"cell_type":"code","metadata":{"origin_pos":31,"tab":["tensorflow"],"colab":{"base_uri":"https://localhost:8080/"},"id":"uIju2Uk--CDr","executionInfo":{"status":"ok","timestamp":1616587390673,"user_tz":-480,"elapsed":3462,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"8a3abf73-391b-490a-c44c-f164eadd8389"},"source":["tf.random.normal(shape=[3, 4])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n","array([[-0.06066323, -0.2710563 ,  0.09940508, -0.48237324],\n","       [-0.28749883, -0.08045979,  0.17377815, -1.6697613 ],\n","       [ 2.6813815 ,  0.7925951 , -0.99550605,  0.6647856 ]],\n","      dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"evAXKHVXpm3t","executionInfo":{"status":"ok","timestamp":1617535891848,"user_tz":-480,"elapsed":1120,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"5f6f6c69-44db-4b86-e475-2bf83e04d6e4"},"source":["tf.random.normal(shape=(3,4))  # 似乎和上面没有区别"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n","array([[ 0.88779145, -0.7829903 ,  0.40905505,  0.8639114 ],\n","       [ 2.0798135 , -0.73407716, -0.8396039 , -1.2732086 ],\n","       [ 2.2429762 , -0.46559012, -1.6372334 ,  1.7088809 ]],\n","      dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"origin_pos":32,"id":"_YIzXe2p-CDr"},"source":["我们还可以[**通过提供包含数值的 Python 列表（或嵌套列表）来为所需张量中的每个元素赋予确定值**]。在这里，最外层的列表对应于轴 0，内层的列表对应于轴 1。\n"]},{"cell_type":"code","metadata":{"origin_pos":35,"tab":["tensorflow"],"colab":{"base_uri":"https://localhost:8080/"},"id":"A3sWxCI1-CDs","executionInfo":{"status":"ok","timestamp":1616587390673,"user_tz":-480,"elapsed":3456,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"b3fded27-f7b0-440b-e7d6-68516a5f5169"},"source":["tf.constant([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 4), dtype=int32, numpy=\n","array([[2, 1, 4, 3],\n","       [1, 2, 3, 4],\n","       [4, 3, 2, 1]], dtype=int32)>"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"origin_pos":36,"id":"-W8K2DGe-CDs"},"source":["## 运算\n","\n","*按元素*（elementwise） 操作将标准标量运算符应用于数组的每个元素。对于将两个数组作为输入的函数，按元素运算将二元运算符应用于两个数组中的每对位置对应的元素。我们可以基于任何从标量到标量的函数来创建按元素函数。\n","\n","在数学表示法中，我们将通过符号 $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ 来表示 *一元* 标量运算符（只接收一个输入）。这意味着该函数从任何实数（$\\mathbb{R}$）映射到另一个实数。同样，我们通过符号 $f: \\mathbb{R}, \\mathbb{R} \\rightarrow \\mathbb{R}$ 表示 *二元* 标量运算符，这意味着该函数接收两个输入，并产生一个输出。给定同一形状的任意两个向量$\\mathbf{u}$和$\\mathbf{v}$ 和二元运算符 $f$，我们可以得到向量$\\mathbf{c} = F(\\mathbf{u},\\mathbf{v})$。具体计算方法是$c_i \\gets f(u_i, v_i)$ ，其中 $c_i$、u_i$ 和 $v_i$ 分别是向量$\\mathbf{c}$、$\\mathbf{u}$ 和 $\\mathbf{v}$中的元素。在这里，我们通过将标量函数升级为按元素向量运算来生成向量值 $F: \\mathbb{R}^d, \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$。\n","\n","对于任意具有相同形状的张量，[**常见的标准算术运算符（`+`、`-`、`*`、`/` 和 `**`）都可以被升级为按元素运算**]。我们可以在同一形状的任意两个张量上调用按元素操作。在下面的例子中，我们使用逗号来表示一个具有5个元素的元组，其中每个元素都是按元素操作的结果。\n"]},{"cell_type":"code","metadata":{"origin_pos":39,"tab":["tensorflow"],"colab":{"base_uri":"https://localhost:8080/"},"id":"jVzOtZhE-CDs","executionInfo":{"status":"ok","timestamp":1616587390674,"user_tz":-480,"elapsed":3452,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"dbdd7932-cb4d-4f12-e836-6c960f0eeb02"},"source":["x = tf.constant([1.0, 2, 4, 8])\n","y = tf.constant([2.0, 2, 2, 2])\n","x + y, x - y, x * y, x / y, x**y  # **运算符是求幂运算"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 3.,  4.,  6., 10.], dtype=float32)>,\n"," <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-1.,  0.,  2.,  6.], dtype=float32)>,\n"," <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 2.,  4.,  8., 16.], dtype=float32)>,\n"," <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.5, 1. , 2. , 4. ], dtype=float32)>,\n"," <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 1.,  4., 16., 64.], dtype=float32)>)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"origin_pos":40,"id":"YQE7R5w7-CDs"},"source":["可以(**按按元素方式应用更多的计算**)，包括像求幂这样的一元运算符。\n"]},{"cell_type":"code","metadata":{"origin_pos":43,"tab":["tensorflow"],"colab":{"base_uri":"https://localhost:8080/"},"id":"37wqSTlo-CDs","executionInfo":{"status":"ok","timestamp":1616587391476,"user_tz":-480,"elapsed":4248,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"e8c378d2-e8e5-480d-b0b3-1d1ab9c9bf3c"},"source":["tf.exp(x)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(4,), dtype=float32, numpy=\n","array([2.7182817e+00, 7.3890562e+00, 5.4598148e+01, 2.9809580e+03],\n","      dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"origin_pos":44,"id":"AcPtivKC-CDt"},"source":["除了按元素计算外，我们还可以执行线性代数运算，包括向量点积和矩阵乘法。\n","\n","[**我们也可以把多个张量连结在一起**]，把它们端对端地叠起来形成一个更大的张量。\n","我们也可以 *连结*（concatenate） 多个张量在一起，将它们端到端堆叠以形成更大的张量。我们只需要提供张量列表，并给出沿哪个轴连结。下面的例子分别演示了当我们沿行（轴-0，形状的第一个元素）和按列（轴-1，形状的第二个元素）连结两个矩阵时会发生什么情况。我们可以看到，第一个输出张量的轴-0长度 ($6$) 是两个输入张量轴-0长度的总和 ($3 + 3$)；第二个输出张量的轴-1长度 ($8$) 是两个输入张量轴-1长度的总和 ($4 + 4$)。\n"]},{"cell_type":"code","metadata":{"origin_pos":47,"tab":["tensorflow"],"colab":{"base_uri":"https://localhost:8080/"},"id":"JMJxCdXA-CDt","executionInfo":{"status":"ok","timestamp":1617536287402,"user_tz":-480,"elapsed":1048,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"a260e7e3-6140-43de-f17e-f414da2509e1"},"source":["X = tf.reshape(tf.range(12, dtype=tf.float32), (3, 4))\n","Y = tf.constant([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n","tf.concat([X, Y], axis=0), tf.concat([X, Y], axis=1)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(6, 4), dtype=float32, numpy=\n"," array([[ 0.,  1.,  2.,  3.],\n","        [ 4.,  5.,  6.,  7.],\n","        [ 8.,  9., 10., 11.],\n","        [ 2.,  1.,  4.,  3.],\n","        [ 1.,  2.,  3.,  4.],\n","        [ 4.,  3.,  2.,  1.]], dtype=float32)>,\n"," <tf.Tensor: shape=(3, 8), dtype=float32, numpy=\n"," array([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n","        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n","        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]], dtype=float32)>)"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"origin_pos":48,"id":"3FFyHisH-CDt"},"source":["有时，我们想[**通过 *逻辑运算符* 构建二元张量**]。以 `X == Y` 为例子。\n","对于每个位置，如果 `X` 和 `Y` 在该位置相等，则新张量中相应项的值为1，这意味着逻辑语句 `X == Y` 在该位置处为真，否则该位置为 0。\n"]},{"cell_type":"code","metadata":{"origin_pos":49,"tab":["tensorflow"],"colab":{"base_uri":"https://localhost:8080/"},"id":"q2v8iTPp-CDt","executionInfo":{"status":"ok","timestamp":1616587391476,"user_tz":-480,"elapsed":4236,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"f8237b0c-b3c6-4b10-f855-87b7a6f410cc"},"source":["X == Y"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 4), dtype=bool, numpy=\n","array([[False,  True, False,  True],\n","       [False, False, False, False],\n","       [False, False, False, False]])>"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"origin_pos":50,"id":"zNiiRv9Z-CDt"},"source":["[**对张量中的所有元素进行求和会产生一个只有一个元素的张量。**]\n"]},{"cell_type":"code","metadata":{"origin_pos":52,"tab":["tensorflow"],"colab":{"base_uri":"https://localhost:8080/"},"id":"RyH66nKx-CDt","executionInfo":{"status":"ok","timestamp":1616587391477,"user_tz":-480,"elapsed":4230,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"85373f3d-ab49-4fe1-af0d-997d3891c67c"},"source":["tf.reduce_sum(X)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=float32, numpy=66.0>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"origin_pos":53,"id":"xeEajRo1-CDu"},"source":["## 广播机制\n","\n","在上面的部分中，我们看到了如何在相同形状的两个张量上执行按元素操作。在某些情况下，[**即使形状不同，我们仍然可以通过调用 *广播机制* （broadcasting mechanism） 来执行按元素操作**]。这种机制的工作方式如下：首先，通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状。其次，对生成的数组执行按元素操作。\n","\n","在大多数情况下，我们将沿着数组中长度为1的轴进行广播，如下例子：\n"]},{"cell_type":"code","metadata":{"origin_pos":56,"tab":["tensorflow"],"colab":{"base_uri":"https://localhost:8080/"},"id":"ZXmp_bgH-CDu","executionInfo":{"status":"ok","timestamp":1616587391477,"user_tz":-480,"elapsed":4224,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"426858d6-98d9-4161-cfe7-21b409280d67"},"source":["a = tf.reshape(tf.range(3), (3, 1))\n","b = tf.reshape(tf.range(2), (1, 2))\n","a, b"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(3, 1), dtype=int32, numpy=\n"," array([[0],\n","        [1],\n","        [2]], dtype=int32)>,\n"," <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[0, 1]], dtype=int32)>)"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"origin_pos":57,"id":"M3An4aZ6-CDu"},"source":["由于 `a` 和 `b` 分别是 $3\\times1$ 和 $1\\times2$ 矩阵，如果我们让它们相加，它们的形状不匹配。我们将两个矩阵*广播*为一个更大的 $3\\times2$ 矩阵，如下所示：矩阵 `a`将复制列，矩阵 `b`将复制行，然后再按元素相加。\n"]},{"cell_type":"code","metadata":{"origin_pos":58,"tab":["tensorflow"],"colab":{"base_uri":"https://localhost:8080/"},"id":"ZQy_kbtB-CDu","executionInfo":{"status":"ok","timestamp":1616587391478,"user_tz":-480,"elapsed":4218,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"d5e50ab8-f5b2-4184-da41-5893ef3ca13d"},"source":["a + b"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n","array([[0, 1],\n","       [1, 2],\n","       [2, 3]], dtype=int32)>"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"origin_pos":59,"id":"8ri09KTM-CDu"},"source":["## 索引和切片\n","\n","就像在任何其他 Python 数组中一样，张量中的元素可以通过索引访问。与任何 Python 数组一样：第一个元素的索引是 0；可以指定范围以包含第一个元素和最后一个之前的元素。与标准 Python 列表一样，我们可以通过使用负索引根据元素到列表尾部的相对位置访问元素。\n","\n","因此，我们[**可以用 `[-1]` 选择最后一个元素，可以用 `[1:3]` 选择第二个和第三个元素**]，如下所示：\n"]},{"cell_type":"code","metadata":{"origin_pos":60,"tab":["tensorflow"],"colab":{"base_uri":"https://localhost:8080/"},"id":"daqYqJGr-CDu","executionInfo":{"status":"ok","timestamp":1616587391478,"user_tz":-480,"elapsed":4212,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"786fceaa-904a-4d5c-b8df-438dd04cf809"},"source":["X[-1], X[1:3]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 8.,  9., 10., 11.], dtype=float32)>,\n"," <tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n"," array([[ 4.,  5.,  6.,  7.],\n","        [ 8.,  9., 10., 11.]], dtype=float32)>)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"origin_pos":62,"tab":["tensorflow"],"id":"_uhFLz1N-CDv"},"source":["**TensorFlow中的 `Tensors` 是不可变的，也不能被赋值。TensorFlow中的 `Variables` 是支持赋值的可变容器。**请记住，TensorFlow中的梯度不会通过 `Variable` 反向传播。\n","\n","除了为整个 `Variable` 分配一个值之外，我们还可以通过索引来写入 `Variable` 的元素。\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1frFB46xrDwZ","executionInfo":{"status":"ok","timestamp":1617536392984,"user_tz":-480,"elapsed":1070,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"27fc31e6-ba5c-4d63-9a11-f70d729f053a"},"source":["X_var= tf.Variable(X)\n","X_var,X_var[1, 2]"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Variable 'Variable:0' shape=(3, 4) dtype=float32, numpy=\n"," array([[ 0.,  1.,  2.,  3.],\n","        [ 4.,  5.,  6.,  7.],\n","        [ 8.,  9., 10., 11.]], dtype=float32)>,\n"," <tf.Tensor: shape=(), dtype=float32, numpy=6.0>)"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"origin_pos":64,"tab":["tensorflow"],"colab":{"base_uri":"https://localhost:8080/"},"id":"FhMlA4Op-CDv","executionInfo":{"status":"ok","timestamp":1617536395336,"user_tz":-480,"elapsed":935,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"1d12b1b0-ab32-410e-f40e-bc21e84c0b6a"},"source":["X_var[1, 2].assign(9)\n","X_var"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Variable 'Variable:0' shape=(3, 4) dtype=float32, numpy=\n","array([[ 0.,  1.,  2.,  3.],\n","       [ 4.,  5.,  9.,  7.],\n","       [ 8.,  9., 10., 11.]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"origin_pos":65,"id":"Q-uMiq9M-CDv"},"source":["如果我们想[**为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值。**]\n","例如，`[0:2, :]` 访问第1行和第2行，其中 “:” 代表沿轴 1（列）的所有元素。虽然我们讨论的是矩阵的索引，但这也适用于向量和超过2个维度的张量。\n"]},{"cell_type":"code","metadata":{"origin_pos":67,"tab":["tensorflow"],"colab":{"base_uri":"https://localhost:8080/"},"id":"BiWDecA0-CDv","executionInfo":{"status":"ok","timestamp":1616587391479,"user_tz":-480,"elapsed":4201,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"6a5a2858-70be-43c7-d3b0-1f35e3ba4cae"},"source":["X_var = tf.Variable(X)\n","X_var[0:2, :].assign(tf.ones(X_var[0:2, :].shape, dtype=tf.float32) * 12)\n","X_var"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Variable 'Variable:0' shape=(3, 4) dtype=float32, numpy=\n","array([[12., 12., 12., 12.],\n","       [12., 12., 12., 12.],\n","       [ 8.,  9., 10., 11.]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"origin_pos":68,"id":"7RY9zEKG-CDv"},"source":["## 节省内存\n","\n","[**运行一些操作可能会导致为新结果分配内存**]。例如，如果我们用 `Y = X + Y`，我们将取消引用 `Y` 指向的张量，而是指向新分配的内存处的张量。\n","\n","在下面的例子中，我们用 Python 的 `id()` 函数演示了这一点，它给我们提供了内存中引用对象的确切地址。运行 `Y = Y + X` 后，我们会发现 `id(Y)` 指向另一个位置。这是因为 Python 首先计算 `Y + X`，为结果分配新的内存，然后使 `Y` 指向内存中的这个新位置。\n"]},{"cell_type":"code","metadata":{"origin_pos":69,"tab":["tensorflow"],"colab":{"base_uri":"https://localhost:8080/"},"id":"fDr9S0e0-CDv","executionInfo":{"status":"ok","timestamp":1616587391479,"user_tz":-480,"elapsed":4195,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"3d59e563-9ffa-4df0-f4ce-56744e8d2fb6"},"source":["before = id(Y)\n","Y = Y + X\n","id(Y) == before"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"origin_pos":70,"id":"juefLJz6-CDw"},"source":["这可能是不可取的，原因有两个：首先，我们不想总是不必要地分配内存。在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。通常情况下，我们希望原地执行这些更新。其次，我们可能通过多个变量指向相同参数。如果我们不原地更新，其他引用仍然会指向旧的内存位置，这样我们的某些代码可能会无意中引用旧的参数。\n"]},{"cell_type":"markdown","metadata":{"origin_pos":72,"tab":["tensorflow"],"id":"8I9bUH87-CDw"},"source":["`Variables` 是TensorFlow中的可变容器。它们提供了一种存储模型参数的方法。我们可以通过`assign`将一个操作的结果分配给一个 `Variable`。为了说明这一点，我们创建了一个与另一个张量 `Y` 相同的形状的 `Z`，使用 `zeros_like` 来分配一个全$0$的块。\n"]},{"cell_type":"code","metadata":{"origin_pos":75,"tab":["tensorflow"],"colab":{"base_uri":"https://localhost:8080/"},"id":"uHNmGyEw-CDw","executionInfo":{"status":"ok","timestamp":1616587391479,"user_tz":-480,"elapsed":4189,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"c110285d-627c-4e2b-94fa-e92b0610776b"},"source":["Z = tf.Variable(tf.zeros_like(Y))\n","print('id(Z):', id(Z))\n","Z.assign(X + Y)\n","print('id(Z):', id(Z))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["id(Z): 140137986662480\n","id(Z): 140137986662480\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"origin_pos":77,"tab":["tensorflow"],"id":"kMnG5NFC-CDw"},"source":["即使你将状态持久存储在 `Variable` 中，你也可能希望避免为不是模型参数的张量过度分配内存，从而进一步减少内存使用量。\n","\n","由于 TensorFlow的 `Tensors` 是不可变的，而且梯度不会通过 `Variable` 流动，因此 TensorFlow 没有提供一种明确的方式来原地运行单个操作。\n","\n","但是，TensorFlow提供了`tf.function`修饰符，将计算封装在TensorFlow图中，该图在运行前经过编译和优化。这允许TensorFlow删除未使用的值，并复用先前分配的且不再需要的值。这样可以最大限度地减少 TensorFlow 计算的内存开销。\n"]},{"cell_type":"code","metadata":{"origin_pos":79,"tab":["tensorflow"],"colab":{"base_uri":"https://localhost:8080/"},"id":"v6cox_fx-CDw","executionInfo":{"status":"ok","timestamp":1616587391480,"user_tz":-480,"elapsed":4185,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"758bcf0a-cb7e-4a23-8f2c-25fbeed8bf9d"},"source":["@tf.function\n","def computation(X, Y):\n","    Z = tf.zeros_like(Y)  # 这个未使用的值将被删除\n","    A = X + Y  # 当不再需要时，分配将被复用\n","    B = A + Y\n","    C = B + Y\n","    return C + Y\n","\n","computation(X, Y)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n","array([[ 8.,  9., 26., 27.],\n","       [24., 33., 42., 51.],\n","       [56., 57., 58., 59.]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"origin_pos":80,"id":"FUDXKFXm-CDw"},"source":["## 转换为其他 Python 对象\n","\n","[**转换为 NumPy 张量**]很容易，反之也很容易。转换后的结果不共享内存。\n","这个小的不便实际上是非常重要的：当你在 CPU 或 GPU 上执行操作的时候，此时Python的NumPy包也希望使用相同的内存块执行其他操作时，你不希望停止计算。\n"]},{"cell_type":"code","metadata":{"origin_pos":83,"tab":["tensorflow"],"colab":{"base_uri":"https://localhost:8080/"},"id":"qjScsTYa-CDw","executionInfo":{"status":"ok","timestamp":1616587391480,"user_tz":-480,"elapsed":4180,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"64e18eed-60a6-46af-f67d-bb8874ae0598"},"source":["A = X.numpy()\n","B = tf.constant(A)\n","type(A), type(B)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(numpy.ndarray, tensorflow.python.framework.ops.EagerTensor)"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"origin_pos":84,"id":"2xyxKyAh-CDx"},"source":["要(**将大小为1的张量转换为 Python 标量**)，我们可以调用 `item` 函数或 Python 的内置函数。\n"]},{"cell_type":"code","metadata":{"origin_pos":87,"tab":["tensorflow"],"colab":{"base_uri":"https://localhost:8080/"},"id":"7G9LCTmg-CDx","executionInfo":{"status":"ok","timestamp":1616587391480,"user_tz":-480,"elapsed":4175,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"c81e45f4-e8a8-4896-c442-a06059e3b1ed"},"source":["a = tf.constant([3.5]).numpy()\n","a, a.item(), float(a), int(a)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([3.5], dtype=float32), 3.5, 3.5, 3)"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"origin_pos":0,"id":"whjEUwR2C9g8"},"source":["# 线性代数\n","\n","## 标量\n","\n","在本书中，我们采用了数学表示法，其中标量变量由普通小写字母表示（例如，$x$、$y$ 和 $z$）。我们用 $\\mathbb{R}$ 表示所有（连续）*实数* 标量的空间。为了方便，我们之后将严格定义 *空间*（space）是什么，但现在只要记住，表达式 $x \\in \\mathbb{R}$ 是表示$x$是一个实值标量的正式形式。符号 $\\in$ 称为 “属于”，它表示“是集合中的成员”。我们可以用 $x, y \\in \\{0, 1\\}$ 来表明 $x$ 和 $y$ 是值只能为 $0$ 或 $1$的数字。\n","\n","(**标量由只有一个元素的张量表示**)。在下面的代码中，我们实例化两个标量，并使用它们执行一些熟悉的算术运算，即加法，乘法，除法和指数。\n"]},{"cell_type":"code","metadata":{"origin_pos":3,"tab":["tensorflow"],"id":"lJojfMC1C9hB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616587391481,"user_tz":-480,"elapsed":4169,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"4e58f725-6af7-41eb-d7be-153c663c6934"},"source":["import tensorflow as tf\n","\n","x = tf.constant([3.0])\n","y = tf.constant([2.0])\n","\n","x + y, x * y, x / y, x**y"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.], dtype=float32)>,\n"," <tf.Tensor: shape=(1,), dtype=float32, numpy=array([6.], dtype=float32)>,\n"," <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.5], dtype=float32)>,\n"," <tf.Tensor: shape=(1,), dtype=float32, numpy=array([9.], dtype=float32)>)"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"origin_pos":4,"id":"JiFg92IwC9hD"},"source":["## 向量\n","\n","[**你可以将向量视为标量值组成的列表**]。我们将这些标量值称为向量的 *元素*（elements）或*分量*（components）。在数学表示法中，我们通常将向量记为粗体、小写的符号（例如，$\\mathbf{x}$、$\\mathbf{y}$和$\\mathbf{z})$）。\n","\n","我们通过一维张量处理向量。一般来说，张量可以具有任意长度，取决于机器的内存限制。\n"]},{"cell_type":"code","metadata":{"origin_pos":7,"tab":["tensorflow"],"id":"cYs4gBW-C9hD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616587391481,"user_tz":-480,"elapsed":4161,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"f3f961b6-12ea-45e4-b08b-8d9b6d33dbde"},"source":["x = tf.range(4)\n","x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 1, 2, 3], dtype=int32)>"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"origin_pos":8,"id":"8XhHZkz-C9hD"},"source":["我们可以使用下标来引用向量的任一元素。例如，我们可以通过 $x_i$ 来引用第 $i$ 个元素。注意，元素 $x_i$ 是一个标量，所以我们在引用它时不会加粗。大量文献认为**列向量是向量的默认方向**，在本书中也是如此。在数学中，向量 $\\mathbf{x}$ 可以写为：\n","\n","$$\\mathbf{x} =\\begin{bmatrix}x_{1}  \\\\x_{2}  \\\\ \\vdots  \\\\x_{n}\\end{bmatrix},$$\n","\n","\n","其中 $x_1, \\ldots, x_n$ 是向量的元素。在代码中，我们(**通过张量的索引来访问任一元素**)。\n"]},{"cell_type":"code","metadata":{"origin_pos":11,"tab":["tensorflow"],"id":"MDaHMpyOC9hD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616587391481,"user_tz":-480,"elapsed":4153,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"f2a615df-0812-4ce1-cfeb-9713e8ab6d19"},"source":["x[3]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=int32, numpy=3>"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"origin_pos":12,"id":"YWXSWD8CC9hE"},"source":["### 长度、维度和形状\n","\n","向量只是一个数字数组。就像每个数组都有一个长度一样，每个向量也是如此。在数学表示法中，如果我们想说一个向量 $\\mathbf{x}$ 由 $n$ 个实值标量组成，我们可以将其表示为 $\\mathbf{x} \\in \\mathbb{R}^n$。向量的长度通常称为向量的 *维度*（dimension）。\n","\n","与普通的 Python 数组一样，我们可以通过调用 Python 的内置 `len()` 函数来[**访问张量的长度**]。\n"]},{"cell_type":"code","metadata":{"origin_pos":15,"tab":["tensorflow"],"id":"mBeAYRedC9hE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616587391481,"user_tz":-480,"elapsed":4145,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"b844326c-4270-4ada-bb22-e89397179764"},"source":["len(x)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"origin_pos":16,"id":"2yNM_9uzC9hE"},"source":["当用张量表示一个向量（只有一个轴）时，我们也可以通过 `.shape` 属性访问向量的长度。形状（shape）是一个元组，列出了张量沿每个轴的长度（维数）。对于(**只有一个轴的张量，形状只有一个元素。**)\n"]},{"cell_type":"code","metadata":{"origin_pos":19,"tab":["tensorflow"],"id":"tByykoekC9hE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616587391482,"user_tz":-480,"elapsed":4138,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"a1b49b00-9fa1-4c77-a66d-1de22122e74c"},"source":["x.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([4])"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"origin_pos":20,"id":"pF2hjvkGC9hF"},"source":["请注意，*维度*（dimension）这个词在不同上下文时往往会有不同的含义，这经常会使人感到困惑。为了清楚起见，我们在此明确一下。*向量*或*轴*的维度被用来表示*向量*或*轴*的长度，即向量或轴的元素数量。然而，张量的维度用来表示张量具有的轴数。在这个意义上，张量的某个轴的维数就是这个轴的长度。\n","\n","## 矩阵\n","\n","正如向量将标量从零阶推广到一阶，矩阵将向量从一阶推广到二阶。矩阵，我们通常用粗体、大写字母来表示（例如，$\\mathbf{X}$、$\\mathbf{Y}$ 和 $\\mathbf{Z}$），在代码中表示为具有两个轴的张量。\n","\n","在数学表示法中，我们使用 $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ 来表示矩阵 $\\mathbf{A}$ ，其由$m$ 行和 $n$ 列的实值标量组成。直观地，我们可以将任意矩阵 $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ 视为一个表格，其中每个元素 $a_{ij}$ 属于第 $i$ 行第$j$ 列：\n","\n","$$\\mathbf{A}=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix}.$$\n","\n","\n","对于任意$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$,$\\mathbf{A}$的形状是($m$, $n$)或$m \\times n$。当矩阵具有相同数量的行和列时，其形状将变为正方形；因此，它被称为 *方矩阵*（square matrix）。\n","\n","当调用函数来实例化张量时，我们可以[**通过指定两个分量$m$ 和 $n$来创建一个形状为$m \\times n$ 的矩阵**]。\n"]},{"cell_type":"code","metadata":{"origin_pos":23,"tab":["tensorflow"],"id":"D7StYwwkC9hF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616587391482,"user_tz":-480,"elapsed":4129,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"8e5ecfd1-a5e1-4513-f5d6-726eb26b1346"},"source":["A = tf.reshape(tf.range(20), (5, 4))\n","A"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(5, 4), dtype=int32, numpy=\n","array([[ 0,  1,  2,  3],\n","       [ 4,  5,  6,  7],\n","       [ 8,  9, 10, 11],\n","       [12, 13, 14, 15],\n","       [16, 17, 18, 19]], dtype=int32)>"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"origin_pos":24,"id":"0baUveFvC9hF"},"source":["我们可以通过行索引（$i$）和列索引（$j$）来访问矩阵中的标量元素 $a_{ij}$，例如 $[\\mathbf{A}]_{ij}$。如果没有给出矩阵 $\\mathbf{A}$ 的标量元素，如在 :eqref:`eq_matrix_def`那样，我们可以简单地使用矩阵 $\\mathbf{A}$ 的小写字母索引下标 $a_{ij}$来引用$[\\mathbf{A}]_{ij}$。为了表示起来简单，只有在必要时才会将逗号插入到单独的索引中，例如 $a_{2, 3j}$ 和 $[\\mathbf{A}]_{2i-1, 3}$。\n","\n","有时候，我们想翻转轴。当我们交换矩阵的行和列时，结果称为矩阵的 *转置*（transpose）。我们用$\\mathbf{a}^\\top$来表示矩阵的转置，如果$\\mathbf{B} = \\mathbf{A}^\\top$，则对于任意$i$和$j$，都有$b_{ij} = a_{ji}$。因此，在 :eqref:`eq_matrix_def` 中的转置是一个形状为$n \\times m$的矩阵：\n","\n","$$\n","\\mathbf{A}^\\top =\n","\\begin{bmatrix}\n","    a_{11} & a_{21} & \\dots  & a_{m1} \\\\\n","    a_{12} & a_{22} & \\dots  & a_{m2} \\\\\n","    \\vdots & \\vdots & \\ddots  & \\vdots \\\\\n","    a_{1n} & a_{2n} & \\dots  & a_{mn}\n","\\end{bmatrix}.\n","$$\n","\n","现在我们在代码中访问(**矩阵的转置**)。\n"]},{"cell_type":"code","metadata":{"origin_pos":27,"tab":["tensorflow"],"id":"-N8yOhDPC9hF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616587391482,"user_tz":-480,"elapsed":4121,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"309e3d6a-dc0a-481a-9884-ea37a8123c65"},"source":["tf.transpose(A)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(4, 5), dtype=int32, numpy=\n","array([[ 0,  4,  8, 12, 16],\n","       [ 1,  5,  9, 13, 17],\n","       [ 2,  6, 10, 14, 18],\n","       [ 3,  7, 11, 15, 19]], dtype=int32)>"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"origin_pos":28,"id":"z_sqNkIzC9hG"},"source":["作为方矩阵的一种特殊类型，[***对称矩阵*（symmetric matrix） $\\mathbf{A}$ 等于其转置：$\\mathbf{A} = \\mathbf{A}^\\top$**]。这里我们定义一个对称矩阵 `B`：\n"]},{"cell_type":"code","metadata":{"origin_pos":31,"tab":["tensorflow"],"id":"WMppwIUsC9hG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616587391483,"user_tz":-480,"elapsed":4114,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"79f28780-9560-42f9-b07c-1a4ae8e1fb9c"},"source":["B = tf.constant([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n","B"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n","array([[1, 2, 3],\n","       [2, 0, 4],\n","       [3, 4, 5]], dtype=int32)>"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"origin_pos":32,"id":"NCYBolpjC9hG"},"source":["现在我们将 `B` 与它的转置进行比较。\n"]},{"cell_type":"code","metadata":{"origin_pos":35,"tab":["tensorflow"],"id":"N-GXcbWUC9hG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616587392682,"user_tz":-480,"elapsed":5304,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"adb32b61-4287-491f-bc61-325c47306a86"},"source":["B == tf.transpose(B)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 3), dtype=bool, numpy=\n","array([[ True,  True,  True],\n","       [ True,  True,  True],\n","       [ True,  True,  True]])>"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"origin_pos":36,"id":"pO7x4zkNC9hG"},"source":["矩阵是有用的数据结构：它们允许我们组织具有不同变化模式的数据。例如，我们矩阵中的行可能对应于不同的房屋（数据样本），而列可能对应于不同的属性。尽管单个向量的默认方向是列向量，但在表示表格数据集的矩阵中，将每个数据样本作为矩阵中的行向量更为常见。我们将在后面的章节中讲到这点。这种约定将支持常见的深度学习实践。例如，沿着张量的最外轴，我们可以访问或遍历小批量的数据样本。如果不存在小批量，我们也可以只访问数据样本。\n","\n","## 张量\n","\n","[**就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构**]。张量（本小节中的 “张量” 指代数对象）为我们提供了描述具有任意数量轴的$n$维数组的通用方法。例如，向量是一阶张量，矩阵是二阶张量。张量用特殊字体的大写字母（例如，$\\mathsf{X}$、$\\mathsf{Y}$ 和 $\\mathsf{Z}$）表示，它们的索引机制（例如 $x_{ijk}$ 和 $[\\mathsf{X}]_{1, 2i-1, 3}$）与矩阵类似。\n","\n","当我们开始处理图像时，张量将变得更加重要，图像以$n$维数组形式出现，其中3个轴对应于高度、宽度，以及一个*通道*（channel）轴，用于堆叠颜色通道（红色、绿色和蓝色）。现在，我们将跳过高阶张量，集中在基础知识上。\n"]},{"cell_type":"code","metadata":{"origin_pos":39,"tab":["tensorflow"],"id":"UpM44zOKC9hG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616587392682,"user_tz":-480,"elapsed":5296,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"55338624-5600-42cd-e4ca-f4c001731daa"},"source":["X = tf.reshape(tf.range(24), (2, 3, 4))\n","X"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(2, 3, 4), dtype=int32, numpy=\n","array([[[ 0,  1,  2,  3],\n","        [ 4,  5,  6,  7],\n","        [ 8,  9, 10, 11]],\n","\n","       [[12, 13, 14, 15],\n","        [16, 17, 18, 19],\n","        [20, 21, 22, 23]]], dtype=int32)>"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"origin_pos":40,"id":"WLz198bGC9hH"},"source":["## 张量算法的基本性质\n","\n","标量、向量、矩阵和任意数量轴的张量（本小节中的 “张量” 指代数对象）有一些很好的属性，通常会派上用场。例如，你可能已经从按元素操作的定义中注意到，任何按元素的一元运算都不会改变其操作数的形状。同样，[**给定具有相同形状的任何两个张量，任何按元素二元运算的结果都将是相同形状的张量**]。例如，将两个相同形状的矩阵相加会在这两个矩阵上执行元素加法。\n"]},{"cell_type":"code","metadata":{"origin_pos":43,"tab":["tensorflow"],"id":"FRxHhRgIC9hH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617537132171,"user_tz":-480,"elapsed":1033,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"8f7cc460-03ce-47d3-ce8d-64a4d818c7a6"},"source":["A = tf.reshape(tf.range(20, dtype=tf.float32), (5, 4))\n","B = A  # 不能通过分配新内存将A克隆到B\n","A, A + B"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(5, 4), dtype=float32, numpy=\n"," array([[ 0.,  1.,  2.,  3.],\n","        [ 4.,  5.,  6.,  7.],\n","        [ 8.,  9., 10., 11.],\n","        [12., 13., 14., 15.],\n","        [16., 17., 18., 19.]], dtype=float32)>,\n"," <tf.Tensor: shape=(5, 4), dtype=float32, numpy=\n"," array([[ 0.,  2.,  4.,  6.],\n","        [ 8., 10., 12., 14.],\n","        [16., 18., 20., 22.],\n","        [24., 26., 28., 30.],\n","        [32., 34., 36., 38.]], dtype=float32)>)"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"origin_pos":44,"id":"9XLetpzpC9hH"},"source":["具体而言，[**两个矩阵的按元素乘法称为 *哈达玛积*（Hadamard product）（数学符号 $\\odot$）**]。对于矩阵 $\\mathbf{B} \\in \\mathbb{R}^{m \\times n}$，其中第 $i$ 行和第 $j$ 列的元素是 $b_{ij}$。矩阵$\\mathbf{A}$（在 :eqref:`eq_matrix_def` 中定义）和 $\\mathbf{B}$的哈达玛积为：\n","\n","$$\n","\\mathbf{A} \\odot \\mathbf{B} =\n","\\begin{bmatrix}\n","    a_{11}  b_{11} & a_{12}  b_{12} & \\dots  & a_{1n}  b_{1n} \\\\\n","    a_{21}  b_{21} & a_{22}  b_{22} & \\dots  & a_{2n}  b_{2n} \\\\\n","    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n","    a_{m1}  b_{m1} & a_{m2}  b_{m2} & \\dots  & a_{mn}  b_{mn}\n","\\end{bmatrix}.\n","$$\n"]},{"cell_type":"code","metadata":{"origin_pos":47,"tab":["tensorflow"],"id":"j73Vz1ZQC9hH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616587392683,"user_tz":-480,"elapsed":5281,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"b867e076-a564-44d9-c90c-9c2eb4e213d1"},"source":["A * B"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(5, 4), dtype=float32, numpy=\n","array([[  0.,   1.,   4.,   9.],\n","       [ 16.,  25.,  36.,  49.],\n","       [ 64.,  81., 100., 121.],\n","       [144., 169., 196., 225.],\n","       [256., 289., 324., 361.]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"markdown","metadata":{"origin_pos":48,"id":"1Ktxh4DCC9hH"},"source":["将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。\n"]},{"cell_type":"code","metadata":{"origin_pos":51,"tab":["tensorflow"],"id":"5q3ZC0DjC9hI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616587392684,"user_tz":-480,"elapsed":5272,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"fd98aeb1-af59-49d1-bb42-669d3ec40ec1"},"source":["a = 2\n","X = tf.reshape(tf.range(24), (2, 3, 4))\n","a + X, (a * X).shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(2, 3, 4), dtype=int32, numpy=\n"," array([[[ 2,  3,  4,  5],\n","         [ 6,  7,  8,  9],\n","         [10, 11, 12, 13]],\n"," \n","        [[14, 15, 16, 17],\n","         [18, 19, 20, 21],\n","         [22, 23, 24, 25]]], dtype=int32)>, TensorShape([2, 3, 4]))"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"markdown","metadata":{"origin_pos":52,"id":"sLZagznlC9hI"},"source":["## 汇总求和\n","\n","我们可以对任意张量进行的一个有用的操作是[**计算其元素的和**]。在数学表示法中，我们使用 $\\sum$ 符号表示求和。为了表示长度为$d$的向量中元素的总和，可以记为 $\\sum_{i=1}^d x_i$。在代码中，我们可以调用计算求和的函数：\n"]},{"cell_type":"code","metadata":{"origin_pos":55,"tab":["tensorflow"],"id":"138Ur3FGC9hI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617537881170,"user_tz":-480,"elapsed":1021,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"11ec5875-818a-4836-a438-0f98b35ea762"},"source":["x = tf.range(4, dtype=tf.float32)\n","x, tf.reduce_sum(x)"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 1., 2., 3.], dtype=float32)>,\n"," <tf.Tensor: shape=(), dtype=float32, numpy=6.0>)"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"origin_pos":56,"id":"xmKQDKmEC9hI"},"source":["我们可以(**表示任意形状张量的元素和**)。例如，矩阵 $\\mathbf{A}$ 中元素的和可以记为$\\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}$。\n"]},{"cell_type":"code","metadata":{"origin_pos":59,"tab":["tensorflow"],"id":"gP1Yx8kiC9hI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616587392684,"user_tz":-480,"elapsed":5256,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"4bcbda28-a3b6-41e1-c23b-39222d629fc1"},"source":["A.shape, tf.reduce_sum(A)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([5, 4]), <tf.Tensor: shape=(), dtype=float32, numpy=190.0>)"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"markdown","metadata":{"origin_pos":60,"id":"SuTCNr6aC9hI"},"source":["默认情况下，调用求和函数会将一个张量在所有轴上汇总为一个标量。\n","我们还可以[**指定求和汇总张量的轴**]。以矩阵为例。为了通过求和所有行的元素来汇总行维度（轴0），我们可以在调用函数时指定`axis=0`。\n","由于输入矩阵沿0轴汇总以生成输出向量，因此输入的轴0的维数在输出形状中丢失。\n"]},{"cell_type":"code","metadata":{"origin_pos":63,"tab":["tensorflow"],"id":"8gDlVKeEC9hJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616587392684,"user_tz":-480,"elapsed":5245,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"0b16b2e6-8a07-4dae-c42a-94a0cf3d127b"},"source":["A_sum_axis0 = tf.reduce_sum(A, axis=0)\n","A_sum_axis0, A_sum_axis0.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(4,), dtype=float32, numpy=array([40., 45., 50., 55.], dtype=float32)>,\n"," TensorShape([4]))"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"markdown","metadata":{"origin_pos":64,"id":"et-Z5zOiC9hJ"},"source":["指定 `axis=1` 将通过汇总所有列的元素来汇总列维度（轴 1）。因此，输入的轴1的维数在输出形状中丢失。\n"]},{"cell_type":"code","metadata":{"origin_pos":67,"tab":["tensorflow"],"id":"stZyIs_VC9hJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616587392685,"user_tz":-480,"elapsed":5238,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"359af233-a943-43e5-df92-49e1f9ce25b0"},"source":["A_sum_axis1 = tf.reduce_sum(A, axis=1)\n","A_sum_axis1, A_sum_axis1.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(5,), dtype=float32, numpy=array([ 6., 22., 38., 54., 70.], dtype=float32)>,\n"," TensorShape([5]))"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"markdown","metadata":{"origin_pos":72,"id":"Kd3fxJwrC9hJ"},"source":["[**一个与求和相关的量是 *平均值*（mean或average）**]。我们通过将总和除以元素总数来计算平均值。在代码中，我们可以调用函数来计算任意形状张量的平均值。\n"]},{"cell_type":"code","metadata":{"origin_pos":75,"tab":["tensorflow"],"id":"G6BNkKzwC9hJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616587392685,"user_tz":-480,"elapsed":5230,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"d0d3a9af-607f-456d-e825-7ceb0da11b18"},"source":["tf.reduce_mean(A), tf.reduce_sum(A) / tf.size(A).numpy()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(), dtype=float32, numpy=9.5>,\n"," <tf.Tensor: shape=(), dtype=float32, numpy=9.5>)"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"markdown","metadata":{"origin_pos":76,"id":"E3Slx8DzC9hK"},"source":["同样，计算平均值的函数也可以沿指定轴汇总张量。\n"]},{"cell_type":"code","metadata":{"origin_pos":79,"tab":["tensorflow"],"id":"sDmi_LVeC9hK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616587392685,"user_tz":-480,"elapsed":5222,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"47fec6a1-82ee-4e81-f466-35ec06e17e22"},"source":["tf.reduce_mean(A, axis=0), tf.reduce_sum(A, axis=0) / A.shape[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 8.,  9., 10., 11.], dtype=float32)>,\n"," <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 8.,  9., 10., 11.], dtype=float32)>)"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"markdown","metadata":{"origin_pos":80,"id":"h1yFAOV-C9hK"},"source":["## 非汇总求和\n","\n","但是，有时在调用函数来[**计算总和或均值时保持轴数不变**]会很有用。\n"]},{"cell_type":"code","metadata":{"origin_pos":83,"tab":["tensorflow"],"id":"_aQY_lAHC9hK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616587392686,"user_tz":-480,"elapsed":5215,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"183efa5d-e28d-485b-c454-6db277243109"},"source":["sum_A = tf.reduce_sum(A, axis=1, keepdims=True)\n","sum_A"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n","array([[ 6.],\n","       [22.],\n","       [38.],\n","       [54.],\n","       [70.]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"markdown","metadata":{"origin_pos":84,"id":"YxFXU59zC9hK"},"source":["例如，由于 `sum_A` 在对每行进行求和后仍保持两个轴，我们可以(**通过广播将 `A` 除以 `sum_A`**) 。\n"]},{"cell_type":"code","metadata":{"origin_pos":87,"tab":["tensorflow"],"id":"uF08KKhnC9hK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616587392687,"user_tz":-480,"elapsed":5209,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"c9916517-4a0e-43f2-8d99-16697817cb56"},"source":["A / sum_A"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(5, 4), dtype=float32, numpy=\n","array([[0.        , 0.16666667, 0.33333334, 0.5       ],\n","       [0.18181819, 0.22727273, 0.27272728, 0.3181818 ],\n","       [0.21052632, 0.23684211, 0.2631579 , 0.28947368],\n","       [0.22222222, 0.24074075, 0.25925925, 0.2777778 ],\n","       [0.22857143, 0.24285714, 0.25714287, 0.27142859]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"markdown","metadata":{"origin_pos":88,"id":"-qWvSeU2C9hL"},"source":["如果我们想沿[**某个轴计算 `A` 元素的累积总和**]，比如 `axis=0`（按行计算），我们可以调用 `cumsum` 函数。此函数不会沿任何轴汇总输入张量。\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JNAPWYxVIF5n","executionInfo":{"status":"ok","timestamp":1616587550142,"user_tz":-480,"elapsed":1652,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"bce0f897-c14a-4c7e-decd-c75b797d8c56"},"source":["A"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(5, 4), dtype=float32, numpy=\n","array([[ 0.,  1.,  2.,  3.],\n","       [ 4.,  5.,  6.,  7.],\n","       [ 8.,  9., 10., 11.],\n","       [12., 13., 14., 15.],\n","       [16., 17., 18., 19.]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"code","metadata":{"origin_pos":91,"tab":["tensorflow"],"id":"_snIRgvxC9hL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616587392687,"user_tz":-480,"elapsed":5202,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"259ddf73-0d33-4304-9f42-3af744ff26e0"},"source":["tf.cumsum(A, axis=0)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(5, 4), dtype=float32, numpy=\n","array([[ 0.,  1.,  2.,  3.],\n","       [ 4.,  6.,  8., 10.],\n","       [12., 15., 18., 21.],\n","       [24., 28., 32., 36.],\n","       [40., 45., 50., 55.]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"markdown","metadata":{"origin_pos":92,"id":"IPz1lpP6C9hL"},"source":["## 点积（Dot Product）\n","\n","到目前为止，我们只执行了按元素操作、求和及平均值。如果这就是我们所能做的，那么线性代数可能就不需要单独一节了。\n","但是，最基本的操作之一是点积。给定两个向量 $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d$，它们的 *点积*（dot product） $\\mathbf{x}^\\top \\mathbf{y}$（或 $\\langle \\mathbf{x}, \\mathbf{y}  \\rangle$）是相同位置的按元素乘积的和：$\\mathbf{x}^\\top \\mathbf{y} = \\sum_{i=1}^{d} x_i y_i$。\n","\n","[~~点积是相同位置的按元素乘积的和~~]\n"]},{"cell_type":"code","metadata":{"origin_pos":95,"tab":["tensorflow"],"id":"EFABhGwMC9hL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617537884363,"user_tz":-480,"elapsed":960,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"7d08895c-fb50-4476-bf64-eedea1043c15"},"source":["y = tf.ones(4, dtype=tf.float32)\n","x, y, tf.tensordot(x, y, axes=1)"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 1., 2., 3.], dtype=float32)>,\n"," <tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 1., 1., 1.], dtype=float32)>,\n"," <tf.Tensor: shape=(), dtype=float32, numpy=6.0>)"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"origin_pos":96,"id":"zb2hawJ8C9hL"},"source":["注意，(**我们可以通过执行按元素乘法，然后进行求和来表示两个向量的点积**)：\n"]},{"cell_type":"code","metadata":{"origin_pos":99,"tab":["tensorflow"],"id":"gw92L2G4C9hL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616587392688,"user_tz":-480,"elapsed":5190,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"08dfdf47-2190-405f-d029-981157f864e8"},"source":["tf.reduce_sum(x * y)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=float32, numpy=6.0>"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"markdown","metadata":{"origin_pos":100,"id":"1ULY1pQXC9hL"},"source":["点积在很多场合都很有用。例如，给定一组由向量$\\mathbf{x}  \\in \\mathbb{R}^d$ 表示的值，和一组由 $\\mathbf{w} \\in \\mathbb{R}^d$ 表示的权重。$\\mathbf{x}$ 中的值根据权重 $\\mathbf{w}$ 的加权和可以表示为点积 $\\mathbf{x}^\\top \\mathbf{w}$。当权重为非负数且和为1（即 $\\left(\\sum_{i=1}^{d} {w_i} = 1\\right)$）时，点积表示 *加权平均*（weighted average）。将两个向量归一化得到单位长度后，点积表示它们夹角的余弦。我们将在本节的后面正式介绍*长度*（length）的概念。\n","\n","## 矩阵-向量积\n","\n","现在我们知道如何计算点积，我们可以开始理解 *矩阵-向量积*（matrix-vector products）。回顾分别在 :eqref:`eq_matrix_def` 和 :eqref:`eq_vec_def` 中定义和可视化的矩阵 $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ 和向量 $\\mathbf{x} \\in \\mathbb{R}^n$。让我们从可视化矩阵$\\mathbf{A}$开始，用它的行向量表示\n","\n","$$\\mathbf{A}=\n","\\begin{bmatrix}\n","\\mathbf{a}^\\top_{1} \\\\\n","\\mathbf{a}^\\top_{2} \\\\\n","\\vdots \\\\\n","\\mathbf{a}^\\top_m \\\\\n","\\end{bmatrix},$$\n","\n","其中每个$\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^n$ 都是行向量，表示矩阵的 $i^\\mathrm{th}$ 行。[**矩阵向量积 $\\mathbf{A}\\mathbf{x}$ 是一个长度为 $m$ 的列向量，其 $i^\\mathrm{th}$ 元素是点积 $\\mathbf{a}^\\top_i \\mathbf{x}$**]：\n","\n","$$\n","\\mathbf{A}\\mathbf{x}\n","= \\begin{bmatrix}\n","\\mathbf{a}^\\top_{1} \\\\\n","\\mathbf{a}^\\top_{2} \\\\\n","\\vdots \\\\\n","\\mathbf{a}^\\top_m \\\\\n","\\end{bmatrix}\\mathbf{x}\n","= \\begin{bmatrix}\n"," \\mathbf{a}^\\top_{1} \\mathbf{x}  \\\\\n"," \\mathbf{a}^\\top_{2} \\mathbf{x} \\\\\n","\\vdots\\\\\n"," \\mathbf{a}^\\top_{m} \\mathbf{x}\\\\\n","\\end{bmatrix}.\n","$$\n","\n","我们可以把一个矩阵 $\\mathbf{A}\\in \\mathbb{R}^{m \\times n}$ 乘法看作是一个从 $\\mathbb{R}^{n}$ 到 $\\mathbb{R}^{m}$ 向量的转换。这些转换证明是非常有用的。例如，我们可以用方阵的乘法来表示旋转。\n","我们将在后续章节中讲到，我们也可以使用矩阵向量乘积来描述在给定前一层的值时计算神经网络的每一层所需要的计算。\n","\n","在代码中使用张量表示矩阵向量积，我们使用与点积相同的 `dot` 函数。当我们为矩阵 `A` 和向量 `x` 调用 `np.dot(A, x)`时，会执行矩阵向量积。注意，`A` 的列维数（沿轴1的长度）必须与 `x` 的维数（其长度）相同。\n"]},{"cell_type":"code","metadata":{"origin_pos":103,"tab":["tensorflow"],"id":"SBmROirhC9hM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617538115014,"user_tz":-480,"elapsed":945,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"a9767b88-7b4e-4ea7-aa3a-a053365412b4"},"source":["A.shape, x.shape, tf.linalg.matvec(A, x)"],"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([5, 4]),\n"," TensorShape([4]),\n"," <tf.Tensor: shape=(5,), dtype=float32, numpy=array([ 14.,  38.,  62.,  86., 110.], dtype=float32)>)"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"markdown","metadata":{"origin_pos":104,"id":"8vHXC7qvC9hM"},"source":["## 矩阵-矩阵乘法\n","\n","如果你已经掌握了点积和矩阵-向量积的知识，那么 **矩阵-矩阵乘法**（matrix-matrix multiplication） 应该很简单。\n","\n","假设我们有两个矩阵 $\\mathbf{A} \\in \\mathbb{R}^{n \\times k}$ 和 $\\mathbf{B} \\in \\mathbb{R}^{k \\times m}$：\n","\n","$$\\mathbf{A}=\\begin{bmatrix}\n"," a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n"," a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n"," a_{n1} & a_{n2} & \\cdots & a_{nk} \\\\\n","\\end{bmatrix},\\quad\n","\\mathbf{B}=\\begin{bmatrix}\n"," b_{11} & b_{12} & \\cdots & b_{1m} \\\\\n"," b_{21} & b_{22} & \\cdots & b_{2m} \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n"," b_{k1} & b_{k2} & \\cdots & b_{km} \\\\\n","\\end{bmatrix}.$$\n","\n","用行向量$\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^k$ 表示矩阵$\\mathbf{A}$的 $i^\\mathrm{th}$ 行，并让列向量$\\mathbf{b}_{j} \\in \\mathbb{R}^k$ 作为矩阵$\\mathbf{B}$的 $j^\\mathrm{th}$ 列。要生成矩阵积 $\\mathbf{C} = \\mathbf{A}\\mathbf{B}$，最简单的方法是考虑$\\mathbf{A}$的行向量和$\\mathbf{B}$的列向量:\n","\n","$$\\mathbf{A}=\n","\\begin{bmatrix}\n","\\mathbf{a}^\\top_{1} \\\\\n","\\mathbf{a}^\\top_{2} \\\\\n","\\vdots \\\\\n","\\mathbf{a}^\\top_n \\\\\n","\\end{bmatrix},\n","\\quad \\mathbf{B}=\\begin{bmatrix}\n"," \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n","\\end{bmatrix}.\n","$$\n","\n","当我们简单地将每个元素$c_{ij}$计算为点积$\\mathbf{a}^\\top_i \\mathbf{b}_j$:\n","\n","$$\\mathbf{C} = \\mathbf{AB} = \\begin{bmatrix}\n","\\mathbf{a}^\\top_{1} \\\\\n","\\mathbf{a}^\\top_{2} \\\\\n","\\vdots \\\\\n","\\mathbf{a}^\\top_n \\\\\n","\\end{bmatrix}\n","\\begin{bmatrix}\n"," \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n","\\end{bmatrix}\n","= \\begin{bmatrix}\n","\\mathbf{a}^\\top_{1} \\mathbf{b}_1 & \\mathbf{a}^\\top_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^\\top_{1} \\mathbf{b}_m \\\\\n"," \\mathbf{a}^\\top_{2}\\mathbf{b}_1 & \\mathbf{a}^\\top_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^\\top_{2} \\mathbf{b}_m \\\\\n"," \\vdots & \\vdots & \\ddots &\\vdots\\\\\n","\\mathbf{a}^\\top_{n} \\mathbf{b}_1 & \\mathbf{a}^\\top_{n}\\mathbf{b}_2& \\cdots& \\mathbf{a}^\\top_{n} \\mathbf{b}_m\n","\\end{bmatrix}.\n","$$\n","\n","[**我们可以将矩阵-矩阵乘法 $\\mathbf{AB}$ 看作是简单地执行 $m$次矩阵-向量积，并将结果拼接在一起，形成一个 $n \\times m$ 矩阵**]。在下面的代码中，我们在 `A` 和 `B` 上执行矩阵乘法。这里的`A` 是一个5行4列的矩阵，`B`是一个4行3列的矩阵。相乘后，我们得到了一个5行3列的矩阵。\n"]},{"cell_type":"code","metadata":{"origin_pos":107,"tab":["tensorflow"],"id":"5T4mvpr5C9hN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616587392688,"user_tz":-480,"elapsed":5175,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"771aa988-fc67-4153-a798-c763bbd21a92"},"source":["B = tf.ones((4, 3), tf.float32)\n","tf.matmul(A, B)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(5, 3), dtype=float32, numpy=\n","array([[ 6.,  6.,  6.],\n","       [22., 22., 22.],\n","       [38., 38., 38.],\n","       [54., 54., 54.],\n","       [70., 70., 70.]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"markdown","metadata":{"origin_pos":108,"id":"kLrkXmUiC9hN"},"source":["矩阵矩阵乘法可以简单地称为 **矩阵乘法**，不应与 哈达玛积 混淆。\n","\n","## 范数\n","\n","线性代数中一些最有用的运算符是 *范数*（norms）。非正式地说，一个向量的*范数*告诉我们一个向量有多大。\n","这里考虑的 *大小*（size） 概念不涉及维度，而是分量的大小。\n","\n","**在线性代数中，向量范数是将向量映射到标量的函数** $f$。向量范数要满足一些属性。\n","给定任意向量 $\\mathbf{x}$，第一个性质说，如果我们按常数因子 $\\alpha$ 缩放向量的所有元素，其范数也会按相同常数因子的 *绝对值* 缩放：\n","\n","$$f(\\alpha \\mathbf{x}) = |\\alpha| f(\\mathbf{x}).$$\n","\n","第二个性质是我们熟悉的三角不等式:\n","\n","$$f(\\mathbf{x} + \\mathbf{y}) \\leq f(\\mathbf{x}) + f(\\mathbf{y}).$$\n","\n","第三个性质简单地说范数必须是非负的:\n","\n","$$f(\\mathbf{x}) \\geq 0.$$\n","\n","这是有道理的，因为在大多数情况下，任何东西的最小的*大小*是0。最后一个性质要求最小范数，并且只有由所有零组成的向量才能达到最小范数。\n","\n","$$\\forall i, [\\mathbf{x}]_i = 0 \\Leftrightarrow f(\\mathbf{x})=0.$$\n","\n","你可能会注意到，范数听起来很像距离的度量。如果你还记得小学时的欧几里得距离(想想毕达哥拉斯定理)，那么非负性的概念和三角不等式可能会给你一些启发。\n","事实上，欧几里得距离是一个范数：具体而言，它是 $L_2$ 范数。假设$n$-维向量$\\mathbf{x}$中的元素是$x_1, \\ldots, x_n$ 的 [**$L_2$ *范数* 是向量元素平方和的平方根：**]\n","\n","(**$$\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2},$$**)\n","\n","其中，在 $L_2$ 范数中常常省略下标 $2$，也就是说，$\\|\\mathbf{x}\\|$ 等同于 $\\|\\mathbf{x}\\|_2$。在代码中，我们可以按如下方式计算向量的 $L_2$ 范数。\n"]},{"cell_type":"code","metadata":{"origin_pos":111,"tab":["tensorflow"],"id":"KTZyC-ycC9hN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616587392689,"user_tz":-480,"elapsed":5169,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"4687c21d-40aa-4037-d4f7-15f162dfad9e"},"source":["u = tf.constant([3.0, -4.0])\n","tf.norm(u)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=float32, numpy=5.0>"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"markdown","metadata":{"origin_pos":112,"id":"MVBVv5WsC9hN"},"source":["在深度学习中，我们更经常地使用平方 $L_2$ 范数。你还会经常遇到 [**$L_1$ 范数，它表示为向量元素的绝对值之和：**]\n","\n","(**$$\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|.$$**)\n","\n","与 $L_2$ 范数相比，$L_1$ 范数受异常值的影响较小。为了计算 $L_1$ 范数，我们将绝对值函数和按元素求和组合起来。\n"]},{"cell_type":"code","metadata":{"origin_pos":115,"tab":["tensorflow"],"id":"__uy2MpEC9hN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616587392689,"user_tz":-480,"elapsed":5162,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"3328af76-66cf-4a81-9766-488022374531"},"source":["tf.reduce_sum(tf.abs(u))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=float32, numpy=7.0>"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"markdown","metadata":{"origin_pos":116,"id":"oprGPb1nC9hO"},"source":["$L_2$ 范数和 $L_1$ 范数都是更一般的$L_p$范数的特例：\n","\n","$$\\|\\mathbf{x}\\|_p = \\left(\\sum_{i=1}^n \\left|x_i \\right|^p \\right)^{1/p}.$$\n","\n","类似于向量的$L_2$ 范数，[**矩阵**] $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$ (**的 *弗罗贝尼乌斯范数*（Frobenius norm） 是矩阵元素的平方和的平方根：**)\n","\n","(**$$\\|\\mathbf{X}\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}.$$**)\n","\n","弗罗贝尼乌斯范数满足向量范数的所有性质。它的行为就好像它是矩阵形向量的 $L_2$ 范数。调用以下函数将计算矩阵的弗罗贝尼乌斯范数。\n"]},{"cell_type":"code","metadata":{"origin_pos":119,"tab":["tensorflow"],"id":"kXUBBCj2C9hO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616587392689,"user_tz":-480,"elapsed":5154,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"ffdebfde-4a40-4fa3-aedf-597bacc8d74f"},"source":["tf.norm(tf.ones((4, 9)))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=float32, numpy=6.0>"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"markdown","metadata":{"id":"CReJd8vwqwKb"},"source":["# 微分"]},{"cell_type":"markdown","metadata":{"origin_pos":0,"id":"VrUJ7gHxqr9X"},"source":["\n","在深度学习中，我们“训练”模型，不断更新它们，使它们在看到越来越多的数据时变得越来越好。通常情况下，变得更好意味着最小化一个 *损失函数*（loss function），即一个衡量“我们的模型有多糟糕”这个问题的分数。这个问题比看上去要微妙得多。最终，我们真正关心的是生成一个能够在我们从未见过的数据上表现良好的模型。但我们只能将模型与我们实际能看到的数据相拟合。因此，我们可以将拟合模型的任务分解为两个关键问题：（1）*优化*（optimization）：用模型拟合观测数据的过程；（2）*泛化*（generalization）：数学原理和实践者的智慧，能够指导我们生成出有效性超出用于训练的数据集本身的模型。\n","\n","为了帮助你在后面的章节中更好地理解优化问题和方法，这里我们对深度学习中常用的微分知识提供了一个非常简短的入门教程。\n","\n","## 导数和微分\n","\n","(**定义$u = f(x) = 3x^2-4x$.**)\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-OS4qUF9rjAh","collapsed":true,"executionInfo":{"status":"ok","timestamp":1617538527028,"user_tz":-480,"elapsed":5027,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"5fafdcbf-991f-4abf-fd3e-6d743fd37df1"},"source":["!pip install d2l"],"execution_count":38,"outputs":[{"output_type":"stream","text":["Collecting d2l\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/1f/13de7e8cafaba15739caee0596032412aaf51a22726649b317bdb53c4f9a/d2l-0.16.2-py3-none-any.whl (77kB)\n","\r\u001b[K     |████▎                           | 10kB 17.2MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20kB 11.5MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 30kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 51kB 3.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 61kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 3.3MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from d2l) (2.23.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from d2l) (1.1.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from d2l) (3.2.2)\n","Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from d2l) (1.0.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from d2l) (1.19.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->d2l) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->d2l) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->d2l) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->d2l) (2020.12.5)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->d2l) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->d2l) (2.8.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l) (1.3.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l) (2.4.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l) (0.10.0)\n","Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (5.3.1)\n","Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (5.2.0)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (4.10.1)\n","Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (7.6.3)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (5.6.1)\n","Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (5.0.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->d2l) (1.15.0)\n","Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l) (5.0.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l) (2.11.3)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l) (0.9.3)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l) (5.1.2)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l) (1.5.0)\n","Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l) (4.7.1)\n","Requirement already satisfied: jupyter-client>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l) (5.3.5)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l) (0.2.0)\n","Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l) (5.1.1)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from jupyter-console->jupyter->d2l) (5.5.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from jupyter-console->jupyter->d2l) (2.6.1)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-console->jupyter->d2l) (1.0.18)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l) (1.0.0)\n","Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l) (3.5.1)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (0.3)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (0.8.4)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (1.4.3)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (0.7.1)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (3.3.0)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (0.4.4)\n","Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->d2l) (22.0.3)\n","Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->d2l) (1.9.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter->d2l) (1.1.1)\n","Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->d2l) (0.7.0)\n","Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter->d2l) (2.6.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyter-console->jupyter->d2l) (54.2.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->jupyter-console->jupyter->d2l) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->jupyter-console->jupyter->d2l) (0.7.5)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyter-console->jupyter->d2l) (0.8.1)\n","Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->jupyter-console->jupyter->d2l) (4.8.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->d2l) (0.2.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->d2l) (20.9)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->d2l) (0.5.1)\n","Installing collected packages: d2l\n","Successfully installed d2l-0.16.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"origin_pos":3,"tab":["tensorflow"],"id":"ESpldYXZqr9b","executionInfo":{"status":"ok","timestamp":1617538533194,"user_tz":-480,"elapsed":932,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}}},"source":["%matplotlib inline\n","import numpy as np\n","from IPython import display\n","from d2l import tensorflow as d2l\n","\n","\n","def f(x):\n","    return 3 * x ** 2 - 4 * x"],"execution_count":39,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":4,"id":"WGutL4k2qr9b"},"source":["[**通过令 $x=1$ 并让 $h$ 接近 $0$，**] :eqref:`eq_derivative` 中(**$\\frac{f(x+h) - f(x)}{h}$ 的数值结果接近 $2$**)。虽然这个实验不是一个数学证明，但我们稍后会看到，当 $x=1$时，导数 $u'$是 $2$ 。\n"]},{"cell_type":"code","metadata":{"origin_pos":5,"tab":["tensorflow"],"id":"B93b37LTqr9b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617418950709,"user_tz":-480,"elapsed":1370,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"e889b752-6cec-47da-c8d3-0a28ea96daa2"},"source":["def numerical_lim(f, x, h):\n","    return (f(x + h) - f(x)) / h\n","\n","h = 0.1\n","for i in range(5):\n","    print(f'h={h:.5f}, numerical limit={numerical_lim(f, 1, h):.5f}')\n","    h *= 0.1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["h=0.10000, numerical limit=2.30000\n","h=0.01000, numerical limit=2.03000\n","h=0.00100, numerical limit=2.00300\n","h=0.00010, numerical limit=2.00030\n","h=0.00001, numerical limit=2.00003\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"origin_pos":6,"id":"KxCZXoVWqr9c"},"source":["让我们熟悉一下导数的几个等价符号。\n","给定 $y = f(x)$，其中 $x$ 和 $y$ 分别是函数 $f$ 的自变量和因变量。以下表达式是等价的：\n","\n","$$f'(x) = y' = \\frac{dy}{dx} = \\frac{df}{dx} = \\frac{d}{dx} f(x) = Df(x) = D_x f(x),$$\n","\n","其中符号 $\\frac{d}{dx}$ 和 $D$ 是*微分运算符*，表示*微分*操作。我们可以使用以下规则来对常见函数求微分：\n","\n","* $DC = 0$ （$C$ 是一个常数）\n","* $Dx^n = nx^{n-1}$ （*幂律*（power rule）, $n$是任意实数）\n","* $De^x = e^x$\n","* $D\\ln(x) = 1/x$\n","\n","为了微分一个由一些简单函数（如上面的常见函数）组成的函数，下面的法则使用起来很方便。\n","假设函数$f$和$g$都是可微的，$C$是一个常数，我们有：\n","\n","*常数相乘法则*\n","$$\\frac{d}{dx} [Cf(x)] = C \\frac{d}{dx} f(x),$$\n","\n","*加法法则*\n","\n","$$\\frac{d}{dx} [f(x) + g(x)] = \\frac{d}{dx} f(x) + \\frac{d}{dx} g(x),$$\n","\n","*乘法法则*\n","\n","$$\\frac{d}{dx} [f(x)g(x)] = f(x) \\frac{d}{dx} [g(x)] + g(x) \\frac{d}{dx} [f(x)],$$\n","\n","*除法法则*\n","\n","$$\\frac{d}{dx} \\left[\\frac{f(x)}{g(x)}\\right] = \\frac{g(x) \\frac{d}{dx} [f(x)] - f(x) \\frac{d}{dx} [g(x)]}{[g(x)]^2}.$$\n","\n","现在我们可以应用上述几个法则来计算 $u' = f'(x) = 3 \\frac{d}{dx} x^2-4\\frac{d}{dx}x = 6x-4$。因此，通过令 $x = 1$ ，我们有 $u' = 2$ ：这一点得到了我们在本节前面的实验的支持，在这个实验中，数值结果接近$2$。当 $x=1$ 时，此导数也是曲线 $u = f(x)$ 切线的斜率。\n","\n","[**为了对导数的这种解释进行可视化，**]我们将使用 `matplotlib`，这是一个Python中流行的绘图库。要配置`matplotlib`生成图形的属性，我们需要(**定义几个函数**)。\n","在下面，`use_svg_display` 函数指定 `matplotlib` 软件包输出svg图表以获得更清晰的图像。\n","\n","注意，注释`#@save`是一个特殊的标记，会将对应的函数、类或语句保存在`d2l`包中\n","因此，以后无需重新定义就可以直接调用它们（例如，`d2l.use_svg_display()`）。\n"]},{"cell_type":"code","metadata":{"origin_pos":7,"tab":["tensorflow"],"id":"Dx-k84Lzqr9c"},"source":["def use_svg_display():  #@save\n","    \"\"\"使用svg格式在Jupyter中显示绘图。\"\"\"\n","    display.set_matplotlib_formats('svg')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":8,"id":"fITNNIqzqr9d"},"source":["我们定义 `set_figsize` 函数来设置图表大小。注意，这里我们直接使用 `d2l.plt`，因为导入语句 `from matplotlib import pyplot as plt` 已在前言中标记为保存到`d2l` 包中。\n"]},{"cell_type":"code","metadata":{"origin_pos":9,"tab":["tensorflow"],"id":"-lOvDtx7qr9d"},"source":["def set_figsize(figsize=(3.5, 2.5)):  #@save\n","    \"\"\"设置matplotlib的图表大小。\"\"\"\n","    use_svg_display()\n","    d2l.plt.rcParams['figure.figsize'] = figsize"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":10,"id":"JFDnJAZEqr9d"},"source":["下面的`set_axes`函数用于设置由`matplotlib`生成图表的轴的属性。\n"]},{"cell_type":"code","metadata":{"origin_pos":11,"tab":["tensorflow"],"id":"zC28jRV8qr9d"},"source":["#@save\n","def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n","    \"\"\"设置matplotlib的轴。\"\"\"\n","    axes.set_xlabel(xlabel)\n","    axes.set_ylabel(ylabel)\n","    axes.set_xscale(xscale)\n","    axes.set_yscale(yscale)\n","    axes.set_xlim(xlim)\n","    axes.set_ylim(ylim)\n","    if legend:\n","        axes.legend(legend)\n","    axes.grid()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":12,"id":"P4ZrFU4uqr9e"},"source":["通过这三个用于图形配置的函数，我们定义了 `plot` 函数来简洁地绘制多条曲线，因为我们需要在整个书中可视化许多曲线。\n"]},{"cell_type":"code","metadata":{"origin_pos":13,"tab":["tensorflow"],"id":"MmUlTRCAqr9e"},"source":["#@save\n","def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,\n","         ylim=None, xscale='linear', yscale='linear',\n","         fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):\n","    \"\"\"绘制数据点。\"\"\"\n","    if legend is None:\n","        legend = []\n","\n","    set_figsize(figsize)\n","    axes = axes if axes else d2l.plt.gca()\n","\n","    # 如果 `X` 有一个轴，输出True\n","    def has_one_axis(X):\n","        return (hasattr(X, \"ndim\") and X.ndim == 1 or\n","                isinstance(X, list) and not hasattr(X[0], \"__len__\"))\n","\n","    if has_one_axis(X):\n","        X = [X]\n","    if Y is None:\n","        X, Y = [[]] * len(X), X\n","    elif has_one_axis(Y):\n","        Y = [Y]\n","    if len(X) != len(Y):\n","        X = X * len(Y)\n","    axes.cla()\n","    for x, y, fmt in zip(X, Y, fmts):\n","        if len(x):\n","            axes.plot(x, y, fmt)\n","        else:\n","            axes.plot(y, fmt)\n","    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":14,"id":"qFKfwFWwqr9e"},"source":["现在我们可以[**绘制函数 $u = f(x)$ 及其在 $x=1$ 处的切线 $y = 2x - 3$**]，其中系数$2$是切线的斜率。\n"]},{"cell_type":"code","metadata":{"origin_pos":15,"tab":["tensorflow"],"id":"FUCnI3Irqr9e","outputId":"4901db3e-a515-4d0d-eff8-938c20409c71"},"source":["x = np.arange(0, 3, 0.1)\n","plot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/svg+xml":"<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"180.65625pt\" version=\"1.1\" viewBox=\"0 0 243.529359 180.65625\" width=\"243.529359pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-03-07T23:11:05.837250</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.0rc1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 180.65625 \nL 243.529359 180.65625 \nL 243.529359 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 40.603125 143.1 \nL 235.903125 143.1 \nL 235.903125 7.2 \nL 40.603125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p695c221bc2)\" d=\"M 49.480398 143.1 \nL 49.480398 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m94a51f1a02\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"49.480398\" xlink:href=\"#m94a51f1a02\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(46.299148 157.698438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p695c221bc2)\" d=\"M 110.702968 143.1 \nL 110.702968 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"110.702968\" xlink:href=\"#m94a51f1a02\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 1 -->\n      <g transform=\"translate(107.521718 157.698438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p695c221bc2)\" d=\"M 171.925539 143.1 \nL 171.925539 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"171.925539\" xlink:href=\"#m94a51f1a02\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 2 -->\n      <g transform=\"translate(168.744289 157.698438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p695c221bc2)\" d=\"M 233.148109 143.1 \nL 233.148109 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"233.148109\" xlink:href=\"#m94a51f1a02\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 3 -->\n      <g transform=\"translate(229.966859 157.698438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_5\">\n     <!-- x -->\n     <g transform=\"translate(135.29375 171.376563)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 3513 3500 \nL 2247 1797 \nL 3578 0 \nL 2900 0 \nL 1881 1375 \nL 863 0 \nL 184 0 \nL 1544 1831 \nL 300 3500 \nL 978 3500 \nL 1906 2253 \nL 2834 3500 \nL 3513 3500 \nz\n\" id=\"DejaVuSans-78\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-78\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p695c221bc2)\" d=\"M 40.603125 114.635514 \nL 235.903125 114.635514 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mdf8df5644e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mdf8df5644e\" y=\"114.635514\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0 -->\n      <g transform=\"translate(27.240625 118.434732)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p695c221bc2)\" d=\"M 40.603125 77.490157 \nL 235.903125 77.490157 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mdf8df5644e\" y=\"77.490157\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 5 -->\n      <g transform=\"translate(27.240625 81.289376)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p695c221bc2)\" d=\"M 40.603125 40.344801 \nL 235.903125 40.344801 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mdf8df5644e\" y=\"40.344801\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 10 -->\n      <g transform=\"translate(20.878125 44.14402)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- f(x) -->\n     <g transform=\"translate(14.798437 83.771094)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2375 4863 \nL 2375 4384 \nL 1825 4384 \nQ 1516 4384 1395 4259 \nQ 1275 4134 1275 3809 \nL 1275 3500 \nL 2222 3500 \nL 2222 3053 \nL 1275 3053 \nL 1275 0 \nL 697 0 \nL 697 3053 \nL 147 3053 \nL 147 3500 \nL 697 3500 \nL 697 3744 \nQ 697 4328 969 4595 \nQ 1241 4863 1831 4863 \nL 2375 4863 \nz\n\" id=\"DejaVuSans-66\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1984 4856 \nQ 1566 4138 1362 3434 \nQ 1159 2731 1159 2009 \nQ 1159 1288 1364 580 \nQ 1569 -128 1984 -844 \nL 1484 -844 \nQ 1016 -109 783 600 \nQ 550 1309 550 2009 \nQ 550 2706 781 3412 \nQ 1013 4119 1484 4856 \nL 1984 4856 \nz\n\" id=\"DejaVuSans-28\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 513 4856 \nL 1013 4856 \nQ 1481 4119 1714 3412 \nQ 1947 2706 1947 2009 \nQ 1947 1309 1714 600 \nQ 1481 -109 1013 -844 \nL 513 -844 \nQ 928 -128 1133 580 \nQ 1338 1288 1338 2009 \nQ 1338 2731 1133 3434 \nQ 928 4138 513 4856 \nz\n\" id=\"DejaVuSans-29\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"35.205078\" xlink:href=\"#DejaVuSans-28\"/>\n      <use x=\"74.21875\" xlink:href=\"#DejaVuSans-78\"/>\n      <use x=\"133.398438\" xlink:href=\"#DejaVuSans-29\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p695c221bc2)\" d=\"M 49.480398 114.635514 \nL 55.602655 117.38427 \nL 61.724912 119.687282 \nL 67.847169 121.54455 \nL 73.969426 122.956073 \nL 80.091683 123.921853 \nL 86.21394 124.441888 \nL 92.336197 124.516178 \nL 98.458454 124.144725 \nL 104.580711 123.327527 \nL 110.702968 122.064585 \nL 116.825225 120.355898 \nL 122.947482 118.201468 \nL 129.069739 115.601293 \nL 135.191996 112.555374 \nL 141.314254 109.06371 \nL 147.436511 105.126302 \nL 153.558768 100.74315 \nL 159.681025 95.914254 \nL 165.803282 90.639614 \nL 171.925539 84.919229 \nL 178.047796 78.7531 \nL 184.170053 72.141226 \nL 190.29231 65.083608 \nL 196.414567 57.580247 \nL 202.536824 49.63114 \nL 208.659081 41.23629 \nL 214.781338 32.395695 \nL 220.903595 23.109356 \nL 227.025852 13.377273 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#p695c221bc2)\" d=\"M 49.480398 136.922727 \nL 55.602655 135.436913 \nL 61.724912 133.951099 \nL 67.847169 132.465285 \nL 73.969426 130.97947 \nL 80.091683 129.493656 \nL 86.21394 128.007842 \nL 92.336197 126.522028 \nL 98.458454 125.036213 \nL 104.580711 123.550399 \nL 110.702968 122.064585 \nL 116.825225 120.578771 \nL 122.947482 119.092956 \nL 129.069739 117.607142 \nL 135.191996 116.121328 \nL 141.314254 114.635514 \nL 147.436511 113.149699 \nL 153.558768 111.663885 \nL 159.681025 110.178071 \nL 165.803282 108.692257 \nL 171.925539 107.206442 \nL 178.047796 105.720628 \nL 184.170053 104.234814 \nL 190.29231 102.749 \nL 196.414567 101.263185 \nL 202.536824 99.777371 \nL 208.659081 98.291557 \nL 214.781338 96.805743 \nL 220.903595 95.319928 \nL 227.025852 93.834114 \n\" style=\"fill:none;stroke:#bf00bf;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 40.603125 143.1 \nL 40.603125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 235.903125 143.1 \nL 235.903125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 40.603125 143.1 \nL 235.903125 143.1 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 40.603125 7.2 \nL 235.903125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 47.603125 44.55625 \nL 172.153125 44.55625 \nQ 174.153125 44.55625 174.153125 42.55625 \nL 174.153125 14.2 \nQ 174.153125 12.2 172.153125 12.2 \nL 47.603125 12.2 \nQ 45.603125 12.2 45.603125 14.2 \nL 45.603125 42.55625 \nQ 45.603125 44.55625 47.603125 44.55625 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_17\">\n     <path d=\"M 49.603125 20.298437 \nL 69.603125 20.298437 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_18\"/>\n    <g id=\"text_10\">\n     <!-- f(x) -->\n     <g transform=\"translate(77.603125 23.798437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"35.205078\" xlink:href=\"#DejaVuSans-28\"/>\n      <use x=\"74.21875\" xlink:href=\"#DejaVuSans-78\"/>\n      <use x=\"133.398438\" xlink:href=\"#DejaVuSans-29\"/>\n     </g>\n    </g>\n    <g id=\"line2d_19\">\n     <path d=\"M 49.603125 34.976562 \nL 69.603125 34.976562 \n\" style=\"fill:none;stroke:#bf00bf;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_20\"/>\n    <g id=\"text_11\">\n     <!-- Tangent line (x=1) -->\n     <g transform=\"translate(77.603125 38.476562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M -19 4666 \nL 3928 4666 \nL 3928 4134 \nL 2272 4134 \nL 2272 0 \nL 1638 0 \nL 1638 4134 \nL -19 4134 \nL -19 4666 \nz\n\" id=\"DejaVuSans-54\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" id=\"DejaVuSans-67\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 678 2906 \nL 4684 2906 \nL 4684 2381 \nL 678 2381 \nL 678 2906 \nz\nM 678 1631 \nL 4684 1631 \nL 4684 1100 \nL 678 1100 \nL 678 1631 \nz\n\" id=\"DejaVuSans-3d\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-54\"/>\n      <use x=\"44.583984\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"105.863281\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"169.242188\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"232.71875\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"294.242188\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"357.621094\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"396.830078\" xlink:href=\"#DejaVuSans-20\"/>\n      <use x=\"428.617188\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"456.400391\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"484.183594\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"547.5625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"609.085938\" xlink:href=\"#DejaVuSans-20\"/>\n      <use x=\"640.873047\" xlink:href=\"#DejaVuSans-28\"/>\n      <use x=\"679.886719\" xlink:href=\"#DejaVuSans-78\"/>\n      <use x=\"739.066406\" xlink:href=\"#DejaVuSans-3d\"/>\n      <use x=\"822.855469\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"886.478516\" xlink:href=\"#DejaVuSans-29\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p695c221bc2\">\n   <rect height=\"135.9\" width=\"195.3\" x=\"40.603125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n","text/plain":["<Figure size 252x180 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"origin_pos":16,"id":"8VIDmsc2qr9e"},"source":["## 偏导数\n","\n","到目前为止，我们只讨论了仅含一个变量的函数的微分。在深度学习中，函数通常依赖于许多变量。因此，我们需要将微分的思想推广到这些 *多元函数* （multivariate function）上。\n","\n","设 $y = f(x_1, x_2, \\ldots, x_n)$ 是一个具有 $n$ 个变量的函数。$y$ 关于第$i$ 个参数$x_i$的*偏导数*（partial derivative）为：\n","\n","$$ \\frac{\\partial y}{\\partial x_i} = \\lim_{h \\rightarrow 0} \\frac{f(x_1, \\ldots, x_{i-1}, x_i+h, x_{i+1}, \\ldots, x_n) - f(x_1, \\ldots, x_i, \\ldots, x_n)}{h}.$$\n","\n","为了计算 $\\frac{\\partial y}{\\partial x_i}$，我们可以简单地将 $x_1, \\ldots, x_{i-1}, x_{i+1}, \\ldots, x_n$ 看作常数，并计算 $y$关于$x_i$ 的导数。对于偏导数的表示，以下是等价的：\n","\n","$$\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial f}{\\partial x_i} = f_{x_i} = f_i = D_i f = D_{x_i} f.$$\n","\n","## 梯度\n","\n","我们可以连结一个多元函数对其所有变量的偏导数，以得到该函数的*梯度*（gradient）向量。设函数 $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ 的输入是一个 $n$ 维向量 $\\mathbf{x} = [x_1, x_2, \\ldots, x_n]^\\top$，并且输出是一个标量。\n","函数$f(\\mathbf{x})$相对于$\\mathbf{x}$的梯度是一个包含$n$个偏导数的向量:\n","\n","$$\\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\bigg[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_n}\\bigg]^\\top,$$\n","\n","其中 $\\nabla_{\\mathbf{x}} f(\\mathbf{x})$ 通常在没有歧义时被 $\\nabla f(\\mathbf{x})$ 取代。\n","\n","假设$\\mathbf{x}$为$n$维向量，在微分多元函数时经常使用以下规则:\n","\n","* 对于所有$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$，都有 $\\nabla_{\\mathbf{x}} \\mathbf{A} \\mathbf{x} = \\mathbf{A}^\\top$\n","* 对于所有$\\mathbf{A} \\in \\mathbb{R}^{n \\times m}$，都有 $\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A}  = \\mathbf{A}$\n","* 对于所有$\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$，都有 $\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x}  = (\\mathbf{A} + \\mathbf{A}^\\top)\\mathbf{x}$\n","* $\\nabla_{\\mathbf{x}} \\|\\mathbf{x} \\|^2 = \\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{x} = 2\\mathbf{x}$\n","\n","同样，对于任何矩阵 $\\mathbf{X}$，我们都有 $\\nabla_{\\mathbf{X}} \\|\\mathbf{X} \\|_F^2 = 2\\mathbf{X}$。正如我们之后将看到的，梯度对于设计深度学习中的优化算法有很大用处。\n","\n","## 链式法则\n","\n","然而，上面方法可能很难找到梯度。\n","这是因为在深度学习中，多元函数通常是 *复合*（composite）的，所以我们可能没法应用上述任何规则来微分这些函数。\n","幸运的是，链式法则使我们能够微分复合函数。\n","\n","让我们先考虑单变量函数。假设函数 $y=f(u)$ 和 $u=g(x)$ 都是可微的，根据链式法则：\n","\n","$$\\frac{dy}{dx} = \\frac{dy}{du} \\frac{du}{dx}.$$\n","\n","现在让我们把注意力转向一个更一般的场景，即函数具有任意数量的变量的情况。假设可微分函数 $y$ 有变量 $u_1, u_2, \\ldots, u_m$，其中每个可微分函数 $u_i$ 都有变量 $x_1, x_2, \\ldots, x_n$。注意，$y$是 $x_1, x_2， \\ldots, x_n$ 的函数。对于任意 $i = 1, 2, \\ldots, n$，链式法则给出：\n","\n","$$\\frac{dy}{dx_i} = \\frac{dy}{du_1} \\frac{du_1}{dx_i} + \\frac{dy}{du_2} \\frac{du_2}{dx_i} + \\cdots + \\frac{dy}{du_m} \\frac{du_m}{dx_i}$$\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qx8CC73dr4z_"},"source":["# 自动求导"]},{"cell_type":"markdown","metadata":{"origin_pos":0,"id":"oRkDPw-wsBeW"},"source":["深度学习框架通过自动计算导数，即 *自动求导* （automatic differentiation），来加快这项工作。实际中，根据我们设计的模型，系统会构建一个 *计算图* （computational graph），来跟踪数据通过若干操作组合起来产生输出。自动求导使系统能够随后反向传播梯度。\n","这里，*反向传播*（backpropagate）只是意味着跟踪整个计算图，填充关于每个参数的偏导数。\n","\n","\n","## 一个简单的例子\n","\n","作为一个演示例子，(**假设我们想对函数 $y = 2\\mathbf{x}^{\\top}\\mathbf{x}$关于列向量 $\\mathbf{x}$求导**)。首先，我们创建变量 `x` 并为其分配一个初始值。\n"]},{"cell_type":"code","metadata":{"origin_pos":3,"tab":["tensorflow"],"id":"Oe6D6UtqsBea","outputId":"3b30aaaa-fe96-40b5-b71f-a320e26f1d9e"},"source":["import tensorflow as tf\n","\n","x = tf.range(4, dtype=tf.float32)\n","x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 1., 2., 3.], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"origin_pos":4,"id":"GDpLOf1YsBeb"},"source":["[**在我们计算$y$关于$\\mathbf{x}$的梯度之前，我们需要一个地方来存储梯度。**]\n","重要的是，我们不会在每次对一个参数求导时都分配新的内存。因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。注意，标量函数关于向量$\\mathbf{x}$的梯度是向量，并且与$\\mathbf{x}$具有相同的形状。\n"]},{"cell_type":"code","metadata":{"origin_pos":7,"tab":["tensorflow"],"id":"ae1EiVkUsBeb"},"source":["x = tf.Variable(x)   # x的形状固定了，但是值能改变"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":8,"id":"QMnhIOiksBec"},"source":["(**现在让我们计算 $y$。**)\n"]},{"cell_type":"code","metadata":{"origin_pos":11,"tab":["tensorflow"],"id":"w8ndKRt8sBec","outputId":"40f8d71c-7c3f-41f5-d3c6-e8851fa44344"},"source":["# 把所有计算记录在磁带上\n","with tf.GradientTape() as t:\n","    y = 2 * tf.tensordot(x, x, axes=1)\n","y"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=float32, numpy=28.0>"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"origin_pos":12,"id":"rtiuX2AwsBec"},"source":["`x` 是一个长度为 4 的向量，计算 `x` 和 `x` 的内积，得到了我们赋值给 `y` 的标量输出。接下来，我们可以[**通过调用反向传播函数来自动计算`y`关于`x` 每个分量的梯度**]，并打印这些梯度。\n"]},{"cell_type":"code","metadata":{"origin_pos":15,"tab":["tensorflow"],"id":"0oI4yenvsBec","outputId":"b7a8f2e0-fbd7-4f21-8646-9e58af69df9d"},"source":["x_grad = t.gradient(y, x)\n","x_grad"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.,  4.,  8., 12.], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"origin_pos":16,"id":"oEDqfXuasBed"},"source":["函数 $y = 2\\mathbf{x}^{\\top}\\mathbf{x}$ 关于$\\mathbf{x}$ 的梯度应为$4\\mathbf{x}$。让我们快速验证我们想要的梯度是否正确计算。\n"]},{"cell_type":"code","metadata":{"origin_pos":19,"tab":["tensorflow"],"id":"EhTmzgvVsBed","outputId":"ad2c2c86-24a4-454e-f6d8-77a419e7481d"},"source":["x_grad == 4 * x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"origin_pos":20,"id":"uT_S1Gd6sBed"},"source":["[**现在让我们计算 `x` 的另一个函数。**]\n"]},{"cell_type":"code","metadata":{"origin_pos":23,"tab":["tensorflow"],"id":"RxkZNaWwsBed","outputId":"e3397508-c5ff-4bc6-de3b-f4d524cf7ab7"},"source":["with tf.GradientTape() as t:\n","    y = tf.reduce_sum(x)\n","t.gradient(y, x)  # 被新计算的梯度覆盖"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 1., 1., 1.], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"origin_pos":24,"id":"xfYzWT7osBee"},"source":["## 非标量变量的反向传播\n","\n","当 `y` 不是标量时，向量`y`关于向量`x`的导数的最自然解释是一个矩阵。对于高阶和高维的 `y` 和 `x`，求导的结果可以是一个高阶张量。\n","\n","然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括[**深度学习中**]），但当我们调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。这里(**，我们的目的不是计算微分矩阵，而是批量中每个样本单独计算的偏导数之和。**)\n"]},{"cell_type":"code","metadata":{"origin_pos":27,"tab":["tensorflow"],"id":"re-gxq6FsBee","outputId":"0073ae3e-73e6-4efe-d8f7-e67b5db8b6bb"},"source":["with tf.GradientTape() as t:\n","    y = x * x\n","t.gradient(y, x)  # 等价于 `y = tf.reduce_sum(x * x)`"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 2., 4., 6.], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"origin_pos":28,"id":"C2vopfEqsBee"},"source":["## 分离计算\n","\n","有时，我们希望[**将某些计算移动到记录的计算图之外**]。\n","例如，假设`y`是作为`x`的函数计算的，而`z`则是作为`y`和`x`的函数计算的。\n","现在，想象一下，我们想计算 `z` 关于 `x` 的梯度，但由于某种原因，我们希望将 `y` 视为一个常数，并且只考虑到 `x` 在`y`被计算后发挥的作用。\n","\n","在这里，我们可以分离 `y` 来返回一个新变量 `u`，该变量与 `y` 具有相同的值，但丢弃计算图中如何计算 `y` 的任何信息。换句话说，梯度不会向后流经 `u` 到 `x`。因此，下面的反向传播函数计算 `z = u * x` 关于 `x` 的偏导数，同时将 `u` 作为常数处理，而不是`z = x * x * x`关于 `x` 的偏导数。\n"]},{"cell_type":"code","metadata":{"origin_pos":31,"tab":["tensorflow"],"id":"SgUzSs6KsBee","outputId":"61acf4ac-da52-42ea-fc63-81d9a6507199"},"source":["# 设置 `persistent=True` 来运行 `t.gradient`多次\n","with tf.GradientTape(persistent=True) as t:\n","    y = x * x\n","    u = tf.stop_gradient(y)\n","    z = u * x\n","\n","x_grad = t.gradient(z, x)\n","x_grad == u"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"origin_pos":32,"id":"0HZmlADbsBee"},"source":["由于记录了 `y` 的计算结果，我们可以随后在 `y` 上调用反向传播，得到 `y = x * x` 关于的`x`的导数，这里是 `2 * x`。\n"]},{"cell_type":"code","metadata":{"origin_pos":35,"tab":["tensorflow"],"id":"dvM78N2usBef","outputId":"bfda4f2f-037a-4651-f7bb-f6cb00e18593"},"source":["t.gradient(y, x) == 2 * x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"origin_pos":36,"id":"baMpBcgYsBef"},"source":["## Python控制流的梯度计算\n","\n","使用自动求导的一个好处是，[**即使构建函数的计算图需要通过 Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度**]。在下面的代码中，`while` 循环的迭代次数和 `if` 语句的结果都取决于输入 `a` 的值。\n"]},{"cell_type":"code","metadata":{"origin_pos":39,"tab":["tensorflow"],"id":"VMAu7a9nsBef"},"source":["def f(a):\n","    b = a * 2\n","    while tf.norm(b) < 1000:\n","        b = b * 2\n","    if tf.reduce_sum(b) > 0:\n","        c = b\n","    else:\n","        c = 100 * b\n","    return c"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":40,"id":"IiNR0q4qsBef"},"source":["让我们计算梯度。\n"]},{"cell_type":"code","metadata":{"origin_pos":43,"tab":["tensorflow"],"id":"PVDIYaxVsBef","outputId":"4176c00d-e6d3-40c8-d4ae-f1ba2766bc75"},"source":["a = tf.Variable(tf.random.normal(shape=()))\n","with tf.GradientTape() as t:\n","    d = f(a)\n","d_grad = t.gradient(d, a)\n","d_grad"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=float32, numpy=4096.0>"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"origin_pos":44,"id":"PYe2IVzosBef"},"source":["我们现在可以分析上面定义的 `f` 函数。请注意，它在其输入 `a` 中是分段线性的。换言之，对于任何 `a`，存在某个常量标量 `k`，使得 `f(a) = k * a`，其中 `k` 的值取决于输入 `a`。因此，`d / a` 允许我们验证梯度是否正确。\n"]},{"cell_type":"code","metadata":{"origin_pos":47,"tab":["tensorflow"],"id":"ztjBvOA_sBeg","outputId":"20b04b74-3ecf-4d51-e76f-b89f148aea0d"},"source":["d_grad == d / a"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=bool, numpy=True>"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"6DcYqt9-BP63"},"source":["# 概率"]},{"cell_type":"markdown","metadata":{"origin_pos":0,"id":"CdXhzuZIBC-N"},"source":["## 基本概率论\n","\n","假设我们掷骰子，想知道看到1的几率有多大，而不是看到另一个数字。如果骰子是公平的，那么所有六个结果$\\{1, \\ldots, 6\\}$都有相同的可能发生，因此我们将在每六次中看到一个$1$。我们可以说$1$发生的概率为$\\frac{1}{6}$。\n","\n","对于我们从工厂收到的真实骰子，我们可能不知道那些比例，我们需要检查它是否有污染。调查骰子的唯一方法是多次投掷并记录结果。对于每个骰子，我们将观察到 $\\{1, \\ldots, 6\\}$ 中的一个值。给定这些结果，我们想调查每个结果的概率。\n","\n","对于每个值，一种自然的方法是将单个计数的值除以投掷的总次数。\n","这给了我们一个给定*事件*的概率的*估计值*。*大数定律*（law of large numbers）告诉我们，随着投掷次数的增加，这个估计值会越来越接近真实的潜在概率。在深入了解这里的细节之前，让我们先试一试。\n","\n","首先，让我们导入必要的软件包。\n"]},{"cell_type":"code","metadata":{"origin_pos":3,"tab":["tensorflow"],"id":"p24VkwNiBC-S"},"source":["%matplotlib inline\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow_probability as tfp\n","from d2l import tensorflow as d2l\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":4,"id":"3TkU7rlmBC-S"},"source":["接下来，我们将希望能够投掷骰子。在统计学中，我们把从概率分布中抽取样本的过程称为*抽样*（sampling）。\n","将概率分配给一些离散选择的分布称为*多项分布*（multinomial distribution）。稍后我们将给出*分布*（distribution）的更正式定义。但笼统来说，可以把它看作是对事件的概率分配。\n","\n","为了抽取一个样本，我们只需传入一个概率向量。\n","输出是另一个相同长度的向量：它在索引$i$处的值是采样结果对应于$i$的次数。\n"]},{"cell_type":"code","metadata":{"origin_pos":7,"tab":["tensorflow"],"id":"tmXnut9qBC-S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617424909281,"user_tz":-480,"elapsed":1156,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"1b149681-d4b8-48a1-88be-28315e394d75"},"source":["fair_probs = tf.ones(6) / 6\n","tfp.distributions.Multinomial(1, fair_probs).sample()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(6,), dtype=float32, numpy=array([0., 0., 0., 0., 1., 0.], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"origin_pos":8,"id":"LhJH6aPvBC-T"},"source":["如果你运行采样器很多次，你会发现每次你都得到随机的值。在估计一个骰子的公平性时，我们经常希望从同一分布中生成多个样本。如果用Python的for循环来完成这个任务，速度会慢得令人难以忍受，因此我们使用的函数支持同时抽取多个样本，返回我们想要的任意形状的独立样本数组。\n"]},{"cell_type":"code","metadata":{"origin_pos":11,"tab":["tensorflow"],"id":"oDFCIE34BC-T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617424911742,"user_tz":-480,"elapsed":808,"user":{"displayName":"黄不盈","photoUrl":"","userId":"09846410536788032197"}},"outputId":"bfcf84dd-186f-45ce-c148-feec71cbffa0"},"source":["tfp.distributions.Multinomial(10, fair_probs).sample()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(6,), dtype=float32, numpy=array([1., 2., 3., 2., 1., 1.], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"origin_pos":12,"id":"eDSOBLsRBC-U"},"source":["现在我们知道如何对骰子进行采样，我们可以模拟1000次投掷。然后，我们可以统计1000次投掷后, 每个数字被投中了多少次。具体来说，我们计算相对频率作为真实概率的估计。\n"]},{"cell_type":"code","metadata":{"origin_pos":15,"tab":["tensorflow"],"id":"A9n5W8H3BC-U","outputId":"78bf51ca-60b9-47b0-9884-f88ccf4f37d7"},"source":["counts = tfp.distributions.Multinomial(1000, fair_probs).sample()\n","counts / 1000"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(6,), dtype=float32, numpy=array([0.165, 0.167, 0.147, 0.191, 0.163, 0.167], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"origin_pos":16,"id":"azcPbdFpBC-U"},"source":["因为我们是从一个公平的骰子中生成的数据，我们知道每个结果都有真实的概率$\\frac{1}{6}$，大约是$0.167$，所以上面输出的估计值看起来不错。\n","\n","我们也可以看到这些概率如何随着时间的推移收敛到真实概率。让我们进行500组实验，每组抽取10个样本。\n"]},{"cell_type":"code","metadata":{"origin_pos":19,"tab":["tensorflow"],"id":"vHshKKNKBC-U","outputId":"10c60143-ac6c-465c-cecc-f19a973be5e5"},"source":["counts = tfp.distributions.Multinomial(10, fair_probs).sample(500)\n","cum_counts = tf.cumsum(counts, axis=0)\n","estimates = cum_counts / tf.reduce_sum(cum_counts, axis=1, keepdims=True)\n","\n","d2l.set_figsize((6, 4.5))\n","for i in range(6):\n","    d2l.plt.plot(estimates[:, i].numpy(), label=(\"P(die=\" + str(i + 1) + \")\"))\n","d2l.plt.axhline(y=0.167, color='black', linestyle='dashed')\n","d2l.plt.gca().set_xlabel('Groups of experiments')\n","d2l.plt.gca().set_ylabel('Estimated probability')\n","d2l.plt.legend();"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/svg+xml":"<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"289.37625pt\" version=\"1.1\" viewBox=\"0 0 392.14375 289.37625\" width=\"392.14375pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-03-07T23:12:02.160736</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.0rc1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 289.37625 \nL 392.14375 289.37625 \nL 392.14375 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 50.14375 251.82 \nL 384.94375 251.82 \nL 384.94375 7.2 \nL 50.14375 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m409ad9cbe4\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"65.361932\" xlink:href=\"#m409ad9cbe4\" y=\"251.82\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(62.180682 266.418437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"126.356649\" xlink:href=\"#m409ad9cbe4\" y=\"251.82\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(116.812899 266.418437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"187.351365\" xlink:href=\"#m409ad9cbe4\" y=\"251.82\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <g transform=\"translate(177.807615 266.418437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"248.346082\" xlink:href=\"#m409ad9cbe4\" y=\"251.82\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <g transform=\"translate(238.802332 266.418437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"309.340799\" xlink:href=\"#m409ad9cbe4\" y=\"251.82\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <g transform=\"translate(299.797049 266.418437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"370.335515\" xlink:href=\"#m409ad9cbe4\" y=\"251.82\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <g transform=\"translate(360.791765 266.418437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- Groups of experiments -->\n     <g transform=\"translate(160.397656 280.096562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 3809 666 \nL 3809 1919 \nL 2778 1919 \nL 2778 2438 \nL 4434 2438 \nL 4434 434 \nQ 4069 175 3628 42 \nQ 3188 -91 2688 -91 \nQ 1594 -91 976 548 \nQ 359 1188 359 2328 \nQ 359 3472 976 4111 \nQ 1594 4750 2688 4750 \nQ 3144 4750 3555 4637 \nQ 3966 4525 4313 4306 \nL 4313 3634 \nQ 3963 3931 3569 4081 \nQ 3175 4231 2741 4231 \nQ 1884 4231 1454 3753 \nQ 1025 3275 1025 2328 \nQ 1025 1384 1454 906 \nQ 1884 428 2741 428 \nQ 3075 428 3337 486 \nQ 3600 544 3809 666 \nz\n\" id=\"DejaVuSans-47\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2375 4863 \nL 2375 4384 \nL 1825 4384 \nQ 1516 4384 1395 4259 \nQ 1275 4134 1275 3809 \nL 1275 3500 \nL 2222 3500 \nL 2222 3053 \nL 1275 3053 \nL 1275 0 \nL 697 0 \nL 697 3053 \nL 147 3053 \nL 147 3500 \nL 697 3500 \nL 697 3744 \nQ 697 4328 969 4595 \nQ 1241 4863 1831 4863 \nL 2375 4863 \nz\n\" id=\"DejaVuSans-66\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 3500 \nL 2247 1797 \nL 3578 0 \nL 2900 0 \nL 1881 1375 \nL 863 0 \nL 184 0 \nL 1544 1831 \nL 300 3500 \nL 978 3500 \nL 1906 2253 \nL 2834 3500 \nL 3513 3500 \nz\n\" id=\"DejaVuSans-78\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-47\"/>\n      <use x=\"77.490234\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"116.353516\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"177.535156\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"240.914062\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"304.390625\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"356.490234\" xlink:href=\"#DejaVuSans-20\"/>\n      <use x=\"388.277344\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"449.458984\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"484.664062\" xlink:href=\"#DejaVuSans-20\"/>\n      <use x=\"516.451172\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"576.224609\" xlink:href=\"#DejaVuSans-78\"/>\n      <use x=\"635.404297\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"698.880859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"760.404297\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"801.517578\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"829.300781\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"926.712891\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"988.236328\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1051.615234\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1090.824219\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m5ebea67526\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m5ebea67526\" y=\"212.903185\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.10 -->\n      <g transform=\"translate(20.878125 216.702404)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m5ebea67526\" y=\"171.206595\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.15 -->\n      <g transform=\"translate(20.878125 175.005814)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m5ebea67526\" y=\"129.510006\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.20 -->\n      <g transform=\"translate(20.878125 133.309224)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m5ebea67526\" y=\"87.813416\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.25 -->\n      <g transform=\"translate(20.878125 91.612634)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m5ebea67526\" y=\"46.116826\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.30 -->\n      <g transform=\"translate(20.878125 49.916045)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_13\">\n     <!-- Estimated probability -->\n     <g transform=\"translate(14.798438 183.033437)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-45\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\nM 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2969 \nz\n\" id=\"DejaVuSans-62\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"63.183594\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"115.283203\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"154.492188\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"182.275391\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"279.6875\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"340.966797\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"380.175781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"441.699219\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"505.175781\" xlink:href=\"#DejaVuSans-20\"/>\n      <use x=\"536.962891\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"600.439453\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"639.302734\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"700.484375\" xlink:href=\"#DejaVuSans-62\"/>\n      <use x=\"763.960938\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"825.240234\" xlink:href=\"#DejaVuSans-62\"/>\n      <use x=\"888.716797\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"916.5\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"944.283203\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"972.066406\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1011.275391\" xlink:href=\"#DejaVuSans-79\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_12\">\n    <path clip-path=\"url(#p26c6b37c46)\" d=\"M 65.361932 46.116816 \nL 65.971879 46.116816 \nL 66.581826 73.914541 \nL 67.191773 108.661716 \nL 67.80172 129.510003 \nL 68.411668 143.408866 \nL 69.021615 153.336624 \nL 69.631562 150.358303 \nL 70.241509 166.573636 \nL 70.851456 154.527958 \nL 71.461403 167.415995 \nL 72.071351 164.257165 \nL 73.291245 171.206591 \nL 73.901192 168.426821 \nL 75.121086 173.659331 \nL 75.731034 180.472499 \nL 76.340981 177.790265 \nL 76.950928 183.715568 \nL 77.560875 177.163246 \nL 78.170822 182.578389 \nL 78.780769 180.271077 \nL 79.390717 181.63074 \nL 80.610611 177.62145 \nL 81.220558 178.928189 \nL 81.830505 168.228269 \nL 82.440452 166.89316 \nL 83.660347 169.861541 \nL 84.270294 168.600556 \nL 84.880241 169.943059 \nL 85.490188 168.75385 \nL 86.100135 170.015269 \nL 86.710083 168.89012 \nL 87.32003 161.064188 \nL 87.929977 164.622929 \nL 88.539924 165.860874 \nL 89.149871 164.952108 \nL 89.759819 160.019711 \nL 90.369766 163.264383 \nL 90.979713 160.540023 \nL 91.58966 159.834804 \nL 92.199607 161.014096 \nL 92.809554 156.703437 \nL 93.419502 154.350532 \nL 94.029449 157.307728 \nL 94.639396 158.442334 \nL 95.249343 161.199414 \nL 96.469237 156.773163 \nL 97.689132 155.763418 \nL 98.299079 153.769839 \nL 98.909026 154.825798 \nL 99.518973 154.38166 \nL 100.12892 155.390647 \nL 100.738868 152.12511 \nL 101.348815 154.527958 \nL 101.958762 152.750725 \nL 102.568709 152.375878 \nL 103.178656 154.660326 \nL 103.788603 154.267361 \nL 104.398551 151.320528 \nL 105.008498 152.253601 \nL 105.618445 150.669464 \nL 106.838339 152.473339 \nL 107.448286 150.95397 \nL 108.058234 150.651942 \nL 108.668181 151.516545 \nL 109.278128 153.499822 \nL 109.888075 150.921773 \nL 110.498022 150.636273 \nL 111.107969 151.45558 \nL 111.717917 151.170577 \nL 112.327864 151.962012 \nL 112.937811 150.622206 \nL 114.157705 154.219096 \nL 114.767652 152.900776 \nL 115.3776 152.618954 \nL 115.987547 153.336624 \nL 116.597494 152.075218 \nL 117.207441 151.812831 \nL 117.817388 152.515018 \nL 118.427335 151.305952 \nL 119.64723 150.821602 \nL 120.257177 151.503807 \nL 120.867124 151.264745 \nL 122.087018 154.350532 \nL 123.91686 153.58226 \nL 124.526807 154.187582 \nL 125.136754 152.253601 \nL 125.746701 151.192237 \nL 126.356649 152.628907 \nL 126.966596 151.584667 \nL 127.576543 152.989638 \nL 128.796437 154.130855 \nL 129.406384 153.898578 \nL 131.846173 156.044196 \nL 132.45612 157.307728 \nL 133.066067 156.314958 \nL 133.676015 156.077748 \nL 134.285962 155.113174 \nL 136.115803 156.594966 \nL 136.72575 157.778881 \nL 138.555592 159.145603 \nL 139.165539 158.219129 \nL 142.215275 160.372054 \nL 142.825222 158.827918 \nL 144.045116 159.659851 \nL 144.655064 160.702873 \nL 145.265011 159.834804 \nL 145.874958 160.233808 \nL 147.704799 157.716524 \nL 148.314747 158.119343 \nL 148.924694 159.120625 \nL 149.534641 159.507551 \nL 151.364482 158.873797 \nL 152.584377 157.307728 \nL 153.194324 158.266275 \nL 154.414218 157.875037 \nL 155.024165 156.556444 \nL 156.24406 156.195825 \nL 157.463954 156.941977 \nL 158.073901 156.762675 \nL 158.683848 157.127232 \nL 159.293796 156.949048 \nL 160.51369 157.661848 \nL 161.123637 157.483663 \nL 161.733584 158.356703 \nL 162.953479 157.998359 \nL 164.173373 158.672039 \nL 164.78332 159.511217 \nL 165.393267 159.329379 \nL 166.003214 159.652121 \nL 167.223109 159.29328 \nL 167.833056 160.103951 \nL 168.443003 160.41454 \nL 169.05295 161.209168 \nL 169.662897 160.540023 \nL 170.272845 160.360659 \nL 170.882792 159.224822 \nL 172.102686 158.887143 \nL 172.712633 158.250033 \nL 173.932528 157.928907 \nL 175.762369 158.835076 \nL 176.372316 158.219129 \nL 177.592211 158.810309 \nL 178.202158 158.204428 \nL 178.812105 158.496937 \nL 180.031999 158.1902 \nL 180.641946 158.917078 \nL 181.251894 158.763113 \nL 182.471788 159.324147 \nL 183.081735 158.310737 \nL 184.911577 157.872154 \nL 186.131471 156.748981 \nL 187.351365 157.307728 \nL 187.961312 157.995799 \nL 188.57126 158.266275 \nL 189.181207 157.716524 \nL 189.791154 157.985721 \nL 191.011048 157.710597 \nL 191.620995 157.975941 \nL 192.84089 156.910625 \nL 193.450837 157.175994 \nL 194.060784 156.652128 \nL 194.670731 156.916217 \nL 195.280678 156.788149 \nL 196.500573 155.763418 \nL 197.720467 155.522554 \nL 198.330414 155.784568 \nL 199.550309 155.546786 \nL 200.160256 155.053862 \nL 200.770203 154.939314 \nL 201.38015 155.570372 \nL 201.990097 155.08391 \nL 202.600044 153.863771 \nL 203.209992 153.75648 \nL 203.819939 154.015897 \nL 205.039833 153.802807 \nL 205.64978 154.419661 \nL 206.259727 154.312283 \nL 208.089569 155.060262 \nL 208.699516 155.658712 \nL 209.309463 154.844636 \nL 209.91941 155.088582 \nL 210.529358 155.679415 \nL 211.749252 156.154295 \nL 212.359199 156.044196 \nL 212.969146 156.278188 \nL 213.579093 156.852034 \nL 214.189041 156.400055 \nL 214.798988 156.629735 \nL 216.018882 156.411028 \nL 216.628829 156.97282 \nL 217.238776 157.196535 \nL 217.848724 157.750722 \nL 218.458671 157.969579 \nL 219.068618 156.868238 \nL 219.678565 157.088847 \nL 220.288512 156.980698 \nL 220.898459 156.221883 \nL 221.508407 156.766924 \nL 222.118354 156.984501 \nL 222.728301 156.234459 \nL 223.338248 154.8487 \nL 223.948195 155.390647 \nL 224.558142 154.65528 \nL 225.16809 154.242582 \nL 226.387984 154.685303 \nL 227.607878 155.746058 \nL 228.217825 155.648162 \nL 230.047667 156.281991 \nL 230.657614 156.796748 \nL 231.267561 156.696789 \nL 232.487456 157.105573 \nL 233.097403 156.401286 \nL 234.317297 155.607875 \nL 237.367033 155.146775 \nL 237.97698 155.643788 \nL 239.196874 156.044196 \nL 240.416769 155.280821 \nL 241.636663 155.678209 \nL 242.24661 155.015141 \nL 242.856557 154.927795 \nL 244.076452 155.889486 \nL 244.686399 155.800064 \nL 245.906293 156.184591 \nL 246.51624 156.095083 \nL 247.126188 156.285073 \nL 247.736135 155.917843 \nL 248.346082 156.384224 \nL 248.956029 156.295237 \nL 249.565976 156.757281 \nL 250.785871 156.03177 \nL 251.395818 155.945094 \nL 252.615712 156.314958 \nL 253.225659 155.958329 \nL 253.835606 155.873008 \nL 255.055501 156.238585 \nL 256.275395 157.130675 \nL 256.885342 157.042993 \nL 257.495289 157.21976 \nL 258.105237 157.658492 \nL 258.715184 157.045491 \nL 260.545025 156.788149 \nL 261.154972 157.221401 \nL 261.76492 157.393794 \nL 262.374867 157.82251 \nL 262.984814 157.735388 \nL 263.594761 157.137198 \nL 265.424603 157.645693 \nL 266.03455 157.560434 \nL 266.644497 157.727634 \nL 267.254444 157.14028 \nL 269.084286 157.639641 \nL 269.694233 157.555924 \nL 270.30418 157.225241 \nL 270.914127 156.649792 \nL 271.524074 156.815736 \nL 272.134021 156.735423 \nL 272.743969 156.900137 \nL 274.57381 156.661274 \nL 275.183757 156.099134 \nL 275.793704 155.781263 \nL 276.403652 155.94589 \nL 277.013599 155.63028 \nL 277.623546 155.077547 \nL 278.233493 155.004491 \nL 280.673282 155.658712 \nL 281.283229 155.115237 \nL 281.893176 155.511805 \nL 282.503123 154.971785 \nL 283.11307 154.667719 \nL 283.723018 154.597646 \nL 284.332965 154.759601 \nL 284.942912 154.689665 \nL 285.552859 154.850477 \nL 286.162806 154.780664 \nL 286.772753 154.940346 \nL 287.382701 154.642195 \nL 287.992648 155.029233 \nL 288.602595 155.186926 \nL 289.212542 155.117151 \nL 289.822489 154.821759 \nL 290.432436 154.753351 \nL 291.042384 154.910087 \nL 291.652331 154.393453 \nL 292.262278 154.550313 \nL 292.872225 154.929323 \nL 293.482172 155.08391 \nL 294.092119 154.79411 \nL 294.702067 154.727044 \nL 295.312014 154.439717 \nL 295.921961 154.593968 \nL 296.531908 154.089052 \nL 297.141855 154.243415 \nL 297.751802 154.178673 \nL 298.971697 154.484527 \nL 299.581644 154.852863 \nL 300.801538 155.152864 \nL 301.411485 154.87185 \nL 302.021433 154.163512 \nL 302.63138 154.314134 \nL 303.241327 154.037407 \nL 303.851274 153.549367 \nL 304.461221 153.912595 \nL 305.071168 153.850649 \nL 305.681116 154.000153 \nL 306.291063 153.727725 \nL 306.90101 153.036548 \nL 307.510957 152.977435 \nL 308.120904 153.336624 \nL 308.730851 153.485544 \nL 309.340799 153.42576 \nL 309.950746 153.573711 \nL 310.560693 153.307062 \nL 311.780587 153.189556 \nL 313.000482 153.892775 \nL 313.610429 153.628624 \nL 314.220376 153.569647 \nL 314.830323 153.307571 \nL 315.44027 153.249676 \nL 316.050217 152.584818 \nL 318.490006 152.362942 \nL 319.099953 152.10816 \nL 319.7099 152.054092 \nL 320.319848 152.199322 \nL 320.929795 152.145303 \nL 322.149689 151.642798 \nL 322.759636 151.59047 \nL 323.369583 151.145028 \nL 323.979531 151.094129 \nL 324.589478 150.847698 \nL 325.199425 150.993026 \nL 325.809372 150.942836 \nL 326.419319 151.087257 \nL 327.029266 150.84315 \nL 327.639214 150.987136 \nL 328.249161 150.744384 \nL 329.469055 151.030828 \nL 330.079002 150.981358 \nL 330.688949 151.123381 \nL 331.298897 151.45558 \nL 331.908844 151.215077 \nL 333.128738 151.116422 \nL 337.398368 152.084003 \nL 338.008315 152.033614 \nL 338.618263 151.611981 \nL 339.22821 151.933505 \nL 339.838157 151.883787 \nL 340.448104 152.018789 \nL 341.058051 152.337281 \nL 341.667998 152.287003 \nL 342.277946 152.420216 \nL 342.887893 152.369975 \nL 343.49784 151.955003 \nL 344.107787 151.905993 \nL 344.717734 151.675518 \nL 345.327681 151.989909 \nL 345.937629 151.579348 \nL 347.157523 151.484024 \nL 347.76747 151.616392 \nL 348.377417 151.389508 \nL 348.987365 151.70047 \nL 350.207259 151.605631 \nL 350.817206 151.914145 \nL 351.427153 151.689038 \nL 352.0371 151.819007 \nL 353.256995 151.724677 \nL 353.866942 152.029687 \nL 354.476889 152.157842 \nL 355.086836 152.460652 \nL 355.696783 152.237707 \nL 356.306731 152.190163 \nL 356.916678 152.49101 \nL 358.136572 152.048699 \nL 358.746519 152.347968 \nL 359.356466 152.473339 \nL 359.966414 152.253601 \nL 360.576361 152.378649 \nL 361.186308 152.160004 \nL 361.796255 152.284729 \nL 363.016149 152.873735 \nL 364.236044 153.118253 \nL 365.455938 153.022481 \nL 366.065885 152.637258 \nL 366.675832 152.422068 \nL 367.895727 152.329875 \nL 368.505674 152.116586 \nL 369.115621 152.238403 \nL 369.725568 152.02617 \nL 369.725568 152.02617 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_13\">\n    <path clip-path=\"url(#p26c6b37c46)\" d=\"M 65.361932 212.903184 \nL 66.581826 212.903184 \nL 67.191773 233.751478 \nL 67.80172 229.581823 \nL 68.411668 240.700909 \nL 69.021615 224.816495 \nL 69.631562 223.327334 \nL 70.241509 231.435001 \nL 72.071351 178.156028 \nL 72.681298 167.999161 \nL 73.291245 153.336624 \nL 73.901192 157.307728 \nL 74.511139 165.994522 \nL 75.121086 158.942888 \nL 75.731034 148.04182 \nL 76.950928 146.188636 \nL 77.560875 153.336624 \nL 78.170822 152.253601 \nL 78.780769 147.638963 \nL 80.000664 152.860091 \nL 80.610611 155.169442 \nL 81.220558 160.396372 \nL 81.830505 162.271614 \nL 82.440452 166.89316 \nL 83.0504 168.426821 \nL 84.270294 165.994522 \nL 84.880241 167.415995 \nL 86.100135 165.249935 \nL 87.32003 172.33353 \nL 87.929977 171.206591 \nL 88.539924 172.275734 \nL 89.149871 175.376258 \nL 89.759819 176.291548 \nL 90.979713 181.87317 \nL 91.58966 174.997198 \nL 92.199607 172.133189 \nL 92.809554 171.206591 \nL 93.419502 173.868085 \nL 94.029449 174.681315 \nL 95.249343 169.538736 \nL 95.85929 170.38901 \nL 96.469237 172.810312 \nL 97.079185 171.993328 \nL 98.909026 174.184924 \nL 100.12892 178.395662 \nL 100.738868 178.980542 \nL 101.348815 171.206591 \nL 101.958762 170.523043 \nL 102.568709 168.516491 \nL 103.178656 167.897338 \nL 103.788603 169.90358 \nL 105.008498 168.679527 \nL 105.618445 169.339588 \nL 106.228392 167.527486 \nL 106.838339 166.976505 \nL 107.448286 165.249935 \nL 108.058234 167.095663 \nL 108.668181 167.731878 \nL 109.278128 164.923552 \nL 109.888075 163.318054 \nL 110.498022 163.979184 \nL 111.107969 163.525639 \nL 111.717917 164.166911 \nL 112.327864 162.653444 \nL 113.547758 163.909694 \nL 114.157705 161.425912 \nL 114.767652 162.053689 \nL 115.3776 163.671067 \nL 115.987547 163.264383 \nL 116.597494 161.886179 \nL 117.817388 163.058985 \nL 118.427335 164.573048 \nL 119.037283 165.116076 \nL 119.64723 166.573636 \nL 121.477071 165.378041 \nL 122.696966 166.378564 \nL 123.91686 163.898945 \nL 124.526807 165.249935 \nL 125.746701 166.203002 \nL 126.356649 165.01403 \nL 126.966596 165.48353 \nL 128.18649 163.188022 \nL 128.796437 164.455717 \nL 129.406384 164.126041 \nL 130.016332 165.361277 \nL 130.626279 165.029327 \nL 131.236226 162.408231 \nL 131.846173 163.6254 \nL 132.45612 164.069338 \nL 133.066067 165.249935 \nL 133.676015 164.933655 \nL 134.285962 165.354442 \nL 135.505856 164.736433 \nL 136.115803 165.860874 \nL 136.72575 164.846097 \nL 137.335698 163.147586 \nL 138.555592 162.591597 \nL 139.165539 163.003985 \nL 139.775486 162.731682 \nL 140.385433 163.13629 \nL 140.995381 164.201569 \nL 142.215275 163.655248 \nL 144.655064 165.158998 \nL 145.265011 166.152463 \nL 145.874958 166.503973 \nL 146.484905 167.472573 \nL 147.094852 167.809097 \nL 147.704799 167.527486 \nL 148.314747 167.858691 \nL 148.924694 168.789402 \nL 150.144588 168.228269 \nL 150.754535 169.136551 \nL 151.364482 168.857488 \nL 151.97443 167.415995 \nL 152.584377 166.573636 \nL 153.194324 166.318035 \nL 153.804271 167.208298 \nL 154.414218 167.519148 \nL 156.24406 166.758954 \nL 156.854007 165.960001 \nL 158.073901 165.48353 \nL 158.683848 165.791459 \nL 159.293796 163.94332 \nL 160.51369 164.567009 \nL 161.123637 165.400744 \nL 161.733584 165.175015 \nL 162.343531 164.430901 \nL 162.953479 165.249935 \nL 163.563426 163.999774 \nL 164.173373 163.276574 \nL 164.78332 163.579173 \nL 165.393267 163.372693 \nL 166.003214 162.666331 \nL 166.613162 162.467792 \nL 167.223109 162.768005 \nL 167.833056 162.571205 \nL 169.05295 163.159889 \nL 169.662897 163.933938 \nL 170.272845 163.734952 \nL 170.882792 164.017532 \nL 172.712633 163.432651 \nL 173.32258 162.773572 \nL 173.932528 162.587745 \nL 175.762369 160.667893 \nL 176.372316 160.497636 \nL 176.982263 160.782453 \nL 178.812105 158.942888 \nL 180.031999 159.513901 \nL 180.641946 158.47816 \nL 181.251894 158.326494 \nL 181.861841 158.610751 \nL 182.471788 158.459968 \nL 183.081735 159.170468 \nL 183.691682 158.590707 \nL 184.301629 157.591377 \nL 184.911577 157.872154 \nL 185.521524 158.57126 \nL 187.351365 158.137511 \nL 187.961312 157.582951 \nL 188.57126 156.623062 \nL 189.181207 156.490148 \nL 189.791154 155.951743 \nL 193.450837 157.571221 \nL 194.060784 158.225578 \nL 194.670731 158.090762 \nL 195.890626 158.600649 \nL 197.11052 157.563926 \nL 197.720467 157.817788 \nL 198.330414 158.450101 \nL 198.940361 158.318554 \nL 199.550309 157.810854 \nL 200.770203 157.557042 \nL 201.38015 157.804119 \nL 201.990097 157.678362 \nL 203.209992 158.899669 \nL 204.429886 158.642998 \nL 205.039833 158.153753 \nL 206.259727 157.906825 \nL 207.479622 158.376871 \nL 209.309463 158.011469 \nL 209.91941 157.541322 \nL 210.529358 157.424041 \nL 211.139305 156.960257 \nL 211.749252 156.846355 \nL 212.359199 157.077998 \nL 212.969146 156.278188 \nL 214.189041 156.059667 \nL 214.798988 156.629735 \nL 215.408935 156.519947 \nL 216.018882 156.747291 \nL 216.628829 156.637912 \nL 217.238776 156.862969 \nL 217.848724 156.089503 \nL 220.288512 156.980698 \nL 220.898459 156.873395 \nL 221.508407 157.091407 \nL 222.118354 157.630968 \nL 222.728301 157.844369 \nL 223.338248 158.376871 \nL 223.948195 158.266275 \nL 224.558142 158.793111 \nL 226.387984 159.405678 \nL 226.997931 159.29328 \nL 229.43772 161.322958 \nL 230.047667 160.897846 \nL 230.657614 161.089041 \nL 231.877508 162.075958 \nL 233.097403 162.444268 \nL 233.70735 162.927487 \nL 234.317297 162.807285 \nL 234.927244 163.285732 \nL 235.537191 163.462947 \nL 236.147139 163.342111 \nL 236.757086 163.51786 \nL 237.367033 163.103012 \nL 240.416769 163.967602 \nL 241.636663 163.729969 \nL 242.856557 164.637952 \nL 243.466505 164.802679 \nL 244.686399 164.563405 \nL 246.51624 165.050054 \nL 247.126188 164.65228 \nL 247.736135 164.813117 \nL 248.956029 164.579324 \nL 249.565976 164.738806 \nL 250.785871 163.960954 \nL 251.395818 163.848369 \nL 252.615712 162.000852 \nL 253.835606 162.32926 \nL 254.445554 162.760014 \nL 255.055501 162.92074 \nL 256.885342 161.808315 \nL 258.105237 162.130684 \nL 258.715184 161.765853 \nL 259.935078 162.085464 \nL 260.545025 161.724199 \nL 261.154972 161.883135 \nL 261.76492 161.524728 \nL 262.984814 162.354089 \nL 264.204708 162.663261 \nL 264.814655 162.562184 \nL 265.424603 162.208238 \nL 266.03455 162.614574 \nL 266.644497 162.262617 \nL 267.864391 162.065905 \nL 268.474338 162.218105 \nL 269.694233 162.023418 \nL 270.914127 162.324476 \nL 271.524074 161.98169 \nL 272.134021 162.37673 \nL 273.353916 161.696848 \nL 273.963863 161.603003 \nL 274.57381 161.267287 \nL 275.183757 161.658686 \nL 276.403652 161.954041 \nL 277.623546 161.768115 \nL 278.84344 162.534659 \nL 280.063335 162.820035 \nL 280.673282 162.254788 \nL 281.283229 161.927634 \nL 281.893176 162.305078 \nL 283.11307 162.121861 \nL 283.723018 161.566432 \nL 284.942912 161.850851 \nL 285.552859 161.761516 \nL 286.772753 162.50071 \nL 287.382701 162.638806 \nL 287.992648 161.864731 \nL 288.602595 161.776577 \nL 290.432436 162.191115 \nL 291.652331 162.912115 \nL 292.262278 162.598991 \nL 292.872225 162.510514 \nL 293.482172 162.200136 \nL 294.092119 162.113188 \nL 294.702067 162.247916 \nL 295.312014 162.602533 \nL 296.531908 162.428362 \nL 297.141855 162.779723 \nL 297.751802 162.474328 \nL 298.36175 162.605987 \nL 299.581644 162.000852 \nL 300.191591 162.132722 \nL 300.801538 161.832944 \nL 301.411485 161.749636 \nL 302.63138 162.011961 \nL 303.241327 161.92884 \nL 303.851274 162.058871 \nL 304.461221 161.763852 \nL 305.071168 161.893648 \nL 305.681116 161.81167 \nL 307.510957 162.196781 \nL 308.730851 161.616374 \nL 309.340799 161.744281 \nL 309.950746 161.664092 \nL 311.17064 161.917755 \nL 312.390534 162.579718 \nL 313.000482 162.49846 \nL 313.610429 162.826397 \nL 316.050217 163.312586 \nL 316.660165 163.028813 \nL 317.270112 162.947854 \nL 317.880059 163.269168 \nL 318.490006 162.787092 \nL 319.099953 163.107262 \nL 320.319848 162.946885 \nL 320.929795 162.668717 \nL 321.539742 162.986128 \nL 322.149689 162.906797 \nL 322.759636 163.222146 \nL 323.369583 163.339316 \nL 323.979531 163.259711 \nL 324.589478 162.984736 \nL 325.199425 163.101633 \nL 325.809372 163.412843 \nL 327.029266 163.25515 \nL 327.639214 163.563838 \nL 328.249161 163.29197 \nL 328.859108 163.59913 \nL 329.469055 163.712733 \nL 330.079002 164.017532 \nL 331.298897 163.859603 \nL 331.908844 163.971566 \nL 332.518791 163.893068 \nL 333.738685 164.115341 \nL 334.348632 164.414386 \nL 334.95858 164.523852 \nL 336.178474 165.116076 \nL 336.788421 165.223231 \nL 338.008315 165.063798 \nL 338.618263 165.35607 \nL 339.838157 165.19711 \nL 340.448104 165.487146 \nL 342.887893 165.171561 \nL 343.49784 165.276006 \nL 344.107787 164.469597 \nL 344.717734 164.575111 \nL 345.327681 164.498887 \nL 345.937629 164.603879 \nL 346.547576 164.527915 \nL 347.157523 164.272177 \nL 348.987365 164.048387 \nL 349.597312 163.795855 \nL 350.207259 163.900784 \nL 350.817206 163.471832 \nL 351.427153 163.399572 \nL 352.0371 163.504676 \nL 352.647048 163.786013 \nL 353.256995 163.889861 \nL 354.476889 163.745105 \nL 355.086836 164.023571 \nL 355.696783 163.951211 \nL 356.306731 164.053619 \nL 356.916678 163.807411 \nL 357.526625 163.909694 \nL 358.136572 164.18493 \nL 358.746519 163.593936 \nL 360.576361 163.383107 \nL 361.186308 163.14182 \nL 361.796255 163.072766 \nL 362.406202 163.174875 \nL 363.626097 163.718226 \nL 364.236044 163.818396 \nL 364.845991 163.070678 \nL 365.455938 162.833443 \nL 366.065885 163.103609 \nL 366.675832 163.204214 \nL 367.28578 163.13629 \nL 367.895727 163.236424 \nL 369.115621 163.769734 \nL 369.725568 163.534425 \nL 369.725568 163.534425 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#p26c6b37c46)\" d=\"M 65.361932 212.903184 \nL 66.581826 212.903184 \nL 67.191773 192.05489 \nL 67.80172 196.224552 \nL 68.411668 199.004322 \nL 69.021615 212.903184 \nL 69.631562 192.05489 \nL 70.241509 203.637276 \nL 70.851456 204.563868 \nL 71.461403 197.74079 \nL 72.681298 200.073465 \nL 73.291245 200.989873 \nL 73.901192 190.665005 \nL 74.511139 176.418672 \nL 75.121086 173.659331 \nL 75.731034 175.839545 \nL 76.340981 169.012037 \nL 76.950928 167.036936 \nL 77.560875 173.192142 \nL 78.170822 171.206591 \nL 78.780769 173.019488 \nL 79.390717 171.206591 \nL 80.000664 172.874457 \nL 80.610611 171.206591 \nL 81.220558 166.573636 \nL 81.830505 168.228269 \nL 82.440452 172.644405 \nL 83.0504 173.986373 \nL 83.660347 172.551653 \nL 84.270294 176.418672 \nL 84.880241 177.524262 \nL 85.490188 171.206591 \nL 86.100135 170.015269 \nL 86.710083 166.573636 \nL 87.32003 165.57192 \nL 87.929977 166.817483 \nL 88.539924 170.137447 \nL 89.759819 168.155623 \nL 90.369766 163.264383 \nL 90.979713 164.418773 \nL 91.58966 167.415995 \nL 92.199607 162.86728 \nL 92.809554 165.767911 \nL 93.419502 157.89917 \nL 94.029449 153.833016 \nL 95.85929 157.307728 \nL 97.689132 160.396372 \nL 98.299079 158.318554 \nL 98.909026 159.29328 \nL 100.12892 155.390647 \nL 100.738868 154.951989 \nL 102.568709 157.756078 \nL 103.178656 155.984027 \nL 103.788603 155.570372 \nL 104.398551 157.735388 \nL 105.008498 158.57126 \nL 105.618445 156.892843 \nL 106.228392 158.942888 \nL 106.838339 158.516335 \nL 107.448286 159.29328 \nL 108.668181 158.46597 \nL 109.278128 156.926941 \nL 109.888075 158.810309 \nL 110.498022 156.195825 \nL 111.717917 157.668744 \nL 112.327864 156.238585 \nL 112.937811 155.900247 \nL 114.157705 157.307728 \nL 114.767652 159.00271 \nL 116.597494 155.018509 \nL 117.207441 155.69158 \nL 117.817388 153.473565 \nL 118.427335 152.253601 \nL 119.037283 152.93506 \nL 120.867124 152.1712 \nL 121.477071 153.720928 \nL 122.087018 153.463363 \nL 123.91686 155.30171 \nL 124.526807 155.038528 \nL 126.966596 157.307728 \nL 129.406384 156.258766 \nL 130.016332 157.56753 \nL 130.626279 155.763418 \nL 131.236226 156.287632 \nL 131.846173 155.286077 \nL 132.45612 153.551281 \nL 133.676015 154.601759 \nL 134.285962 154.38166 \nL 134.895909 153.440225 \nL 136.115803 154.456679 \nL 136.72575 154.245266 \nL 137.335698 155.438973 \nL 139.165539 156.852034 \nL 139.775486 156.629735 \nL 140.385433 157.083553 \nL 140.995381 156.195825 \nL 141.605328 155.984027 \nL 142.215275 155.11894 \nL 142.825222 154.91886 \nL 143.435169 155.368353 \nL 144.655064 154.973574 \nL 145.265011 156.044196 \nL 145.874958 155.217681 \nL 147.094852 157.307728 \nL 147.704799 157.716524 \nL 148.314747 158.728058 \nL 148.924694 157.912031 \nL 149.534641 157.707701 \nL 150.144588 158.101959 \nL 150.754535 157.89917 \nL 152.584377 159.045084 \nL 153.194324 158.841401 \nL 153.804271 158.069314 \nL 154.414218 158.442334 \nL 155.024165 157.683383 \nL 156.854007 160.437281 \nL 157.463954 159.685164 \nL 158.073901 159.487942 \nL 158.683848 160.376316 \nL 160.51369 161.380009 \nL 161.733584 160.979128 \nL 162.343531 161.30366 \nL 162.953479 160.588214 \nL 164.173373 161.230107 \nL 164.78332 160.528206 \nL 165.393267 160.340217 \nL 166.003214 160.656858 \nL 167.223109 162.271614 \nL 168.443003 161.886179 \nL 169.05295 162.672208 \nL 169.662897 162.479398 \nL 170.882792 163.058985 \nL 171.492739 162.86728 \nL 172.712633 161.548065 \nL 175.152422 162.682982 \nL 175.762369 162.50071 \nL 177.592211 163.318054 \nL 178.202158 164.03299 \nL 178.812105 163.848369 \nL 179.422052 164.109302 \nL 180.641946 162.86728 \nL 181.251894 163.565864 \nL 181.861841 162.954142 \nL 184.911577 162.105309 \nL 185.521524 162.783041 \nL 186.131471 162.615842 \nL 186.741418 162.033347 \nL 187.351365 162.286426 \nL 187.961312 162.124173 \nL 188.57126 162.374307 \nL 189.181207 161.804425 \nL 189.791154 162.053689 \nL 190.401101 161.490891 \nL 191.011048 162.142117 \nL 191.620995 160.782453 \nL 192.230943 160.233808 \nL 192.84089 160.08751 \nL 194.670731 158.482286 \nL 195.280678 158.736583 \nL 195.890626 158.212779 \nL 197.11052 159.485432 \nL 197.720467 158.965393 \nL 198.940361 158.697613 \nL 199.550309 158.942888 \nL 200.160256 158.434667 \nL 201.38015 158.176406 \nL 202.600044 158.660719 \nL 203.819939 157.673491 \nL 204.429886 157.914666 \nL 205.039833 157.428589 \nL 205.64978 157.668744 \nL 206.259727 156.828461 \nL 206.869675 157.069126 \nL 207.479622 156.951347 \nL 208.699516 157.425519 \nL 209.91941 157.190931 \nL 211.139305 155.570372 \nL 213.579093 155.143159 \nL 214.189041 155.719292 \nL 214.798988 155.612746 \nL 215.408935 155.169442 \nL 216.018882 154.393453 \nL 216.628829 154.293519 \nL 217.238776 154.527958 \nL 219.068618 154.231299 \nL 220.288512 154.691479 \nL 220.898459 155.244622 \nL 221.508407 155.144489 \nL 222.118354 155.368353 \nL 223.338248 156.452421 \nL 223.948195 156.668705 \nL 225.16809 157.730517 \nL 225.778037 157.939494 \nL 228.217825 160.004526 \nL 228.827773 160.201176 \nL 229.43772 159.469774 \nL 230.047667 159.051496 \nL 230.657614 158.942888 \nL 231.267561 158.529606 \nL 231.877508 159.032409 \nL 232.487456 158.318554 \nL 233.70735 158.712674 \nL 234.317297 159.207574 \nL 234.927244 159.101128 \nL 235.537191 159.29328 \nL 236.147139 159.780836 \nL 236.757086 159.673495 \nL 237.367033 159.272242 \nL 237.97698 159.167436 \nL 238.586927 158.770768 \nL 239.196874 158.960049 \nL 239.806822 159.438571 \nL 240.416769 159.045084 \nL 242.24661 159.600328 \nL 242.856557 159.497274 \nL 243.466505 159.964165 \nL 244.076452 160.144238 \nL 244.686399 160.040389 \nL 245.296346 160.500718 \nL 245.906293 160.396372 \nL 246.51624 160.852414 \nL 247.126188 160.468683 \nL 247.736135 160.643462 \nL 248.346082 161.094136 \nL 249.565976 160.885656 \nL 250.175923 161.056769 \nL 250.785871 160.953331 \nL 252.615712 161.45934 \nL 253.225659 161.086059 \nL 253.835606 161.25322 \nL 255.055501 161.049736 \nL 256.275395 161.380009 \nL 256.885342 161.808315 \nL 257.495289 161.442203 \nL 258.105237 161.867614 \nL 259.935078 161.564257 \nL 260.545025 161.983989 \nL 261.154972 161.624153 \nL 262.984814 161.327705 \nL 263.594761 161.48592 \nL 264.204708 161.133105 \nL 264.814655 161.0367 \nL 265.424603 161.194343 \nL 266.644497 160.499016 \nL 267.864391 160.312891 \nL 268.474338 159.970987 \nL 272.743969 161.057577 \nL 273.353916 160.965322 \nL 273.963863 160.630489 \nL 274.57381 160.540023 \nL 275.183757 160.691801 \nL 275.793704 160.601684 \nL 276.403652 160.271759 \nL 277.013599 160.183356 \nL 277.623546 160.573364 \nL 278.233493 160.246347 \nL 279.453387 160.071716 \nL 280.063335 160.45761 \nL 280.673282 160.37019 \nL 281.283229 160.753089 \nL 281.893176 160.66532 \nL 283.723018 161.798734 \nL 284.332965 161.709039 \nL 284.942912 161.850851 \nL 285.552859 161.761516 \nL 286.162806 161.213195 \nL 286.772753 161.355205 \nL 287.382701 160.582535 \nL 287.992648 160.953331 \nL 288.602595 161.094894 \nL 289.212542 161.009064 \nL 289.822489 161.149695 \nL 290.432436 160.838795 \nL 291.042384 160.304789 \nL 291.652331 160.222015 \nL 292.262278 160.363243 \nL 292.872225 160.057774 \nL 293.482172 159.976318 \nL 294.092119 160.338862 \nL 294.702067 159.814686 \nL 295.312014 159.513901 \nL 295.921961 159.874805 \nL 296.531908 160.014355 \nL 297.141855 159.496529 \nL 298.36175 160.210894 \nL 298.971697 159.913775 \nL 299.581644 160.268006 \nL 300.191591 160.188327 \nL 302.021433 160.594862 \nL 302.63138 160.515158 \nL 303.241327 160.649141 \nL 303.851274 160.995183 \nL 304.461221 161.127265 \nL 305.071168 160.835365 \nL 305.681116 160.756059 \nL 306.90101 161.018769 \nL 309.340799 160.704464 \nL 310.560693 161.377374 \nL 313.000482 161.064188 \nL 313.610429 160.578049 \nL 314.830323 160.833302 \nL 316.050217 161.490891 \nL 317.880059 161.259695 \nL 319.7099 161.630341 \nL 320.319848 161.951742 \nL 321.539742 162.193799 \nL 322.149689 162.51157 \nL 322.759636 162.236409 \nL 323.369583 162.355903 \nL 324.589478 162.201702 \nL 325.199425 161.734538 \nL 325.809372 161.854094 \nL 326.419319 161.584301 \nL 327.029266 161.897587 \nL 327.639214 161.435468 \nL 328.249161 161.747648 \nL 328.859108 161.095416 \nL 329.469055 161.214785 \nL 330.079002 160.950199 \nL 331.298897 160.8063 \nL 331.908844 160.925246 \nL 332.518791 160.853682 \nL 333.738685 160.333333 \nL 334.348632 160.452267 \nL 334.95858 160.38243 \nL 335.568527 160.125064 \nL 336.178474 160.431067 \nL 336.788421 160.548722 \nL 337.398368 160.479282 \nL 338.008315 160.224016 \nL 339.838157 161.129154 \nL 340.448104 161.243689 \nL 341.058051 161.173641 \nL 341.667998 161.471269 \nL 342.277946 161.401022 \nL 342.887893 161.513967 \nL 343.49784 161.808899 \nL 344.107787 161.920452 \nL 345.327681 161.779535 \nL 345.937629 161.890442 \nL 346.547576 161.820356 \nL 347.157523 161.930679 \nL 347.76747 161.860804 \nL 348.377417 161.97058 \nL 348.987365 161.364053 \nL 350.207259 161.940682 \nL 351.427153 161.44782 \nL 352.0371 161.380009 \nL 352.647048 161.135814 \nL 353.256995 161.068947 \nL 353.866942 160.826431 \nL 354.476889 160.409371 \nL 355.086836 160.344454 \nL 356.916678 160.673634 \nL 357.526625 160.608717 \nL 358.136572 160.197311 \nL 359.356466 160.415547 \nL 359.966414 160.6963 \nL 360.576361 160.803938 \nL 361.186308 160.567958 \nL 361.796255 160.846661 \nL 362.406202 160.611563 \nL 363.016149 160.547964 \nL 364.236044 160.761216 \nL 364.845991 161.0367 \nL 365.455938 160.972741 \nL 366.065885 161.077869 \nL 366.675832 160.84563 \nL 367.28578 160.782453 \nL 367.895727 160.887321 \nL 369.115621 160.427315 \nL 369.725568 160.36548 \nL 369.725568 160.36548 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p26c6b37c46)\" d=\"M 65.361932 129.510003 \nL 65.971879 171.206591 \nL 66.581826 185.105453 \nL 67.191773 192.05489 \nL 67.80172 162.86728 \nL 68.411668 185.105453 \nL 69.021615 165.249935 \nL 69.631562 171.206591 \nL 70.241509 166.573636 \nL 70.851456 179.545913 \nL 71.461403 167.415995 \nL 72.071351 178.156028 \nL 73.291245 183.119901 \nL 73.901192 190.665005 \nL 74.511139 186.842822 \nL 75.121086 193.281261 \nL 75.731034 194.371368 \nL 76.340981 199.735842 \nL 76.950928 192.05489 \nL 77.560875 189.076557 \nL 78.170822 190.159593 \nL 78.780769 194.774236 \nL 79.390717 195.529609 \nL 80.000664 189.55309 \nL 81.220558 185.105453 \nL 81.830505 183.119901 \nL 82.440452 175.520033 \nL 83.660347 172.551653 \nL 84.270294 163.388487 \nL 85.490188 171.206591 \nL 86.100135 170.015269 \nL 86.710083 166.573636 \nL 87.32003 170.079664 \nL 89.149871 160.782453 \nL 90.369766 159.29328 \nL 90.979713 154.721899 \nL 91.58966 156.044196 \nL 93.419502 164.996471 \nL 94.029449 165.994522 \nL 95.249343 164.535135 \nL 95.85929 167.11869 \nL 96.469237 166.395452 \nL 97.079185 164.126041 \nL 97.689132 165.029327 \nL 98.299079 167.415995 \nL 99.518973 160.233808 \nL 100.12892 162.579718 \nL 100.738868 163.432651 \nL 101.958762 162.320438 \nL 103.178656 163.926234 \nL 105.008498 162.361868 \nL 105.618445 163.116209 \nL 106.228392 160.169265 \nL 106.838339 160.933523 \nL 107.448286 162.86728 \nL 108.058234 163.572003 \nL 108.668181 161.940682 \nL 110.498022 163.979184 \nL 111.107969 162.428362 \nL 111.717917 158.751768 \nL 112.327864 160.515158 \nL 112.937811 160.12269 \nL 113.547758 157.655199 \nL 114.157705 156.278188 \nL 114.767652 156.968732 \nL 115.3776 156.637912 \nL 115.987547 157.307728 \nL 116.597494 158.942888 \nL 117.207441 159.570342 \nL 117.817388 158.266275 \nL 118.427335 158.887143 \nL 119.037283 158.557069 \nL 119.64723 160.08751 \nL 120.257177 158.835076 \nL 121.477071 159.99784 \nL 122.087018 157.89917 \nL 123.306913 160.782453 \nL 123.91686 161.319777 \nL 124.526807 160.144238 \nL 125.136754 161.519509 \nL 125.746701 161.199414 \nL 126.356649 160.059986 \nL 126.966596 159.760468 \nL 127.576543 160.276419 \nL 128.18649 161.584301 \nL 128.796437 160.484613 \nL 129.406384 160.192403 \nL 130.016332 157.56753 \nL 131.236226 160.11301 \nL 131.846173 159.834804 \nL 132.45612 161.064188 \nL 133.066067 160.782453 \nL 133.676015 161.243689 \nL 134.285962 160.233808 \nL 134.895909 160.691801 \nL 136.72575 159.899049 \nL 137.335698 160.344454 \nL 139.165539 159.586236 \nL 139.775486 160.019711 \nL 140.995381 159.531547 \nL 141.605328 159.95513 \nL 142.215275 159.71541 \nL 142.825222 160.130941 \nL 143.435169 157.954195 \nL 144.045116 159.018367 \nL 144.655064 158.793111 \nL 145.265011 159.834804 \nL 145.874958 159.606789 \nL 146.484905 160.004526 \nL 147.704799 162.008817 \nL 148.314747 160.554177 \nL 148.924694 159.724929 \nL 149.534641 160.107505 \nL 150.144588 161.08028 \nL 150.754535 160.856379 \nL 151.364482 161.2229 \nL 151.97443 162.167479 \nL 153.194324 161.717029 \nL 153.804271 160.925246 \nL 154.414218 161.278832 \nL 155.024165 162.191115 \nL 155.634113 162.531465 \nL 156.854007 162.0941 \nL 158.073901 162.758262 \nL 158.683848 160.917828 \nL 159.293796 161.25322 \nL 159.903743 161.049736 \nL 160.51369 161.380009 \nL 161.733584 159.930153 \nL 162.343531 160.782453 \nL 162.953479 161.106177 \nL 163.563426 160.911142 \nL 164.173373 161.230107 \nL 165.393267 159.834804 \nL 166.003214 159.652121 \nL 166.613162 159.970987 \nL 167.223109 160.782453 \nL 167.833056 160.597409 \nL 168.443003 160.905091 \nL 169.05295 160.721488 \nL 170.882792 161.621171 \nL 171.492739 161.43768 \nL 172.102686 162.203926 \nL 172.712633 162.490358 \nL 173.932528 163.985397 \nL 174.542475 164.257165 \nL 175.152422 164.065188 \nL 175.762369 164.791731 \nL 176.372316 165.054639 \nL 176.982263 163.955014 \nL 177.592211 163.318054 \nL 178.812105 163.848369 \nL 179.422052 163.665724 \nL 180.031999 163.926234 \nL 180.641946 163.745105 \nL 181.251894 163.129245 \nL 182.471788 162.780854 \nL 183.081735 163.469086 \nL 183.691682 163.722588 \nL 184.301629 164.39899 \nL 184.911577 163.798576 \nL 185.521524 162.783041 \nL 186.131471 163.034902 \nL 186.741418 162.450308 \nL 187.961312 162.124173 \nL 188.57126 162.374307 \nL 189.791154 163.680872 \nL 190.401101 163.51499 \nL 191.011048 162.947854 \nL 192.84089 162.470165 \nL 193.450837 161.918724 \nL 194.060784 161.765853 \nL 194.670731 162.005934 \nL 195.280678 161.854094 \nL 195.890626 162.091528 \nL 197.720467 161.643153 \nL 198.330414 161.115646 \nL 198.940361 160.971983 \nL 199.550309 161.584301 \nL 200.770203 162.044568 \nL 201.38015 162.643901 \nL 203.209992 163.308125 \nL 204.429886 164.469597 \nL 205.039833 164.680178 \nL 205.64978 164.527915 \nL 206.259727 164.736433 \nL 206.869675 163.511523 \nL 207.479622 163.366207 \nL 208.089569 163.577011 \nL 208.699516 163.432651 \nL 209.309463 163.641393 \nL 209.91941 163.497978 \nL 211.139305 163.909694 \nL 211.749252 163.76695 \nL 212.359199 163.97 \nL 212.969146 164.514544 \nL 214.189041 164.909559 \nL 214.798988 164.426664 \nL 216.018882 164.145078 \nL 217.238776 164.535135 \nL 219.068618 164.119827 \nL 219.678565 163.326926 \nL 220.288512 163.19431 \nL 221.508407 162.283195 \nL 222.118354 162.156171 \nL 222.728301 161.708144 \nL 223.338248 161.905043 \nL 223.948195 160.822392 \nL 225.16809 159.950098 \nL 225.778037 160.150687 \nL 226.997931 159.29328 \nL 227.607878 158.557069 \nL 228.217825 158.759858 \nL 228.827773 158.03109 \nL 229.43772 157.61659 \nL 230.047667 157.820609 \nL 231.267561 157.002259 \nL 231.877508 156.901927 \nL 233.097403 157.912031 \nL 233.70735 157.508442 \nL 234.927244 157.905532 \nL 236.147139 157.109885 \nL 236.757086 157.307728 \nL 237.367033 157.798863 \nL 237.97698 157.992891 \nL 238.586927 157.892944 \nL 239.196874 156.627374 \nL 240.416769 156.43905 \nL 241.026716 156.057319 \nL 241.636663 155.965772 \nL 242.24661 156.161428 \nL 243.466505 155.979516 \nL 244.076452 155.322176 \nL 244.686399 155.800064 \nL 245.296346 155.711239 \nL 245.906293 154.780664 \nL 246.51624 154.975712 \nL 247.126188 155.448355 \nL 247.736135 155.361892 \nL 248.346082 155.553062 \nL 248.956029 156.019095 \nL 250.175923 156.393333 \nL 250.785871 156.852034 \nL 251.395818 157.035201 \nL 253.225659 156.767968 \nL 253.835606 156.949048 \nL 254.445554 156.324527 \nL 255.055501 155.971302 \nL 255.665448 155.353901 \nL 256.275395 155.271588 \nL 256.885342 154.925073 \nL 257.495289 155.372442 \nL 258.105237 154.501651 \nL 258.715184 154.947553 \nL 259.325131 154.867812 \nL 259.935078 155.049165 \nL 260.545025 155.489189 \nL 261.154972 155.408504 \nL 261.76492 154.811967 \nL 262.374867 154.476487 \nL 262.984814 154.656262 \nL 263.594761 155.090732 \nL 264.814655 155.443248 \nL 265.424603 155.11095 \nL 267.254444 155.633176 \nL 267.864391 156.055579 \nL 268.474338 155.976099 \nL 270.30418 154.998129 \nL 270.914127 154.922725 \nL 272.134021 155.263784 \nL 272.743969 155.188255 \nL 274.57381 155.69158 \nL 277.013599 155.390647 \nL 277.623546 155.794385 \nL 278.84344 156.119799 \nL 279.453387 156.518021 \nL 280.063335 156.205269 \nL 281.283229 156.994517 \nL 281.893176 156.917311 \nL 282.503123 157.074134 \nL 283.11307 156.997139 \nL 283.723018 157.385158 \nL 284.332965 157.307728 \nL 284.942912 156.768714 \nL 286.162806 157.53747 \nL 286.772753 157.002259 \nL 287.382701 157.38389 \nL 289.822489 157.081739 \nL 291.042384 157.38266 \nL 291.652331 157.756078 \nL 294.702067 158.48748 \nL 295.312014 158.85205 \nL 295.921961 158.774633 \nL 297.141855 159.058766 \nL 297.751802 158.981423 \nL 298.36175 159.339942 \nL 301.411485 158.955539 \nL 302.021433 158.665466 \nL 302.63138 159.018367 \nL 303.241327 158.942888 \nL 303.851274 159.29328 \nL 305.681116 159.067079 \nL 306.291063 158.781853 \nL 306.90101 159.128243 \nL 307.510957 159.263357 \nL 308.120904 159.606789 \nL 308.730851 159.531547 \nL 309.340799 159.04076 \nL 310.560693 159.308067 \nL 311.780587 159.984557 \nL 312.390534 159.909488 \nL 313.000482 159.629903 \nL 313.610429 159.760468 \nL 314.220376 159.686506 \nL 315.44027 159.13386 \nL 316.050217 159.466779 \nL 316.660165 159.596165 \nL 317.270112 159.523494 \nL 317.880059 159.049272 \nL 318.490006 158.978267 \nL 319.099953 159.30757 \nL 320.929795 159.690395 \nL 321.539742 159.222535 \nL 323.369583 158.422266 \nL 323.979531 158.35423 \nL 324.589478 158.678041 \nL 325.199425 158.609732 \nL 325.809372 158.15205 \nL 326.419319 158.279683 \nL 327.639214 158.146185 \nL 328.249161 157.886855 \nL 328.859108 158.206503 \nL 329.469055 157.563926 \nL 330.079002 157.499445 \nL 333.738685 158.253227 \nL 334.348632 157.999527 \nL 335.568527 158.24684 \nL 336.178474 157.807462 \nL 336.788421 157.744024 \nL 338.008315 157.990269 \nL 338.618263 157.741104 \nL 339.22821 156.937094 \nL 340.448104 157.18473 \nL 341.058051 156.939554 \nL 341.667998 156.879136 \nL 343.49784 157.2469 \nL 344.107787 157.550506 \nL 344.717734 157.489417 \nL 345.327681 157.247298 \nL 345.937629 157.368034 \nL 346.547576 157.668744 \nL 348.377417 158.025088 \nL 348.987365 158.321809 \nL 350.207259 157.842306 \nL 352.0371 158.193008 \nL 354.476889 157.951473 \nL 355.086836 158.066916 \nL 355.696783 158.356703 \nL 356.306731 158.29636 \nL 356.916678 158.410361 \nL 358.136572 158.983685 \nL 358.746519 159.095549 \nL 359.356466 159.034298 \nL 359.966414 159.317897 \nL 360.576361 159.256435 \nL 361.186308 159.36682 \nL 361.796255 159.134282 \nL 362.406202 159.073566 \nL 363.016149 158.501497 \nL 364.236044 158.383408 \nL 364.845991 158.494215 \nL 366.065885 159.052118 \nL 366.675832 158.992445 \nL 367.28578 159.269259 \nL 369.115621 158.588993 \nL 369.725568 158.697613 \nL 369.725568 158.697613 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#p26c6b37c46)\" d=\"M 65.361932 129.510003 \nL 65.971879 87.813416 \nL 66.581826 101.712278 \nL 67.191773 46.116816 \nL 67.80172 62.795461 \nL 68.411668 18.319091 \nL 69.021615 34.203505 \nL 69.631562 46.116816 \nL 70.241509 36.850908 \nL 71.461403 53.698032 \nL 72.071351 60.015691 \nL 72.681298 58.946535 \nL 73.291245 69.943437 \nL 73.901192 73.914541 \nL 74.511139 82.601347 \nL 75.121086 80.455182 \nL 75.731034 83.180449 \nL 76.340981 90.00797 \nL 76.950928 100.322393 \nL 78.170822 102.97581 \nL 78.780769 100.503684 \nL 79.390717 108.661716 \nL 81.220558 110.978186 \nL 81.830505 114.618371 \nL 82.440452 112.256245 \nL 83.0504 112.831371 \nL 84.270294 119.085853 \nL 84.880241 119.401736 \nL 85.490188 124.604522 \nL 86.100135 124.744681 \nL 86.710083 127.193532 \nL 87.32003 127.256137 \nL 87.929977 129.510003 \nL 89.149871 129.510003 \nL 89.759819 133.577972 \nL 90.979713 133.388753 \nL 92.199607 140.629096 \nL 92.809554 138.574477 \nL 93.419502 140.155942 \nL 94.029449 136.459441 \nL 94.639396 138.01952 \nL 95.249343 136.181459 \nL 96.469237 135.924863 \nL 97.079185 138.950741 \nL 97.689132 135.687279 \nL 98.299079 137.091206 \nL 98.909026 136.955819 \nL 99.518973 138.288231 \nL 100.12892 138.136888 \nL 101.348815 143.408866 \nL 101.958762 144.548119 \nL 102.568709 144.305566 \nL 103.788603 146.449245 \nL 104.398551 146.188636 \nL 105.008498 143.408866 \nL 105.618445 143.201429 \nL 106.228392 144.226446 \nL 107.448286 148.571303 \nL 108.058234 145.953736 \nL 108.668181 146.883591 \nL 109.278128 146.645585 \nL 109.888075 148.667894 \nL 111.107969 150.358303 \nL 111.717917 152.253601 \nL 112.327864 151.962012 \nL 114.157705 154.219096 \nL 114.767652 153.917765 \nL 115.3776 152.618954 \nL 115.987547 153.336624 \nL 116.597494 155.018509 \nL 117.207441 154.721899 \nL 117.817388 155.390647 \nL 118.427335 155.096547 \nL 119.037283 153.87206 \nL 120.867124 153.077643 \nL 122.696966 149.699932 \nL 123.306913 149.489625 \nL 123.91686 150.143373 \nL 124.526807 149.93283 \nL 125.136754 148.884178 \nL 125.746701 150.358303 \nL 128.18649 149.556442 \nL 128.796437 150.159739 \nL 130.626279 149.586142 \nL 131.236226 150.167034 \nL 131.846173 148.463005 \nL 132.45612 147.540968 \nL 133.066067 148.124556 \nL 133.676015 147.221829 \nL 134.895909 149.81443 \nL 136.115803 148.04182 \nL 136.72575 147.884773 \nL 137.335698 147.029578 \nL 137.945645 146.883591 \nL 138.555592 146.0508 \nL 139.775486 145.78184 \nL 140.385433 144.305566 \nL 140.995381 144.187203 \nL 141.605328 144.732567 \nL 142.215275 144.612713 \nL 142.825222 145.146222 \nL 143.435169 146.317934 \nL 144.655064 144.788151 \nL 145.265011 143.408866 \nL 146.484905 141.956748 \nL 147.094852 140.629096 \nL 148.924694 142.200272 \nL 149.534641 141.50902 \nL 150.144588 140.23198 \nL 150.754535 139.5645 \nL 151.364482 139.493694 \nL 151.97443 140.007047 \nL 152.584377 141.092395 \nL 153.194324 140.437391 \nL 153.804271 140.933732 \nL 154.414218 140.28872 \nL 155.024165 140.779357 \nL 155.634113 140.703717 \nL 156.24406 140.073144 \nL 156.854007 141.107729 \nL 158.073901 142.046232 \nL 158.683848 143.047862 \nL 159.903743 143.943443 \nL 160.51369 143.851512 \nL 161.123637 144.288541 \nL 161.733584 145.244566 \nL 162.953479 145.049108 \nL 163.563426 145.467958 \nL 164.78332 145.273345 \nL 165.393267 146.188636 \nL 166.003214 146.590535 \nL 166.613162 146.488252 \nL 167.223109 145.890808 \nL 167.833056 146.287327 \nL 168.443003 146.188636 \nL 169.05295 144.140392 \nL 169.662897 143.570485 \nL 170.272845 143.489203 \nL 172.712633 145.057881 \nL 173.32258 144.970535 \nL 173.932528 145.350055 \nL 174.542475 145.26205 \nL 175.152422 146.096493 \nL 175.762369 145.088948 \nL 176.372316 145.915227 \nL 176.982263 145.826066 \nL 178.812105 146.902168 \nL 180.031999 146.718119 \nL 180.641946 147.505378 \nL 181.251894 147.847767 \nL 181.861841 148.620947 \nL 183.691682 149.609901 \nL 184.301629 149.507345 \nL 184.911577 149.829156 \nL 185.521524 150.568883 \nL 186.131471 150.882132 \nL 186.741418 151.609197 \nL 187.351365 151.499247 \nL 187.961312 152.216073 \nL 188.57126 152.515018 \nL 190.401101 152.179998 \nL 191.011048 151.667614 \nL 191.620995 151.962012 \nL 192.230943 152.652605 \nL 192.84089 152.542406 \nL 193.450837 152.828478 \nL 194.060784 152.718491 \nL 194.670731 153.392556 \nL 195.280678 152.891269 \nL 196.500573 154.219096 \nL 197.11052 153.720928 \nL 197.720467 153.992411 \nL 198.330414 154.642195 \nL 198.940361 154.527958 \nL 199.550309 155.169442 \nL 200.160256 155.429504 \nL 200.770203 156.061196 \nL 201.38015 155.570372 \nL 201.990097 155.825191 \nL 204.429886 153.90888 \nL 205.039833 154.165376 \nL 205.64978 154.058644 \nL 206.259727 154.312283 \nL 206.869675 154.921656 \nL 207.479622 155.169442 \nL 208.089569 155.060262 \nL 208.699516 154.598628 \nL 209.309463 155.196507 \nL 210.529358 154.981564 \nL 212.359199 155.699596 \nL 213.579093 154.801379 \nL 214.189041 154.698152 \nL 214.798988 154.934754 \nL 215.408935 155.507071 \nL 216.628829 155.968084 \nL 217.238776 155.528681 \nL 217.848724 155.425018 \nL 219.068618 156.538624 \nL 220.288512 156.326639 \nL 220.898459 156.547633 \nL 221.508407 156.117948 \nL 222.118354 155.368353 \nL 223.338248 155.169442 \nL 223.948195 155.390647 \nL 225.16809 155.193835 \nL 225.778037 154.780664 \nL 226.387984 154.685303 \nL 227.607878 155.121388 \nL 228.217825 154.714655 \nL 228.827773 154.000936 \nL 229.43772 153.910234 \nL 230.047667 154.127922 \nL 230.657614 153.73082 \nL 231.267561 153.947563 \nL 231.877508 153.249676 \nL 232.487456 153.769839 \nL 233.097403 153.379794 \nL 234.317297 153.208072 \nL 234.927244 152.525332 \nL 235.537191 152.740958 \nL 236.757086 151.984765 \nL 237.367033 152.200017 \nL 237.97698 152.707406 \nL 238.586927 152.91862 \nL 239.196874 153.419932 \nL 239.806822 153.336624 \nL 240.416769 153.833016 \nL 241.026716 153.748851 \nL 241.636663 154.240395 \nL 242.856557 154.071009 \nL 243.466505 153.133326 \nL 244.686399 154.103927 \nL 245.296346 153.739108 \nL 245.906293 154.219096 \nL 246.51624 154.41602 \nL 247.736135 154.249976 \nL 248.346082 153.336624 \nL 248.956029 153.257728 \nL 249.565976 152.628907 \nL 250.175923 152.552857 \nL 250.785871 152.750725 \nL 251.395818 153.219827 \nL 252.005765 153.414241 \nL 253.225659 154.339037 \nL 254.445554 154.179368 \nL 255.665448 155.087464 \nL 256.885342 154.925073 \nL 257.495289 154.580746 \nL 258.715184 155.472028 \nL 259.325131 155.390647 \nL 260.545025 154.709808 \nL 261.154972 154.89054 \nL 261.76492 155.328327 \nL 262.984814 155.169442 \nL 263.594761 155.346545 \nL 264.814655 155.189001 \nL 265.424603 155.617903 \nL 266.644497 155.964033 \nL 267.254444 156.386722 \nL 267.864391 155.554727 \nL 269.084286 154.901364 \nL 269.694233 155.073981 \nL 270.914127 154.922725 \nL 271.524074 154.601759 \nL 272.134021 154.037407 \nL 272.743969 153.720928 \nL 273.353916 153.89398 \nL 273.963863 153.822888 \nL 275.793704 154.335148 \nL 276.403652 154.744255 \nL 277.013599 154.911379 \nL 278.233493 154.289691 \nL 278.84344 154.219096 \nL 279.453387 154.622723 \nL 280.063335 154.31534 \nL 281.283229 154.175591 \nL 281.893176 153.637807 \nL 282.503123 153.570219 \nL 285.552859 154.389738 \nL 286.772753 154.253033 \nL 287.382701 154.413721 \nL 288.602595 154.27801 \nL 289.212542 153.984085 \nL 289.822489 153.917765 \nL 290.432436 153.626412 \nL 291.042384 153.561408 \nL 291.652331 153.048403 \nL 292.262278 152.761722 \nL 292.872225 152.922534 \nL 293.482172 152.860091 \nL 294.092119 152.576206 \nL 294.702067 152.736223 \nL 295.312014 153.116016 \nL 295.921961 153.053722 \nL 296.531908 153.430681 \nL 297.141855 153.367902 \nL 297.751802 153.742053 \nL 298.36175 153.243314 \nL 298.971697 153.615849 \nL 299.581644 153.336624 \nL 300.801538 153.21349 \nL 302.021433 153.949142 \nL 302.63138 153.672651 \nL 303.241327 153.610854 \nL 303.851274 153.123894 \nL 304.461221 153.275995 \nL 305.071168 153.639 \nL 305.681116 153.577911 \nL 306.291063 153.938306 \nL 306.90101 153.876782 \nL 308.120904 153.336624 \nL 309.340799 153.633718 \nL 309.950746 153.366262 \nL 310.560693 153.514001 \nL 311.17064 153.454577 \nL 311.780587 153.601372 \nL 312.390534 153.131226 \nL 313.000482 153.073194 \nL 315.44027 153.655477 \nL 316.050217 153.394458 \nL 316.660165 153.740475 \nL 317.270112 153.88338 \nL 317.880059 153.82464 \nL 318.490006 154.167128 \nL 319.099953 153.708029 \nL 319.7099 153.849642 \nL 320.319848 153.393488 \nL 322.149689 153.81655 \nL 322.759636 153.75909 \nL 323.979531 154.42985 \nL 325.199425 154.703732 \nL 326.419319 154.586276 \nL 327.029266 154.721899 \nL 328.249161 155.377338 \nL 328.859108 155.317591 \nL 329.469055 155.450282 \nL 331.908844 155.213382 \nL 333.738685 155.605825 \nL 334.348632 155.358114 \nL 334.95858 155.299771 \nL 335.568527 154.866035 \nL 337.398368 154.136174 \nL 338.008315 154.453498 \nL 339.838157 154.8423 \nL 340.448104 154.232765 \nL 341.667998 154.491225 \nL 342.277946 154.253033 \nL 342.887893 154.198779 \nL 343.49784 154.327232 \nL 344.107787 154.637212 \nL 344.717734 154.582461 \nL 345.327681 154.89054 \nL 346.547576 155.141681 \nL 348.377417 154.976308 \nL 348.987365 155.279578 \nL 350.207259 155.525823 \nL 350.817206 155.825974 \nL 351.427153 155.947418 \nL 352.0371 155.891288 \nL 352.647048 155.658712 \nL 353.256995 155.779734 \nL 353.866942 155.548377 \nL 354.476889 155.669126 \nL 355.086836 155.614175 \nL 355.696783 155.909107 \nL 356.306731 155.679415 \nL 358.746519 156.154295 \nL 359.356466 155.926479 \nL 359.966414 155.527301 \nL 360.576361 155.645602 \nL 361.186308 155.935004 \nL 361.796255 155.88075 \nL 362.406202 155.655817 \nL 363.016149 155.602345 \nL 363.626097 155.378916 \nL 366.065885 155.844688 \nL 366.675832 156.128436 \nL 367.28578 156.074766 \nL 368.505674 156.302992 \nL 369.115621 156.082184 \nL 369.725568 156.195825 \nL 369.725568 156.195825 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#p26c6b37c46)\" d=\"M 65.361932 212.903184 \nL 65.971879 212.903184 \nL 66.581826 157.307728 \nL 67.191773 171.206591 \nL 67.80172 162.86728 \nL 68.411668 157.307728 \nL 69.021615 153.336624 \nL 69.631562 160.782453 \nL 70.241509 138.775911 \nL 70.851456 146.188636 \nL 71.461403 159.834804 \nL 72.681298 167.999161 \nL 73.901192 162.86728 \nL 74.511139 160.782453 \nL 75.121086 163.848369 \nL 76.340981 160.233808 \nL 77.560875 149.365521 \nL 78.170822 144.672397 \nL 78.780769 147.638963 \nL 79.390717 136.459441 \nL 80.610611 142.339722 \nL 81.220558 141.864556 \nL 81.830505 147.379969 \nL 82.440452 149.63939 \nL 83.0504 146.188636 \nL 83.660347 145.650616 \nL 84.270294 150.358303 \nL 84.880241 142.145334 \nL 85.490188 141.773705 \nL 87.32003 147.540968 \nL 88.539924 142.339722 \nL 89.759819 145.78184 \nL 90.369766 145.394417 \nL 90.979713 148.903763 \nL 92.199607 148.04182 \nL 92.809554 149.451848 \nL 94.029449 155.570372 \nL 94.639396 155.038528 \nL 95.249343 156.195825 \nL 95.85929 154.037407 \nL 97.079185 153.111853 \nL 97.689132 154.219096 \nL 98.299079 153.769839 \nL 98.909026 154.825798 \nL 99.518973 157.307728 \nL 100.12892 153.952832 \nL 100.738868 153.538544 \nL 101.348815 155.917843 \nL 102.568709 157.756078 \nL 103.788603 154.267361 \nL 105.618445 160.62686 \nL 106.228392 161.395629 \nL 106.838339 158.516335 \nL 107.448286 156.910625 \nL 108.058234 157.699251 \nL 108.668181 157.307728 \nL 109.278128 159.211687 \nL 109.888075 158.810309 \nL 110.498022 159.531547 \nL 111.107969 159.136531 \nL 112.937811 161.178301 \nL 113.547758 159.740039 \nL 114.157705 160.396372 \nL 114.767652 159.00271 \nL 115.3776 160.656858 \nL 115.987547 160.286062 \nL 116.597494 160.905091 \nL 117.207441 159.570342 \nL 117.817388 161.141903 \nL 119.037283 162.305078 \nL 119.64723 160.08751 \nL 120.257177 161.584301 \nL 120.867124 162.142117 \nL 121.477071 159.99784 \nL 122.087018 161.44782 \nL 122.696966 160.233808 \nL 124.526807 159.29328 \nL 125.136754 159.834804 \nL 125.746701 158.697613 \nL 126.966596 159.760468 \nL 128.18649 159.178732 \nL 128.796437 158.101959 \nL 129.406384 159.405678 \nL 130.016332 159.126279 \nL 130.626279 159.624211 \nL 131.236226 159.347932 \nL 131.846173 160.592923 \nL 132.45612 160.312891 \nL 133.066067 159.29328 \nL 134.285962 160.233808 \nL 134.895909 159.241486 \nL 135.505856 159.704089 \nL 136.115803 158.733252 \nL 137.335698 159.643671 \nL 137.945645 159.392556 \nL 139.165539 160.269783 \nL 139.775486 160.019711 \nL 140.385433 160.44619 \nL 140.995381 160.198691 \nL 141.605328 159.29328 \nL 142.215275 160.372054 \nL 142.825222 160.782453 \nL 143.435169 160.540023 \nL 144.045116 159.659851 \nL 144.655064 159.429699 \nL 145.265011 158.57126 \nL 145.874958 159.606789 \nL 146.484905 158.759858 \nL 147.094852 158.543188 \nL 148.924694 156.099134 \nL 150.754535 157.307728 \nL 151.364482 157.111973 \nL 153.804271 158.640501 \nL 154.414218 158.442334 \nL 155.024165 159.373779 \nL 155.634113 158.613672 \nL 156.24406 158.975595 \nL 156.854007 157.675914 \nL 158.073901 157.307728 \nL 158.683848 156.585708 \nL 159.293796 157.487068 \nL 160.51369 155.006008 \nL 161.123637 154.844636 \nL 161.733584 154.160816 \nL 162.953479 153.8546 \nL 163.563426 154.219096 \nL 164.173373 154.067505 \nL 165.393267 154.780664 \nL 166.003214 154.628439 \nL 166.613162 153.978667 \nL 167.223109 152.840246 \nL 168.443003 151.584667 \nL 169.662897 152.297678 \nL 170.882792 152.035751 \nL 171.492739 152.383558 \nL 172.102686 152.253601 \nL 172.712633 153.067403 \nL 173.32258 152.93506 \nL 173.932528 151.872429 \nL 174.542475 151.748188 \nL 175.152422 151.164587 \nL 175.762369 151.962012 \nL 176.372316 151.38363 \nL 177.592211 152.048699 \nL 179.422052 151.689038 \nL 180.641946 152.333404 \nL 181.251894 152.21391 \nL 181.861841 151.661314 \nL 182.471788 151.546542 \nL 183.081735 151.003092 \nL 183.691682 151.320528 \nL 184.301629 152.060206 \nL 184.911577 152.369056 \nL 185.521524 151.832428 \nL 186.741418 152.443131 \nL 187.351365 152.32903 \nL 187.961312 151.803238 \nL 188.57126 151.693412 \nL 189.181207 152.402247 \nL 189.791154 151.883787 \nL 190.401101 152.584818 \nL 191.011048 152.876208 \nL 191.620995 153.565733 \nL 192.230943 153.849642 \nL 192.84089 154.527958 \nL 193.450837 154.804623 \nL 194.060784 155.472028 \nL 194.670731 154.958625 \nL 195.280678 155.2294 \nL 196.500573 154.991257 \nL 197.11052 155.642433 \nL 197.720467 155.905093 \nL 198.330414 155.022982 \nL 198.940361 155.665137 \nL 199.550309 154.792097 \nL 200.160256 155.429504 \nL 200.770203 154.939314 \nL 201.38015 154.081211 \nL 201.990097 153.972007 \nL 202.600044 154.601759 \nL 203.819939 155.113174 \nL 204.429886 155.001372 \nL 205.039833 155.615691 \nL 205.64978 155.141681 \nL 206.259727 155.750097 \nL 206.869675 155.637488 \nL 208.089569 154.705397 \nL 208.699516 154.598628 \nL 209.309463 154.844636 \nL 209.91941 155.438973 \nL 210.529358 155.679415 \nL 211.139305 156.265314 \nL 211.749252 156.154295 \nL 212.359199 155.699596 \nL 212.969146 156.278188 \nL 214.189041 156.059667 \nL 214.798988 155.612746 \nL 215.408935 155.844688 \nL 216.018882 156.411028 \nL 216.628829 155.633176 \nL 217.238776 155.195102 \nL 217.848724 155.757267 \nL 218.458671 154.991257 \nL 219.068618 155.549769 \nL 219.678565 155.775571 \nL 220.288512 155.672568 \nL 220.898459 156.221883 \nL 221.508407 156.44243 \nL 222.118354 156.338047 \nL 222.728301 156.878428 \nL 225.16809 157.730517 \nL 226.387984 156.888146 \nL 226.997931 156.158197 \nL 227.607878 155.121388 \nL 228.217825 154.403494 \nL 229.43772 155.454556 \nL 231.267561 156.08585 \nL 231.877508 155.68451 \nL 232.487456 154.982832 \nL 233.70735 155.401023 \nL 234.317297 155.307898 \nL 234.927244 155.514328 \nL 235.537191 155.421452 \nL 236.147139 155.922789 \nL 237.367033 156.325484 \nL 237.97698 155.056509 \nL 238.586927 154.966876 \nL 239.196874 155.169442 \nL 239.806822 155.080045 \nL 241.026716 155.480205 \nL 241.636663 154.815521 \nL 243.466505 154.556427 \nL 244.076452 154.187582 \nL 244.686399 153.538544 \nL 245.296346 153.175639 \nL 245.906293 153.376737 \nL 246.51624 152.457123 \nL 247.736135 152.860091 \nL 249.565976 152.628907 \nL 250.175923 153.101502 \nL 250.785871 153.297568 \nL 251.395818 152.674774 \nL 252.005765 153.142596 \nL 253.225659 153.529398 \nL 253.835606 153.182908 \nL 254.445554 153.374936 \nL 255.055501 153.031155 \nL 256.275395 152.88134 \nL 256.885342 153.336624 \nL 257.495289 153.261233 \nL 258.105237 152.66016 \nL 259.935078 153.224947 \nL 260.545025 153.151059 \nL 261.154972 152.818661 \nL 261.76492 153.26286 \nL 263.594761 152.276851 \nL 265.424603 152.069253 \nL 266.03455 151.748188 \nL 267.254444 152.116586 \nL 269.084286 153.407754 \nL 269.694233 153.58482 \nL 270.914127 154.429266 \nL 271.524074 154.601759 \nL 272.134021 154.527958 \nL 272.743969 154.943701 \nL 273.353916 155.113174 \nL 273.963863 155.524791 \nL 274.57381 155.69158 \nL 275.183757 155.615691 \nL 275.793704 155.781263 \nL 276.403652 155.465231 \nL 277.013599 155.869914 \nL 278.233493 156.195825 \nL 278.84344 155.64462 \nL 279.453387 154.622723 \nL 280.063335 154.551581 \nL 281.283229 154.880325 \nL 281.893176 154.809059 \nL 282.503123 154.971785 \nL 283.11307 154.900668 \nL 284.332965 154.296315 \nL 284.942912 154.458655 \nL 285.552859 153.928998 \nL 286.772753 153.794829 \nL 287.382701 154.185246 \nL 287.992648 154.345685 \nL 288.602595 154.27801 \nL 289.212542 154.663929 \nL 289.822489 154.821759 \nL 290.432436 155.204124 \nL 291.042384 155.134858 \nL 291.652331 155.514328 \nL 292.262278 155.668194 \nL 292.872225 155.375275 \nL 294.702067 155.833057 \nL 295.312014 155.322176 \nL 295.921961 155.034042 \nL 297.141855 154.900059 \nL 297.751802 154.615279 \nL 298.36175 154.114266 \nL 298.971697 154.267361 \nL 300.191591 154.139082 \nL 300.801538 154.290922 \nL 301.411485 154.227062 \nL 302.021433 154.592278 \nL 302.63138 154.314134 \nL 303.241327 154.677251 \nL 303.851274 154.825798 \nL 304.461221 154.549183 \nL 305.071168 154.485633 \nL 305.681116 154.633522 \nL 306.291063 154.570072 \nL 307.510957 154.863214 \nL 308.120904 154.799665 \nL 308.730851 154.944931 \nL 309.340799 155.297423 \nL 309.950746 155.025828 \nL 310.560693 154.548648 \nL 311.17064 154.28026 \nL 311.780587 153.601372 \nL 312.390534 153.542035 \nL 313.610429 153.833016 \nL 314.220376 153.569647 \nL 314.830323 153.714367 \nL 315.44027 153.452577 \nL 316.050217 153.596874 \nL 316.660165 153.538544 \nL 317.270112 153.681946 \nL 317.880059 154.02559 \nL 318.490006 154.167128 \nL 319.099953 154.10799 \nL 319.7099 153.849642 \nL 320.929795 153.73374 \nL 322.149689 154.014157 \nL 322.759636 154.350532 \nL 323.369583 154.488628 \nL 323.979531 154.42985 \nL 326.419319 154.975053 \nL 327.029266 154.91584 \nL 327.639214 154.663395 \nL 328.249161 154.798211 \nL 328.859108 154.739818 \nL 330.079002 155.007225 \nL 330.688949 154.948746 \nL 331.298897 154.699705 \nL 333.128738 154.527958 \nL 333.738685 154.282124 \nL 334.348632 154.22608 \nL 334.95858 153.982047 \nL 335.568527 154.11475 \nL 336.788421 154.004415 \nL 337.398368 154.136174 \nL 338.618263 154.026485 \nL 339.22821 154.157324 \nL 339.838157 153.732858 \nL 341.058051 153.625915 \nL 341.667998 153.389114 \nL 342.277946 153.519916 \nL 342.887893 153.467252 \nL 343.49784 153.232353 \nL 344.107787 153.362646 \nL 344.717734 153.674043 \nL 345.327681 153.440225 \nL 345.937629 153.388319 \nL 346.547576 153.156129 \nL 347.157523 153.285166 \nL 347.76747 153.233931 \nL 348.377417 153.362248 \nL 348.987365 153.132108 \nL 349.597312 153.260102 \nL 350.207259 153.031155 \nL 350.817206 152.981001 \nL 351.427153 153.285937 \nL 352.0371 153.058394 \nL 352.647048 153.361863 \nL 353.256995 153.311436 \nL 354.476889 153.913477 \nL 355.086836 153.336624 \nL 355.696783 152.937024 \nL 356.306731 153.06247 \nL 356.916678 152.665106 \nL 357.526625 152.443131 \nL 358.746519 152.347968 \nL 359.966414 152.598201 \nL 360.576361 152.378649 \nL 361.186308 152.674774 \nL 361.796255 152.627217 \nL 363.016149 152.873735 \nL 363.626097 152.655873 \nL 364.236044 152.269035 \nL 364.845991 152.561779 \nL 365.455938 152.515018 \nL 366.065885 152.130826 \nL 366.675832 152.253601 \nL 367.895727 152.162079 \nL 369.725568 153.02688 \nL 369.725568 153.02688 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#p26c6b37c46)\" d=\"M 50.14375 157.029755 \nL 384.94375 157.029755 \n\" style=\"fill:none;stroke:#000000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 50.14375 251.82 \nL 50.14375 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 384.94375 251.82 \nL 384.94375 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 50.14375 251.82 \nL 384.94375 251.82 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 50.14375 7.2 \nL 384.94375 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 302.089063 103.26875 \nL 377.94375 103.26875 \nQ 379.94375 103.26875 379.94375 101.26875 \nL 379.94375 14.2 \nQ 379.94375 12.2 377.94375 12.2 \nL 302.089063 12.2 \nQ 300.089063 12.2 300.089063 14.2 \nL 300.089063 101.26875 \nQ 300.089063 103.26875 302.089063 103.26875 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_19\">\n     <path d=\"M 304.089063 20.298437 \nL 324.089063 20.298437 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_20\"/>\n    <g id=\"text_14\">\n     <!-- P(die=1) -->\n     <g transform=\"translate(332.089063 23.798437)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 1259 4147 \nL 1259 2394 \nL 2053 2394 \nQ 2494 2394 2734 2622 \nQ 2975 2850 2975 3272 \nQ 2975 3691 2734 3919 \nQ 2494 4147 2053 4147 \nL 1259 4147 \nz\nM 628 4666 \nL 2053 4666 \nQ 2838 4666 3239 4311 \nQ 3641 3956 3641 3272 \nQ 3641 2581 3239 2228 \nQ 2838 1875 2053 1875 \nL 1259 1875 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-50\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1984 4856 \nQ 1566 4138 1362 3434 \nQ 1159 2731 1159 2009 \nQ 1159 1288 1364 580 \nQ 1569 -128 1984 -844 \nL 1484 -844 \nQ 1016 -109 783 600 \nQ 550 1309 550 2009 \nQ 550 2706 781 3412 \nQ 1013 4119 1484 4856 \nL 1984 4856 \nz\n\" id=\"DejaVuSans-28\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 678 2906 \nL 4684 2906 \nL 4684 2381 \nL 678 2381 \nL 678 2906 \nz\nM 678 1631 \nL 4684 1631 \nL 4684 1100 \nL 678 1100 \nL 678 1631 \nz\n\" id=\"DejaVuSans-3d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 513 4856 \nL 1013 4856 \nQ 1481 4119 1714 3412 \nQ 1947 2706 1947 2009 \nQ 1947 1309 1714 600 \nQ 1481 -109 1013 -844 \nL 513 -844 \nQ 928 -128 1133 580 \nQ 1338 1288 1338 2009 \nQ 1338 2731 1133 3434 \nQ 928 4138 513 4856 \nz\n\" id=\"DejaVuSans-29\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use x=\"60.302734\" xlink:href=\"#DejaVuSans-28\"/>\n      <use x=\"99.316406\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"162.792969\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"190.576172\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"252.099609\" xlink:href=\"#DejaVuSans-3d\"/>\n      <use x=\"335.888672\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"399.511719\" xlink:href=\"#DejaVuSans-29\"/>\n     </g>\n    </g>\n    <g id=\"line2d_21\">\n     <path d=\"M 304.089063 34.976562 \nL 324.089063 34.976562 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_22\"/>\n    <g id=\"text_15\">\n     <!-- P(die=2) -->\n     <g transform=\"translate(332.089063 38.476562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use x=\"60.302734\" xlink:href=\"#DejaVuSans-28\"/>\n      <use x=\"99.316406\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"162.792969\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"190.576172\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"252.099609\" xlink:href=\"#DejaVuSans-3d\"/>\n      <use x=\"335.888672\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"399.511719\" xlink:href=\"#DejaVuSans-29\"/>\n     </g>\n    </g>\n    <g id=\"line2d_23\">\n     <path d=\"M 304.089063 49.654687 \nL 324.089063 49.654687 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_24\"/>\n    <g id=\"text_16\">\n     <!-- P(die=3) -->\n     <g transform=\"translate(332.089063 53.154687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use x=\"60.302734\" xlink:href=\"#DejaVuSans-28\"/>\n      <use x=\"99.316406\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"162.792969\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"190.576172\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"252.099609\" xlink:href=\"#DejaVuSans-3d\"/>\n      <use x=\"335.888672\" xlink:href=\"#DejaVuSans-33\"/>\n      <use x=\"399.511719\" xlink:href=\"#DejaVuSans-29\"/>\n     </g>\n    </g>\n    <g id=\"line2d_25\">\n     <path d=\"M 304.089063 64.332812 \nL 324.089063 64.332812 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_26\"/>\n    <g id=\"text_17\">\n     <!-- P(die=4) -->\n     <g transform=\"translate(332.089063 67.832812)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use x=\"60.302734\" xlink:href=\"#DejaVuSans-28\"/>\n      <use x=\"99.316406\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"162.792969\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"190.576172\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"252.099609\" xlink:href=\"#DejaVuSans-3d\"/>\n      <use x=\"335.888672\" xlink:href=\"#DejaVuSans-34\"/>\n      <use x=\"399.511719\" xlink:href=\"#DejaVuSans-29\"/>\n     </g>\n    </g>\n    <g id=\"line2d_27\">\n     <path d=\"M 304.089063 79.010937 \nL 324.089063 79.010937 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_28\"/>\n    <g id=\"text_18\">\n     <!-- P(die=5) -->\n     <g transform=\"translate(332.089063 82.510937)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use x=\"60.302734\" xlink:href=\"#DejaVuSans-28\"/>\n      <use x=\"99.316406\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"162.792969\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"190.576172\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"252.099609\" xlink:href=\"#DejaVuSans-3d\"/>\n      <use x=\"335.888672\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"399.511719\" xlink:href=\"#DejaVuSans-29\"/>\n     </g>\n    </g>\n    <g id=\"line2d_29\">\n     <path d=\"M 304.089063 93.689062 \nL 324.089063 93.689062 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_30\"/>\n    <g id=\"text_19\">\n     <!-- P(die=6) -->\n     <g transform=\"translate(332.089063 97.189062)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use x=\"60.302734\" xlink:href=\"#DejaVuSans-28\"/>\n      <use x=\"99.316406\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"162.792969\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"190.576172\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"252.099609\" xlink:href=\"#DejaVuSans-3d\"/>\n      <use x=\"335.888672\" xlink:href=\"#DejaVuSans-36\"/>\n      <use x=\"399.511719\" xlink:href=\"#DejaVuSans-29\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p26c6b37c46\">\n   <rect height=\"244.62\" width=\"334.8\" x=\"50.14375\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n","text/plain":["<Figure size 432x324 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"origin_pos":20,"id":"x3NCOrWDBC-V"},"source":["每条实线对应于骰子的6个值中的一个，并给出骰子在每组实验后出现值的估计概率。\n","当我们通过更多的实验获得更多的数据时，这$6$条实体曲线向真实概率收敛。\n","\n","### 概率论公理\n","\n","在处理骰子掷出时，我们将集合 $\\mathcal{S} = \\{1, 2, 3, 4, 5, 6\\}$ 称为 *样本空间*（sample space） 或 *结果空间*（outcome space），其中每个元素都是 *结果*（outcome）。*事件*（event） 是来自给定样本空间的一组结果。例如，“看到 $5$”（$\\{5\\}$）和 “看到奇数”（$\\{1, 3, 5\\}$）都是掷出骰子的有效事件。注意，如果随机实验的结果在事件 $\\mathcal{A}$ 中，则事件 $\\mathcal{A}$ 已经发生。也就是说，如果投掷出$3$点，因为 $3 \\in \\{1, 3, 5\\}$ ，我们可以说，“看到奇数” 的事件发生了。\n","\n","形式上，*概率*（probability） 可以被认为是将集合映射到真实值的函数。在给定的样本空间 $\\mathcal{S}$ 中，事件$\\mathcal{A}$的概率，表示为 $P(\\mathcal{A})$，满足以下属性：\n","\n","* 对于任意事件 $\\mathcal{A}$，其概率从不会是负数，即 $P(\\mathcal{A}) \\geq 0$；\n","* 整个样本空间的概率为 $1$，即 $P(\\mathcal{S}) = 1$；\n","* 对于任意事件 $\\mathcal{A}_1, \\mathcal{A}_2, \\ldots$ 的可数序列，这些事件*互斥*（mutually exclusive）（对于所有 $i \\neq j$ 都有 $\\mathcal{A}_i \\cap \\mathcal{A}_j = \\emptyset$），任何事件发生的概率等于它们各自发生的概率之和，即 $P(\\bigcup_{i=1}^{\\infty} \\mathcal{A}_i) = \\sum_{i=1}^{\\infty} P(\\mathcal{A}_i)$。\n","\n","这些也是概率论的公理，由科尔莫戈罗夫于 1933 年提出。有了这个公理系统，我们可以避免任何关于随机性的哲学争论；相反，我们可以用数学语言严格地推理。例如，让事件 $\\mathcal{A}_1$ 为整个样本空间，且当所有$i > 1$时的$\\mathcal{A}_i = \\emptyset$，我们可以证明 $P(\\emptyset) = 0$，即不可能发生事件的概率是 $0$。\n","\n","### 随机变量\n","\n","在我们掷骰子的随机实验中，我们引入了 *随机变量*（random variable） 的概念。随机变量几乎可以是任何数量，并且不是确定性的。它可以在随机实验的一组可能性中取一个值。考虑一个随机变量 $X$，其值在掷骰子的样本空间 $\\mathcal{S} = \\{1, 2, 3, 4, 5, 6\\}$ 中。我们可以将事件 “看到一个 $5$” 表示为 $\\{X = 5\\}$ 或 $X = 5$，其概率表示为 $P(\\{X = 5\\})$ 或 $P(X = 5)$。通过 $P(X = a)$，我们区分了随机变量 $X$ 和 $X$ 可以采取的值（例如 $a$）。然而，这可能会导致繁琐的表示。\n","为了简化符号，一方面，我们可以将 $P(X)$ 表示为随机变量 $X$ 上的 *分布*（distribution）：分布告诉我们 $X$ 获得任意值的概率。另一方面，我们可以简单用 $P(a)$ 表示随机变量取值 $a$ 的概率。由于概率论中的事件是来自样本空间的一组结果，因此我们可以为随机变量指定值的可取范围。例如，$P(1 \\leq X \\leq 3)$ 表示事件的概率 $\\{1 \\leq X \\leq 3\\}$，这意味着 $\\{X = 1, 2, \\text{or}, 3\\}$。等价地，$P(1 \\leq X \\leq 3)$ 表示随机变量 $X$ 从 $\\{1, 2, 3\\}$ 中取值的概率。\n","\n","请注意，*离散* (discrete) 随机变量（如骰子的侧面）和 *连续* (continuous) 变量（如人的体重和身高）之间存在微妙的区别。问两个人是否具有完全相同的身高没有什么意义。如果我们进行足够精确的测量，你会发现这个星球上没有两个人具有完全相同的身高。事实上，如果我们采取足够精细的测量，在你起床和去睡觉时都不会得到相同的身高。因此，问一个人身高为 1.80139278297192196202 米高的概率是没有任何意义的。考虑到世界上的人口数量，这个概率几乎是 0。在这种情况下，询问某人的身高是否落入给定的区间，比如是否在 1.79 米和 1.81 米之间更有意义。在这些情况下，我们将这个看到某个数值的可能性量化为 *密度* (density)。恰好 1.80 米的高度上没有概率，但密度不是 0。在任何两个不同高度之间的区间，我们都有非零的概率。在本节的其余部分中，我们将考虑离散空间中的概率。对于连续随机变量的概率，您可以参考 :numref:`sec_random_variables`。\n","\n","## 处理多个随机变量\n","\n","很多时候，我们会希望一次考虑多个随机变量。比如，我们可能需要对疾病和症状之间的关系进行建模。给定一个疾病和一个症状，比如 “流感” 和 “咳嗽”，会在某个患者身上，以某个概率存在或不存在关系。虽然我们可能希望这两者发生的概率都接近于零，但我们可能需要估计这些概率和它们之间的关系，以便我们可以运用我们的推断来实现更好的医疗服务。\n","\n","再举一个更复杂的例子：图像包含数百万像素，因此有数百万个随机变量。在许多情况下，图像会附带一个标签，标识图像中的对象。我们也可以将标签视为一个随机变量。我们甚至可以将所有元数据视为随机变量，例如位置、时间、光圈、焦距、ISO、对焦距离和相机类型。所有这些都是联合发生的随机变量。当我们处理多个随机变量时，会有若干个变量是我们感兴趣的。\n","\n","### 联合概率\n","\n","第一个被称为 *联合概率* (joint probability) $P(A = a, B=b)$。给定任何值 $a$ 和 $b$, 联合概率可以回答,  $A=a$ 和 $B=b$ 同时满足的概率是多少? 请注意，对于任何值，对于任何 $a$ 和 $b$ 的取值，$P(A = a, B=b) \\leq P(A=a)$。这点是确定的，因为要同时发生 $A=a$ 和 $B=b$，$A=a$就必须发生，$B=b$也必须发生（反之亦然）。因此，$A=a$ 和 $B=b$ 同时发生的可能性不大于 $A=a$ 或是 $B=b$ 的可能性。\n","\n","### 条件概率\n","\n","这给我们带来了一个有趣的比率：$0 \\leq \\frac{P(A=a, B=b)}{P(A=a)} \\leq 1$。我们称这个比率为 *条件概率* (conditional probability)，并用 $P(B=b \\mid A=a)$ 表示它：它是 $B=b$ 的概率，前提是发生了 $A=a$。\n","\n","### 贝叶斯定理\n","\n","使用条件概率的定义，我们可以得出统计数据中最有用和最著名的方程之一：*Bayes 定理* (Bayes' theorem)。它如下所示。通过构造，我们有 *乘法规则*， $P(A, B) = P(B \\mid A) P(A)$。根据对称性，这也适用于 $P(A, B) = P(A \\mid B) P(B)$。假设 $P(B)>0$， 求解其中一个条件变量，我们得到\n","\n","$$P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}.$$\n","\n","请注意，在这里我们使用更紧凑的表示法，其中 $P(A, B)$ 是一个 *联合分布*，$P(A \\mid B)$ 是一个 *条件分布*。这种分布可以在给定值 $A = a, B=b$ 上进行求值。\n","\n","###  边际化\n","\n","如果我们想从另一件事中推断一件事，但我们只知道相反方向的属性，比如因和果的时候，Bayes 定理是非常有用的，正如我们将在本节后面看到的那样。为了能进行这项工作，我们需要的一个重要操作是 *边际化*。这项操作是从 $P(A, B)$ 中确定 $P(B)$ 的操作。我们可以看到，$B$ 的概率相当于计算 $A$ 的所有可能选择，并将所有选择的联合概率聚合在一起：\n","\n","$$P(B) = \\sum_{A} P(A, B),$$\n","\n","这也称为 *求和规则*。边际化结果的概率或分布称为 *边际概率* 或 *边际分布*。\n","\n","### 独立性\n","\n","另一个要检查的有用属性是 *依赖* 与 *独立*。两个随机变量 $A$ 和 $B$ 是独立的，意味着事件 $A$ 的发生不会透露有关 $B$ 事件的发生情况的任何信息。在这种情况下，统计学家通常将这一点表述为 $A \\perp  B$。根据贝叶斯定理，马上就能同样得到 $P(A \\mid B) = P(A)$。在所有其他情况下，我们称 $A$ 和 $B$ 依赖。比如，一个骰子的两次连续抛出是独立的。相比之下，灯开关的位置和房间的亮度并不是（尽管它们不是具有确定性的，因为总是可能存在灯泡坏掉，电源故障，或者开关故障）。\n","\n","由于 $P(A \\mid B) = \\frac{P(A, B)}{P(B)} = P(A)$ 等价于 $P(A, B) = P(A)P(B)$，因此两个随机变量是独立的当且仅当两个随机变量的联合分布是其各自分布的乘积。同样地，给定另一个随机变量 $C$时，两个随机变量 $A$ 和 $B$ 是 *条件独立的* ，当且仅当 $P(A, B \\mid C) = P(A \\mid C)P(B \\mid C)$ 。这个情况表示为 $A \\perp B \\mid C$。\n","\n","### 应用\n",":label:`subsec_probability_hiv_app`\n","\n","让我们用实战考验一下我们的技能。假设一个医生对患者进行艾滋病病毒（HIV）测试。这个测试是相当准确的，如果患者健康但测试显示他患病，这样的失败概率只有 1% 。此外，如果患者真正感染HIV，它永远不会检测不出。我们使用 $D_1$ 来表示诊断结果（如果阳性，则为 $1$，如果阴性，则为 $0$），$H$ 来表示感染艾滋病病毒的状态（如果阳性，则为 $1$，如果阴性，则为0）。\n","在 :numref:`conditional_prob_D1` 中列出了这样的条件概率。\n","\n",":条件概率为 $P(D_1 \\mid H)$。\n","\n","| 条件概率 | $H=1$ | $H=0$ |\n","|---|---|---|\n","|$P(D_1 = 1 \\mid H)$|            1 |         0.01 |\n","|$P(D_1 = 0 \\mid H)$|            0 |         0.99 |\n",":label:`conditional_prob_D1`\n","\n","请注意，每列的加和都是 1（但每行的加和不是），因为条件概率需要总和为1，就像概率一样。让我们计算如果测试出来呈阳性，患者感染HIV的概率，即 $P(H = 1 \\mid D_1 = 1)$。显然，这将取决于疾病有多常见，因为它会影响错误警报的数量。假设人口总体是相当健康的，例如，$P(H=1) = 0.0015$。为了应用贝叶斯定理，我们需要运用边际化和乘法规则来确定\n","\n","$$\\begin{aligned}\n","&P(D_1 = 1) \\\\\n","=& P(D_1=1, H=0) + P(D_1=1, H=1)  \\\\\n","=& P(D_1=1 \\mid H=0) P(H=0) + P(D_1=1 \\mid H=1) P(H=1) \\\\\n","=& 0.011485.\n","\\end{aligned}\n","$$\n","\n","因此，我们得到\n","\n","$$\\begin{aligned}\n","&P(H = 1 \\mid D_1 = 1)\\\\ =& \\frac{P(D_1=1 \\mid H=1) P(H=1)}{P(D_1=1)} \\\\ =& 0.1306 \\end{aligned}.$$\n","\n","换句话说，尽管使用了非常准确的测试，患者实际上患有艾滋病的几率只有 13.06%。正如我们所看到的，概率可能是违反直觉的。\n","\n","患者在收到这样可怕的消息后应该怎么办？很可能，患者会要求医生进行另一次测试来了解清楚。第二个测试具有不同的特性，它不如第一个测试那么好，如 :numref:`conditional_prob_D2` 所示。\n","\n",":条件概率为 $P(D_2 \\mid H)$。\n","\n","| 条件概率 | $H=1$ | $H=0$ |\n","|---|---|---|\n","|$P(D_2 = 1 \\mid H)$|            0.98 |         0.03 |\n","|$P(D_2 = 0 \\mid H)$|            0.02 |         0.97 |\n",":label:`conditional_prob_D2`\n","\n","不幸的是，第二次测试也显示阳性。让我们通过假设条件独立性来计算出应用 Bayes 定理的必要概率：\n","\n","$$\\begin{aligned}\n","&P(D_1 = 1, D_2 = 1 \\mid H = 0) \\\\\n","=& P(D_1 = 1 \\mid H = 0) P(D_2 = 1 \\mid H = 0)  \\\\\n","=& 0.0003,\n","\\end{aligned}\n","$$\n","\n","$$\\begin{aligned}\n","&P(D_1 = 1, D_2 = 1 \\mid H = 1) \\\\\n","=& P(D_1 = 1 \\mid H = 1) P(D_2 = 1 \\mid H = 1)  \\\\\n","=& 0.98.\n","\\end{aligned}\n","$$\n","\n","现在我们可以应用边际化和乘法规则：\n","\n","$$\\begin{aligned}\n","&P(D_1 = 1, D_2 = 1) \\\\\n","=& P(D_1 = 1, D_2 = 1, H = 0) + P(D_1 = 1, D_2 = 1, H = 1)  \\\\\n","=& P(D_1 = 1, D_2 = 1 \\mid H = 0)P(H=0) + P(D_1 = 1, D_2 = 1 \\mid H = 1)P(H=1)\\\\\n","=& 0.00176955.\n","\\end{aligned}\n","$$\n","\n","最后，鉴于存在两次阳性检测，患者患有艾滋病的概率为\n","\n","$$\\begin{aligned}\n","&P(H = 1 \\mid D_1 = 1, D_2 = 1)\\\\\n","=& \\frac{P(D_1 = 1, D_2 = 1 \\mid H=1) P(H=1)}{P(D_1 = 1, D_2 = 1)} \\\\\n","=& 0.8307.\n","\\end{aligned}\n","$$\n","\n","也就是说，第二次测试使我们能够对患病的情况获得更高的信心。尽管第二次检验比第一次检验的准确性要低得多，但它仍然显著改善了我们的估计。\n","\n","## 期望和差异\n","\n","为了概括概率分布的关键特征，我们需要一些测量方法。随机变量 $X$ 的 *期望*（或平均值）表示为\n","\n","$$E[X] = \\sum_{x} x P(X = x).$$\n","\n","当函数 $f(x)$ 的输入是从分布 $P$ 中抽取的随机变量时，$f(x)$ 的期望值为\n","\n","$$E_{x \\sim P}[f(x)] = \\sum_x f(x) P(x).$$\n","\n","在许多情况下，我们希望衡量随机变量 $X$ 与其期望值的偏置。这可以通过方差来量化\n","\n","$$\\mathrm{Var}[X] = E\\left[(X - E[X])^2\\right] =\n","E[X^2] - E[X]^2.$$\n","\n","它的平方根被称为 *标准差* (standared deviation)。随机变量函数的方差衡量的是，当从该随机变量分布中采样不同值 $x$ 时，函数值偏离该函数的期望的程度：\n","\n","$$\\mathrm{Var}[f(x)] = E\\left[\\left(f(x) - E[f(x)]\\right)^2\\right].$$\n","\n","## 小结\n","\n","* 我们可以从概率分布中采样。\n","* 我们可以使用联合分布、条件分布、Bayes 定理、边缘化和独立性假设来分析多个随机变量。\n","* 期望和方差为概率分布的关键特征的概括提供了实用的度量形式。\n"]}]}